import{_ as i,C as l,c as r,o as d,j as t,a,a2 as n,G as o}from"./chunks/framework.Og7vQWlB.js";const v=JSON.parse('{"title":"Shardy Dialect","description":"","frontmatter":{},"headers":[],"relativePath":"api/shardy.md","filePath":"api/shardy.md","lastUpdated":null}'),p={name:"api/shardy.md"},h={class:"jldocstring custom-block"},c={class:"jldocstring custom-block"},u={class:"jldocstring custom-block"},g={class:"jldocstring custom-block"},f={class:"jldocstring custom-block"},m={class:"jldocstring custom-block"},b={class:"jldocstring custom-block"},y={class:"jldocstring custom-block"},_={class:"jldocstring custom-block"},R={class:"jldocstring custom-block"},T={class:"jldocstring custom-block"};function I(x,e,D,j,L,w){const s=l("Badge");return d(),r("div",null,[e[42]||(e[42]=t("h1",{id:"Shardy-Dialect",tabindex:"-1"},[a("Shardy Dialect "),t("a",{class:"header-anchor",href:"#Shardy-Dialect","aria-label":'Permalink to "Shardy Dialect {#Shardy-Dialect}"'},"â€‹")],-1)),e[43]||(e[43]=t("p",null,[a("Refer to the "),t("a",{href:"https://openxla.org/shardy",target:"_blank",rel:"noreferrer"},"official documentation"),a(" for more details.")],-1)),t("details",h,[t("summary",null,[e[0]||(e[0]=t("a",{id:"Reactant.MLIR.Dialects.sdy.all_gather-Tuple{Reactant.MLIR.IR.Value}",href:"#Reactant.MLIR.Dialects.sdy.all_gather-Tuple{Reactant.MLIR.IR.Value}"},[t("span",{class:"jlbinding"},"Reactant.MLIR.Dialects.sdy.all_gather")],-1)),e[1]||(e[1]=a()),o(s,{type:"info",class:"jlObjectType jlMethod",text:"Method"})]),e[2]||(e[2]=n("",9))]),t("details",c,[t("summary",null,[e[3]||(e[3]=t("a",{id:"Reactant.MLIR.Dialects.sdy.all_slice-Tuple{Reactant.MLIR.IR.Value}",href:"#Reactant.MLIR.Dialects.sdy.all_slice-Tuple{Reactant.MLIR.IR.Value}"},[t("span",{class:"jlbinding"},"Reactant.MLIR.Dialects.sdy.all_slice")],-1)),e[4]||(e[4]=a()),o(s,{type:"info",class:"jlObjectType jlMethod",text:"Method"})]),e[5]||(e[5]=n("",9))]),t("details",u,[t("summary",null,[e[6]||(e[6]=t("a",{id:"Reactant.MLIR.Dialects.sdy.constant-Tuple{}",href:"#Reactant.MLIR.Dialects.sdy.constant-Tuple{}"},[t("span",{class:"jlbinding"},"Reactant.MLIR.Dialects.sdy.constant")],-1)),e[7]||(e[7]=a()),o(s,{type:"info",class:"jlObjectType jlMethod",text:"Method"})]),e[8]||(e[8]=n("",7))]),t("details",g,[t("summary",null,[e[9]||(e[9]=t("a",{id:"Reactant.MLIR.Dialects.sdy.data_flow_edge-Tuple{Reactant.MLIR.IR.Value}",href:"#Reactant.MLIR.Dialects.sdy.data_flow_edge-Tuple{Reactant.MLIR.IR.Value}"},[t("span",{class:"jlbinding"},"Reactant.MLIR.Dialects.sdy.data_flow_edge")],-1)),e[10]||(e[10]=a()),o(s,{type:"info",class:"jlObjectType jlMethod",text:"Method"})]),e[11]||(e[11]=n("",13))]),t("details",f,[t("summary",null,[e[12]||(e[12]=t("a",{id:"Reactant.MLIR.Dialects.sdy.manual_computation-Tuple{Vector{Reactant.MLIR.IR.Value}}",href:"#Reactant.MLIR.Dialects.sdy.manual_computation-Tuple{Vector{Reactant.MLIR.IR.Value}}"},[t("span",{class:"jlbinding"},"Reactant.MLIR.Dialects.sdy.manual_computation")],-1)),e[13]||(e[13]=a()),o(s,{type:"info",class:"jlObjectType jlMethod",text:"Method"})]),e[14]||(e[14]=n("",6))]),t("details",m,[t("summary",null,[e[15]||(e[15]=t("a",{id:"Reactant.MLIR.Dialects.sdy.mesh-Tuple{}",href:"#Reactant.MLIR.Dialects.sdy.mesh-Tuple{}"},[t("span",{class:"jlbinding"},"Reactant.MLIR.Dialects.sdy.mesh")],-1)),e[16]||(e[16]=a()),o(s,{type:"info",class:"jlObjectType jlMethod",text:"Method"})]),e[17]||(e[17]=t("p",null,[t("code",null,"mesh")],-1)),e[18]||(e[18]=t("p",null,[a("Defines a new named mesh. All meshes in a module must have the same number of devices (except for meshes with a single device_id). The mesh is a "),t("code",null,"Symbol"),a(" operation that appears in the module's "),t("code",null,"SymbolTable"),a(" and can be referenced by its "),t("code",null,"name"),a(".")],-1)),e[19]||(e[19]=t("p",null,[t("a",{href:"https://github.com/EnzymeAD/Reactant.jl/blob/9339756f1ea494409f8163350c91237db9ff080a/src/mlir/Dialects/Shardy.jl#L307-L314",target:"_blank",rel:"noreferrer"},"source")],-1))]),t("details",b,[t("summary",null,[e[20]||(e[20]=t("a",{id:"Reactant.MLIR.Dialects.sdy.named_computation-Tuple{Vector{Reactant.MLIR.IR.Value}}",href:"#Reactant.MLIR.Dialects.sdy.named_computation-Tuple{Vector{Reactant.MLIR.IR.Value}}"},[t("span",{class:"jlbinding"},"Reactant.MLIR.Dialects.sdy.named_computation")],-1)),e[21]||(e[21]=a()),o(s,{type:"info",class:"jlObjectType jlMethod",text:"Method"})]),e[22]||(e[22]=n("",7))]),t("details",y,[t("summary",null,[e[23]||(e[23]=t("a",{id:"Reactant.MLIR.Dialects.sdy.propagation_barrier-Tuple{Reactant.MLIR.IR.Value}",href:"#Reactant.MLIR.Dialects.sdy.propagation_barrier-Tuple{Reactant.MLIR.IR.Value}"},[t("span",{class:"jlbinding"},"Reactant.MLIR.Dialects.sdy.propagation_barrier")],-1)),e[24]||(e[24]=a()),o(s,{type:"info",class:"jlObjectType jlMethod",text:"Method"})]),e[25]||(e[25]=n("",5))]),t("details",_,[t("summary",null,[e[26]||(e[26]=t("a",{id:"Reactant.MLIR.Dialects.sdy.reshard-Tuple{Reactant.MLIR.IR.Value}",href:"#Reactant.MLIR.Dialects.sdy.reshard-Tuple{Reactant.MLIR.IR.Value}"},[t("span",{class:"jlbinding"},"Reactant.MLIR.Dialects.sdy.reshard")],-1)),e[27]||(e[27]=a()),o(s,{type:"info",class:"jlObjectType jlMethod",text:"Method"})]),e[28]||(e[28]=n("",6))]),t("details",R,[t("summary",null,[e[29]||(e[29]=t("a",{id:"Reactant.MLIR.Dialects.sdy.sharding_constraint-Tuple{Reactant.MLIR.IR.Value}",href:"#Reactant.MLIR.Dialects.sdy.sharding_constraint-Tuple{Reactant.MLIR.IR.Value}"},[t("span",{class:"jlbinding"},"Reactant.MLIR.Dialects.sdy.sharding_constraint")],-1)),e[30]||(e[30]=a()),o(s,{type:"info",class:"jlObjectType jlMethod",text:"Method"})]),e[31]||(e[31]=t("p",null,[t("code",null,"sharding_constraint")],-1)),e[32]||(e[32]=t("p",null,"Attaches a sharding to an intermediate tensor (e.g. the result of a matmul) to indicate that this is how that tensor, or a subset of its uses, should be sharded.",-1)),e[33]||(e[33]=t("p",null,"If the sharding has open dimensions and unconstraint axes, it means the tensor can be further sharded along the open dimensions.",-1)),e[34]||(e[34]=t("p",null,"This op can either:",-1)),e[35]||(e[35]=t("ul",null,[t("li",null,[t("p",null,"Have no uses (dangling) - which means the attached sharding is how the input tensor itself should be sharded.")]),t("li",null,[t("p",null,"Have uses - which means the attached sharding is how the uses of the sharding constraint op should be sharded, while other uses of the input tensor might have a different sharding (if the input tensor has no other uses then the behavior is the same as the no uses case).")])],-1)),e[36]||(e[36]=t("p",null,[t("a",{href:"https://github.com/EnzymeAD/Reactant.jl/blob/9339756f1ea494409f8163350c91237db9ff080a/src/mlir/Dialects/Shardy.jl#L488-L505",target:"_blank",rel:"noreferrer"},"source")],-1))]),t("details",T,[t("summary",null,[e[37]||(e[37]=t("a",{id:"Reactant.MLIR.Dialects.sdy.sharding_group-Tuple{Reactant.MLIR.IR.Value}",href:"#Reactant.MLIR.Dialects.sdy.sharding_group-Tuple{Reactant.MLIR.IR.Value}"},[t("span",{class:"jlbinding"},"Reactant.MLIR.Dialects.sdy.sharding_group")],-1)),e[38]||(e[38]=a()),o(s,{type:"info",class:"jlObjectType jlMethod",text:"Method"})]),e[39]||(e[39]=t("p",null,[t("code",null,"sharding_group")],-1)),e[40]||(e[40]=t("p",null,"This op provides an interface to assign tensors to sharding groups ( groups of tensors that will be enforced to have identical shardings). During propagation, as soon as one group element is sharded, all other members will be sharded in exactly the same way. This operation takes the argument group ID and returns no result, but instead modifies the internal sharding group representation to add the input tensor to the group with the given ID.",-1)),e[41]||(e[41]=t("p",null,[t("a",{href:"https://github.com/EnzymeAD/Reactant.jl/blob/9339756f1ea494409f8163350c91237db9ff080a/src/mlir/Dialects/Shardy.jl#L528-L538",target:"_blank",rel:"noreferrer"},"source")],-1))])])}const k=i(p,[["render",I]]);export{v as __pageData,k as default};
