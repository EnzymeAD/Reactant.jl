<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>SparseTensor Dialect | Reactant.jl</title>
    <meta name="description" content="Documentation for Reactant.jl">
    <meta name="generator" content="VitePress v1.6.3">
    <link rel="preload stylesheet" href="/Reactant.jl/previews/PR1385/assets/style.6WODduWV.css" as="style">
    <link rel="preload stylesheet" href="/Reactant.jl/previews/PR1385/vp-icons.css" as="style">
    
    <script type="module" src="/Reactant.jl/previews/PR1385/assets/app.xaDruMK0.js"></script>
    <link rel="preload" href="/Reactant.jl/previews/PR1385/assets/inter-roman-latin.Di8DUHzh.woff2" as="font" type="font/woff2" crossorigin="">
    <link rel="modulepreload" href="/Reactant.jl/previews/PR1385/assets/chunks/theme.DntL1eZb.js">
    <link rel="modulepreload" href="/Reactant.jl/previews/PR1385/assets/chunks/framework.CNNerpTq.js">
    <link rel="modulepreload" href="/Reactant.jl/previews/PR1385/assets/api_dialects_sparsetensor.md.4uqqTpex.lean.js">
    <link rel="icon" href="/Reactant.jl/previews/PR1385/favicon.ico">
    <script src="/versions.js"></script>
    <script src="/Reactant.jl/previews/PR1385/siteinfo.js"></script>
    <script id="check-dark-mode">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"auto",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
    <script id="check-mac-os">document.documentElement.classList.toggle("mac",/Mac|iPhone|iPod|iPad/i.test(navigator.platform));</script>
  </head>
  <body>
    <div id="app"><div class="Layout" data-v-a9a9e638><!--[--><!--]--><!--[--><span tabindex="-1" data-v-492508fc></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-492508fc>Skip to content</a><!--]--><!----><header class="VPNav" data-v-a9a9e638 data-v-f1e365da><div class="VPNavBar" data-v-f1e365da data-v-822684d1><div class="wrapper" data-v-822684d1><div class="container" data-v-822684d1><div class="title" data-v-822684d1><div class="VPNavBarTitle has-sidebar" data-v-822684d1 data-v-0f4f798b><a class="title" href="/Reactant.jl/previews/PR1385/" data-v-0f4f798b><!--[--><!--]--><!--[--><!--[--><!--[--><img class="VPImage dark logo" src="/Reactant.jl/previews/PR1385/logo.svg" alt data-v-35a7d0b8><!--]--><!--[--><img class="VPImage light logo" src="/Reactant.jl/previews/PR1385/logo.svg" alt data-v-35a7d0b8><!--]--><!--]--><!--]--><span data-v-0f4f798b>Reactant.jl</span><!--[--><!--]--></a></div></div><div class="content" data-v-822684d1><div class="content-body" data-v-822684d1><!--[--><!--]--><div class="VPNavBarSearch search" data-v-822684d1><!--[--><!----><div id="local-search"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><span class="vp-icon DocSearch-Search-Icon"></span><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"><kbd class="DocSearch-Button-Key"></kbd><kbd class="DocSearch-Button-Key">K</kbd></span></button></div><!--]--></div><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-822684d1 data-v-e6d46098><span id="main-nav-aria-label" class="visually-hidden" data-v-e6d46098> Main Navigation </span><!--[--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/Reactant.jl/previews/PR1385/" tabindex="0" data-v-e6d46098 data-v-956ec74c><!--[--><span data-v-956ec74c>Home</span><!--]--></a><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-e6d46098 data-v-04f5c5e9><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-04f5c5e9><span class="text" data-v-04f5c5e9><!----><span data-v-04f5c5e9>Getting Started</span><span class="vpi-chevron-down text-icon" data-v-04f5c5e9></span></span></button><div class="menu" data-v-04f5c5e9><div class="VPMenu" data-v-04f5c5e9 data-v-7dd3104a><div class="items" data-v-7dd3104a><!--[--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR1385/introduction" data-v-acbfed09><!--[--><span data-v-acbfed09>Introduction</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR1385/introduction/configuration" data-v-acbfed09><!--[--><span data-v-acbfed09>Configuration</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR1385/introduction/FAQs" data-v-acbfed09><!--[--><span data-v-acbfed09>FAQs</span><!--]--></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><a class="VPLink link vp-external-link-icon VPNavBarMenuLink" href="https://enzymead.github.io/Reactant.jl/benchmarks/" target="_blank" rel="noreferrer" tabindex="0" data-v-e6d46098 data-v-956ec74c><!--[--><span data-v-956ec74c>Benchmarks</span><!--]--></a><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-e6d46098 data-v-04f5c5e9><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-04f5c5e9><span class="text" data-v-04f5c5e9><!----><span data-v-04f5c5e9>Tutorials</span><span class="vpi-chevron-down text-icon" data-v-04f5c5e9></span></span></button><div class="menu" data-v-04f5c5e9><div class="VPMenu" data-v-04f5c5e9 data-v-7dd3104a><div class="items" data-v-7dd3104a><!--[--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR1385/tutorials/" data-v-acbfed09><!--[--><span data-v-acbfed09>Overview</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR1385/tutorials/profiling" data-v-acbfed09><!--[--><span data-v-acbfed09>Profiling</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR1385/tutorials/multihost" data-v-acbfed09><!--[--><span data-v-acbfed09>Distributed</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR1385/tutorials/local-build" data-v-acbfed09><!--[--><span data-v-acbfed09>Local build</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR1385/tutorials/control-flow" data-v-acbfed09><!--[--><span data-v-acbfed09>Control Flow</span><!--]--></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup active" data-v-e6d46098 data-v-04f5c5e9><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-04f5c5e9><span class="text" data-v-04f5c5e9><!----><span data-v-04f5c5e9>API</span><span class="vpi-chevron-down text-icon" data-v-04f5c5e9></span></span></button><div class="menu" data-v-04f5c5e9><div class="VPMenu" data-v-04f5c5e9 data-v-7dd3104a><div class="items" data-v-7dd3104a><!--[--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR1385/api/api" data-v-acbfed09><!--[--><span data-v-acbfed09>Core Reactant API</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR1385/api/sharding" data-v-acbfed09><!--[--><span data-v-acbfed09>Sharding</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR1385/api/ops" data-v-acbfed09><!--[--><span data-v-acbfed09>Ops</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR1385/api/config" data-v-acbfed09><!--[--><span data-v-acbfed09>Configuration</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuGroup" data-v-7dd3104a data-v-48c802d0><p class="title" data-v-48c802d0>MLIR Dialects</p><!--[--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR1385/api/dialects/arith" data-v-acbfed09><!--[--><span data-v-acbfed09>ArithOps</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR1385/api/dialects/affine" data-v-acbfed09><!--[--><span data-v-acbfed09>Affine</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR1385/api/dialects/builtin" data-v-acbfed09><!--[--><span data-v-acbfed09>Builtin</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR1385/api/dialects/chlo" data-v-acbfed09><!--[--><span data-v-acbfed09>Chlo</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR1385/api/dialects/enzyme" data-v-acbfed09><!--[--><span data-v-acbfed09>Enzyme</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR1385/api/dialects/enzymexla" data-v-acbfed09><!--[--><span data-v-acbfed09>EnzymeXLA</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR1385/api/dialects/func" data-v-acbfed09><!--[--><span data-v-acbfed09>Func</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR1385/api/dialects/gpu" data-v-acbfed09><!--[--><span data-v-acbfed09>GPU</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR1385/api/dialects/llvm" data-v-acbfed09><!--[--><span data-v-acbfed09>LLVM</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR1385/api/dialects/mpi" data-v-acbfed09><!--[--><span data-v-acbfed09>MPI</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR1385/api/dialects/memref" data-v-acbfed09><!--[--><span data-v-acbfed09>MemRef</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR1385/api/dialects/nvvm" data-v-acbfed09><!--[--><span data-v-acbfed09>NVVM</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR1385/api/dialects/shardy" data-v-acbfed09><!--[--><span data-v-acbfed09>Shardy</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link active" href="/Reactant.jl/previews/PR1385/api/dialects/sparsetensor" data-v-acbfed09><!--[--><span data-v-acbfed09>SparseTensor</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR1385/api/dialects/stablehlo" data-v-acbfed09><!--[--><span data-v-acbfed09>StableHLO</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR1385/api/dialects/triton" data-v-acbfed09><!--[--><span data-v-acbfed09>Triton</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR1385/api/dialects/tpu" data-v-acbfed09><!--[--><span data-v-acbfed09>TPU</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR1385/api/dialects/vhlo" data-v-acbfed09><!--[--><span data-v-acbfed09>VHLO</span><!--]--></a></div><!--]--><!--]--></div><!--]--><!--[--><div class="VPMenuGroup" data-v-7dd3104a data-v-48c802d0><p class="title" data-v-48c802d0>Low-Level API</p><!--[--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR1385/api/mlirc" data-v-acbfed09><!--[--><span data-v-acbfed09>MLIR API</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR1385/api/xla" data-v-acbfed09><!--[--><span data-v-acbfed09>XLA</span><!--]--></a></div><!--]--><!--]--></div><!--]--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR1385/api/internal" data-v-acbfed09><!--[--><span data-v-acbfed09>Internal API</span><!--]--></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><!----><!--]--><!--]--></nav><!----><div class="VPNavBarAppearance appearance" data-v-822684d1 data-v-af096f4a><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-af096f4a data-v-e40a8bb6 data-v-4a1c76db><span class="check" data-v-4a1c76db><span class="icon" data-v-4a1c76db><!--[--><span class="vpi-sun sun" data-v-e40a8bb6></span><span class="vpi-moon moon" data-v-e40a8bb6></span><!--]--></span></span></button></div><div class="VPSocialLinks VPNavBarSocialLinks social-links" data-v-822684d1 data-v-164c457f data-v-ee7a9424><!--[--><a class="VPSocialLink no-icon" href="https://julialang.org/slack/" aria-label="slack" target="_blank" rel="noopener" data-v-ee7a9424 data-v-d26d30cb><span class="vpi-social-slack"></span></a><!--]--></div><div class="VPFlyout VPNavBarExtra extra" data-v-822684d1 data-v-925effce data-v-04f5c5e9><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-04f5c5e9><span class="vpi-more-horizontal icon" data-v-04f5c5e9></span></button><div class="menu" data-v-04f5c5e9><div class="VPMenu" data-v-04f5c5e9 data-v-7dd3104a><!----><!--[--><!--[--><!----><div class="group" data-v-925effce><div class="item appearance" data-v-925effce><p class="label" data-v-925effce>Appearance</p><div class="appearance-action" data-v-925effce><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-925effce data-v-e40a8bb6 data-v-4a1c76db><span class="check" data-v-4a1c76db><span class="icon" data-v-4a1c76db><!--[--><span class="vpi-sun sun" data-v-e40a8bb6></span><span class="vpi-moon moon" data-v-e40a8bb6></span><!--]--></span></span></button></div></div></div><div class="group" data-v-925effce><div class="item social-links" data-v-925effce><div class="VPSocialLinks social-links-list" data-v-925effce data-v-ee7a9424><!--[--><a class="VPSocialLink no-icon" href="https://julialang.org/slack/" aria-label="slack" target="_blank" rel="noopener" data-v-ee7a9424 data-v-d26d30cb><span class="vpi-social-slack"></span></a><!--]--></div></div></div><!--]--><!--]--></div></div></div><!--[--><!--[--><!--[--><!--[--><a target="_blank" data-decoration="★" title="176 GitHub stars" href="https://github.com/EnzymeAD/Reactant.jl" data-v-b4d08338><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="20" height="20" fill="currentColor" style="vertical-align:middle;margin-right:0.25rem;margin-left:0.5rem;" data-v-b4d08338><path d="M12 .297C5.375.297 0 5.673 0 12.3c0 5.292 3.438 9.8 8.207 11.387.6.11.793-.26.793-.577 0-.285-.01-1.04-.015-2.04-3.338.727-4.042-1.61-4.042-1.61-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.807 1.305 3.493.997.107-.774.42-1.305.762-1.605-2.665-.3-5.467-1.333-5.467-5.931 0-1.31.47-2.382 1.236-3.222-.123-.303-.535-1.52.117-3.166 0 0 1.01-.323 3.31 1.23.96-.267 1.98-.4 3-.405 1.02.005 2.04.138 3 .405 2.3-1.553 3.31-1.23 3.31-1.23.653 1.646.24 2.863.117 3.166.765.84 1.236 1.912 1.236 3.222 0 4.61-2.807 5.625-5.477 5.921.43.372.823 1.102.823 2.222 0 1.606-.015 2.902-.015 3.293 0 .32.192.693.8.577C20.565 22.1 24 17.588 24 12.297 24 5.673 18.627.297 12 .297z" data-v-b4d08338></path></svg><span data-v-b4d08338>0.2k</span></a><a class="mobile" target="_blank" title="176 GitHub stars" href="https://github.com/EnzymeAD/Reactant.jl" data-v-b4d08338><svg xmlns="http://www.w3.org/2000/svg" width="21" height="21" viewBox="0 0 21 21" fill="none" data-v-b4d08338><path d="M19.625 5.60534C18.7083 4.03477 17.4649 2.79135 15.8945 1.87479C14.3238 0.958185 12.6091 0.5 10.7492 0.5C8.88947 0.5 7.17422 0.958325 5.60388 1.87479C4.0333 2.7913 2.78997 4.03477 1.87332 5.60534C0.956814 7.17587 0.498535 8.89089 0.498535 10.7504C0.498535 12.984 1.15021 14.9926 2.4539 16.7766C3.75744 18.5607 5.44142 19.7952 7.50571 20.4803C7.746 20.5249 7.92388 20.4936 8.03954 20.387C8.15524 20.2804 8.21302 20.1467 8.21302 19.9868C8.21302 19.9601 8.21073 19.7199 8.20629 19.266C8.20171 18.8122 8.19956 18.4162 8.19956 18.0783L7.89256 18.1315C7.69682 18.1673 7.44989 18.1825 7.15178 18.1782C6.8538 18.174 6.54446 18.1428 6.22419 18.0847C5.90377 18.0272 5.60575 17.8937 5.32988 17.6846C5.05416 17.4755 4.85842 17.2018 4.74272 16.8639L4.60925 16.5568C4.52029 16.3523 4.38023 16.1251 4.18888 15.8761C3.99754 15.6269 3.80405 15.458 3.60831 15.369L3.51486 15.3021C3.45259 15.2577 3.39481 15.204 3.34138 15.1418C3.28799 15.0796 3.24802 15.0173 3.22132 14.955C3.19458 14.8926 3.21674 14.8414 3.28804 14.8012C3.35933 14.761 3.48817 14.7416 3.67512 14.7416L3.94196 14.7814C4.11993 14.8171 4.34007 14.9236 4.60266 15.1017C4.86511 15.2796 5.08085 15.5109 5.24994 15.7956C5.4547 16.1605 5.7014 16.4385 5.99072 16.6299C6.27982 16.8212 6.5713 16.9167 6.86488 16.9167C7.15846 16.9167 7.41203 16.8945 7.62567 16.8502C7.83908 16.8057 8.0393 16.7388 8.22625 16.6499C8.30633 16.0535 8.52437 15.5953 8.88017 15.275C8.37304 15.2217 7.9171 15.1414 7.51212 15.0347C7.10736 14.9278 6.6891 14.7544 6.25761 14.5139C5.82589 14.2738 5.46774 13.9756 5.18309 13.6198C4.89839 13.2639 4.66474 12.7966 4.48247 12.2183C4.3001 11.6399 4.20889 10.9726 4.20889 10.2163C4.20889 9.13941 4.56044 8.22304 5.26341 7.46665C4.93411 6.65705 4.96519 5.74947 5.35676 4.744C5.61482 4.66382 5.9975 4.72399 6.50463 4.92412C7.01186 5.12434 7.38323 5.29587 7.61912 5.43808C7.85502 5.58024 8.04402 5.70071 8.18642 5.79842C9.01411 5.56715 9.86825 5.45149 10.7491 5.45149C11.6299 5.45149 12.4843 5.56715 13.312 5.79842L13.8192 5.47823C14.166 5.26459 14.5756 5.06881 15.0469 4.89083C15.5185 4.71295 15.8791 4.66396 16.1284 4.74414C16.5286 5.74966 16.5643 6.65719 16.2349 7.46679C16.9378 8.22318 17.2895 9.13978 17.2895 10.2164C17.2895 10.9727 17.198 11.6421 17.0159 12.225C16.8336 12.808 16.5979 13.2749 16.3088 13.6265C16.0194 13.9781 15.659 14.274 15.2275 14.5141C14.7959 14.7544 14.3775 14.9278 13.9728 15.0347C13.5678 15.1415 13.1119 15.2219 12.6047 15.2752C13.0673 15.6755 13.2986 16.3073 13.2986 17.1704V19.9864C13.2986 20.1464 13.3542 20.2799 13.4656 20.3867C13.5768 20.4932 13.7524 20.5246 13.9927 20.4799C16.0573 19.7949 17.7413 18.5603 19.0448 16.7762C20.3481 14.9922 21 12.9837 21 10.75C20.9996 8.89075 20.541 7.17587 19.625 5.60534Z" fill="currentColor" data-v-b4d08338></path></svg></a><!--]--><div class="VPFlyout VPNolebaseEnhancedReadabilitiesMenu VPNolebaseEnhancedReadabilitiesMenuFlyout" aria-label="Enhanced Readability" role="menuitem" data-v-04f5c5e9><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-04f5c5e9><span class="text" data-v-04f5c5e9><span class="i-icon-park-outline:book-open option-icon" data-v-04f5c5e9></span><!----><span class="vpi-chevron-down text-icon" data-v-04f5c5e9></span></span></button><div class="menu" data-v-04f5c5e9><div class="VPMenu" data-v-04f5c5e9 data-v-7dd3104a><!----><!--[--><!--]--></div></div></div><!--]--><!--]--><!--]--><button type="button" class="VPNavBarHamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-822684d1 data-v-5dea55bf><span class="container" data-v-5dea55bf><span class="top" data-v-5dea55bf></span><span class="middle" data-v-5dea55bf></span><span class="bottom" data-v-5dea55bf></span></span></button></div></div></div></div><div class="divider" data-v-822684d1><div class="divider-line" data-v-822684d1></div></div></div><!----></header><div class="VPLocalNav has-sidebar empty" data-v-a9a9e638 data-v-070ab83d><div class="container" data-v-070ab83d><button class="menu" aria-expanded="false" aria-controls="VPSidebarNav" data-v-070ab83d><span class="vpi-align-left menu-icon" data-v-070ab83d></span><span class="menu-text" data-v-070ab83d>Menu</span></button><div class="VPLocalNavOutlineDropdown" style="--vp-vh:0px;" data-v-070ab83d data-v-168ddf5d><button data-v-168ddf5d>Return to top</button><!----></div></div></div><aside class="VPSidebar" data-v-a9a9e638 data-v-18756405><div class="curtain" data-v-18756405></div><nav class="nav" id="VPSidebarNav" aria-labelledby="sidebar-aria-label" tabindex="-1" data-v-18756405><span class="visually-hidden" id="sidebar-aria-label" data-v-18756405> Sidebar Navigation </span><!--[--><!--]--><!--[--><div class="no-transition group" data-v-9e426adc><section class="VPSidebarItem level-0 collapsible has-active" data-v-9e426adc data-v-a4b0d9bf><div class="item" role="button" tabindex="0" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><h2 class="text" data-v-a4b0d9bf>API Reference</h2><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-a4b0d9bf><span class="vpi-chevron-right caret-icon" data-v-a4b0d9bf></span></div></div><div class="items" data-v-a4b0d9bf><!--[--><div class="VPSidebarItem level-1 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR1385/api/api" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Reactant API</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR1385/api/sharding" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Sharding</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR1385/api/ops" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Ops</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR1385/api/config" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Configuration</p><!--]--></a><!----></div><!----></div><section class="VPSidebarItem level-1 collapsible has-active" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" role="button" tabindex="0" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><h3 class="text" data-v-a4b0d9bf>MLIR Dialects</h3><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-a4b0d9bf><span class="vpi-chevron-right caret-icon" data-v-a4b0d9bf></span></div></div><div class="items" data-v-a4b0d9bf><!--[--><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR1385/api/dialects/arith" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>ArithOps</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR1385/api/dialects/affine" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Affine</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR1385/api/dialects/builtin" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Builtin</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR1385/api/dialects/chlo" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Chlo</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR1385/api/dialects/enzyme" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Enzyme</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR1385/api/dialects/enzymexla" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>EnzymeXLA</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR1385/api/dialects/func" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Func</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR1385/api/dialects/gpu" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>GPU</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR1385/api/dialects/llvm" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>LLVM</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR1385/api/dialects/mpi" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>MPI</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR1385/api/dialects/memref" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>MemRef</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR1385/api/dialects/nvvm" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>NVVM</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR1385/api/dialects/shardy" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Shardy</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR1385/api/dialects/sparsetensor" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>SparseTensor</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR1385/api/dialects/stablehlo" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>StableHLO</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR1385/api/dialects/triton" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Triton</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR1385/api/dialects/tpu" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>TPU</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR1385/api/dialects/vhlo" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>VHLO</p><!--]--></a><!----></div><!----></div><!--]--></div></section><section class="VPSidebarItem level-1 collapsible" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" role="button" tabindex="0" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><h3 class="text" data-v-a4b0d9bf>Low-Level API</h3><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-a4b0d9bf><span class="vpi-chevron-right caret-icon" data-v-a4b0d9bf></span></div></div><div class="items" data-v-a4b0d9bf><!--[--><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR1385/api/mlirc" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>MLIR API</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR1385/api/xla" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>XLA</p><!--]--></a><!----></div><!----></div><!--]--></div></section><div class="VPSidebarItem level-1 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR1385/api/internal" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Internal API</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><!--]--><!--[--><!--]--></nav></aside><div class="VPContent has-sidebar" id="VPContent" data-v-a9a9e638 data-v-91765379><div class="VPDoc has-sidebar has-aside" data-v-91765379 data-v-83890dd9><!--[--><!--]--><div class="container" data-v-83890dd9><div class="aside" data-v-83890dd9><div class="aside-curtain" data-v-83890dd9></div><div class="aside-container" data-v-83890dd9><div class="aside-content" data-v-83890dd9><div class="VPDocAside" data-v-83890dd9 data-v-6d7b3c46><!--[--><!--]--><!--[--><!--]--><nav aria-labelledby="doc-outline-aria-label" class="VPDocAsideOutline" data-v-6d7b3c46 data-v-b38bf2ff><div class="content" data-v-b38bf2ff><div class="outline-marker" data-v-b38bf2ff></div><div aria-level="2" class="outline-title" id="doc-outline-aria-label" role="heading" data-v-b38bf2ff>On this page</div><ul class="VPDocOutlineItem root" data-v-b38bf2ff data-v-3f927ebe><!--[--><!--]--></ul></div></nav><!--[--><!--]--><div class="spacer" data-v-6d7b3c46></div><!--[--><!--[--><!--[--><!--[--><!--[--><br><h2> Trusted by </h2><a class="enjoyer" href="https://lux.csail.mit.edu/" target="_blank"><img width="32" height="32" src="https://raw.githubusercontent.com/LuxDL/Lux.jl/refs/heads/main/assets/lux-logo.svg"><span><p class="extra-info">Scientific Computing</p><p class="heading">Lux.jl</p><p class="extra-info">Machine Learning</p></span></a><a class="enjoyer" href="https://bsc-quantic.github.io/Tenet.jl/stable/" target="_blank"><img width="32" height="32" src="https://raw.githubusercontent.com/bsc-quantic/Tenet.jl/refs/heads/master/docs/src/assets/logo.svg"><span><p class="extra-info">Quantum Simulation</p><p class="heading">Tenet.jl</p><p class="extra-info">Tensor Networks</p></span></a><!--]--><!--]--><!--]--><!--]--><!--]--><!----><!--[--><!--]--><!--[--><!--]--></div></div></div></div><div class="content" data-v-83890dd9><div class="content-container" data-v-83890dd9><!--[--><!--]--><main class="main" data-v-83890dd9><div style="position:relative;" class="vp-doc _Reactant_jl_previews_PR1385_api_dialects_sparsetensor" data-v-83890dd9><div><h1 id="SparseTensor-Dialect" tabindex="-1">SparseTensor Dialect <a class="header-anchor" href="#SparseTensor-Dialect" aria-label="Permalink to &quot;SparseTensor Dialect {#SparseTensor-Dialect}&quot;">​</a></h1><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.sparse_tensor.assemble-Tuple{Vector{Reactant.MLIR.IR.Value}, Reactant.MLIR.IR.Value}" href="#Reactant.MLIR.Dialects.sparse_tensor.assemble-Tuple{Vector{Reactant.MLIR.IR.Value}, Reactant.MLIR.IR.Value}"><span class="jlbinding">Reactant.MLIR.Dialects.sparse_tensor.assemble</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>assemble</code></p><p>Assembles the per-level position and coordinate arrays together with the values arrays into a sparse tensor. The order and types of the provided levels must be consistent with the actual storage layout of the returned sparse tensor described below.</p><ul><li><p><code>levels: [tensor&lt;? x iType&gt;, ...]</code> supplies the sparse tensor position and coordinate arrays of the sparse tensor for the corresponding level as specifed by <code>sparse_tensor::StorageLayout</code>.</p></li><li><p><code>values : tensor&lt;? x V&gt;</code> supplies the values array for the stored elements in the sparse tensor.</p></li></ul><p>This operation can be used to assemble a sparse tensor from an external source; e.g., by passing numpy arrays from Python. It is the user&#39;s responsibility to provide input that can be correctly interpreted by the sparsifier, which does not perform any sanity test to verify data integrity.</p><p><strong>Example</strong></p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>%pos    = arith.constant dense&lt;[0, 3]&gt;                : tensor&lt;2xindex&gt;</span></span>
<span class="line"><span>%index  = arith.constant dense&lt;[[0,0], [1,2], [1,3]]&gt; : tensor&lt;3x2xindex&gt;</span></span>
<span class="line"><span>%values = arith.constant dense&lt;[ 1.1,   2.2,   3.3 ]&gt; : tensor&lt;3xf64&gt;</span></span>
<span class="line"><span>%s = sparse_tensor.assemble (%pos, %index), %values</span></span>
<span class="line"><span>   : (tensor&lt;2xindex&gt;, tensor&lt;3x2xindex&gt;), tensor&lt;3xf64&gt; to tensor&lt;3x4xf64, #COO&gt;</span></span>
<span class="line"><span>// yields COO format |1.1, 0.0, 0.0, 0.0|</span></span>
<span class="line"><span>//     of 3x4 matrix |0.0, 0.0, 2.2, 3.3|</span></span>
<span class="line"><span>//                   |0.0, 0.0, 0.0, 0.0|</span></span></code></pre></div><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/9ea23073f744053d4d0215296dadcac5d8c3c434/src/mlir/Dialects/SparseTensor.jl#L202-L235" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.sparse_tensor.binary-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}" href="#Reactant.MLIR.Dialects.sparse_tensor.binary-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}"><span class="jlbinding">Reactant.MLIR.Dialects.sparse_tensor.binary</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>binary</code></p><p>Defines a computation within a <code>linalg.generic</code> operation that takes two operands and executes one of the regions depending on whether both operands or either operand is nonzero (i.e. stored explicitly in the sparse storage format).</p><p>Three regions are defined for the operation and must appear in this order:</p><ul><li><p>overlap (elements present in both sparse tensors)</p></li><li><p>left (elements only present in the left sparse tensor)</p></li><li><p>right (element only present in the right sparse tensor)</p></li></ul><p>Each region contains a single block describing the computation and result. Every non-empty block must end with a sparse_tensor.yield and the return type must match the type of <code>output</code>. The primary region&#39;s block has two arguments, while the left and right region&#39;s block has only one argument.</p><p>A region may also be declared empty (i.e. <code>left={}</code>), indicating that the region does not contribute to the output. For example, setting both <code>left={}</code> and <code>right={}</code> is equivalent to the intersection of the two inputs as only the overlap region will contribute values to the output.</p><p>As a convenience, there is also a special token <code>identity</code> which can be used in place of the left or right region. This token indicates that the return value is the input value (i.e. func(%x) =&gt; return %x). As a practical example, setting <code>left=identity</code> and <code>right=identity</code> would be equivalent to a union operation where non-overlapping values in the inputs are copied to the output unchanged.</p><p>Due to the possibility of empty regions, i.e. lack of a value for certain cases, the result of this operation may only feed directly into the output of the <code>linalg.generic</code> operation or into into a custom reduction <code>sparse_tensor.reduce</code> operation that follows in the same region.</p><p>Example of isEqual applied to intersecting elements only:</p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>%C = tensor.empty(...)</span></span>
<span class="line"><span>%0 = linalg.generic #trait</span></span>
<span class="line"><span>  ins(%A: tensor&lt;?xf64, #SparseVector&gt;,</span></span>
<span class="line"><span>      %B: tensor&lt;?xf64, #SparseVector&gt;)</span></span>
<span class="line"><span>  outs(%C: tensor&lt;?xi8, #SparseVector&gt;) {</span></span>
<span class="line"><span>  ^bb0(%a: f64, %b: f64, %c: i8) :</span></span>
<span class="line"><span>    %result = sparse_tensor.binary %a, %b : f64, f64 to i8</span></span>
<span class="line"><span>      overlap={</span></span>
<span class="line"><span>        ^bb0(%arg0: f64, %arg1: f64):</span></span>
<span class="line"><span>          %cmp = arith.cmpf &quot;oeq&quot;, %arg0, %arg1 : f64</span></span>
<span class="line"><span>          %ret_i8 = arith.extui %cmp : i1 to i8</span></span>
<span class="line"><span>          sparse_tensor.yield %ret_i8 : i8</span></span>
<span class="line"><span>      }</span></span>
<span class="line"><span>      left={}</span></span>
<span class="line"><span>      right={}</span></span>
<span class="line"><span>    linalg.yield %result : i8</span></span>
<span class="line"><span>} -&gt; tensor&lt;?xi8, #SparseVector&gt;</span></span></code></pre></div><p>Example of A+B in upper triangle, A-B in lower triangle:</p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>%C = tensor.empty(...)</span></span>
<span class="line"><span>%1 = linalg.generic #trait</span></span>
<span class="line"><span>  ins(%A: tensor&lt;?x?xf64, #CSR&gt;, %B: tensor&lt;?x?xf64, #CSR&gt;</span></span>
<span class="line"><span>  outs(%C: tensor&lt;?x?xf64, #CSR&gt; {</span></span>
<span class="line"><span>  ^bb0(%a: f64, %b: f64, %c: f64) :</span></span>
<span class="line"><span>    %row = linalg.index 0 : index</span></span>
<span class="line"><span>    %col = linalg.index 1 : index</span></span>
<span class="line"><span>    %result = sparse_tensor.binary %a, %b : f64, f64 to f64</span></span>
<span class="line"><span>      overlap={</span></span>
<span class="line"><span>        ^bb0(%x: f64, %y: f64):</span></span>
<span class="line"><span>          %cmp = arith.cmpi &quot;uge&quot;, %col, %row : index</span></span>
<span class="line"><span>          %upperTriangleResult = arith.addf %x, %y : f64</span></span>
<span class="line"><span>          %lowerTriangleResult = arith.subf %x, %y : f64</span></span>
<span class="line"><span>          %ret = arith.select %cmp, %upperTriangleResult, %lowerTriangleResult : f64</span></span>
<span class="line"><span>          sparse_tensor.yield %ret : f64</span></span>
<span class="line"><span>      }</span></span>
<span class="line"><span>      left=identity</span></span>
<span class="line"><span>      right={</span></span>
<span class="line"><span>        ^bb0(%y: f64):</span></span>
<span class="line"><span>          %cmp = arith.cmpi &quot;uge&quot;, %col, %row : index</span></span>
<span class="line"><span>          %lowerTriangleResult = arith.negf %y : f64</span></span>
<span class="line"><span>          %ret = arith.select %cmp, %y, %lowerTriangleResult : f64</span></span>
<span class="line"><span>          sparse_tensor.yield %ret : f64</span></span>
<span class="line"><span>      }</span></span>
<span class="line"><span>    linalg.yield %result : f64</span></span>
<span class="line"><span>} -&gt; tensor&lt;?x?xf64, #CSR&gt;</span></span></code></pre></div><p>Example of set difference. Returns a copy of A where its sparse structure is <em>not</em> overlapped by B. The element type of B can be different than A because we never use its values, only its sparse structure:</p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>%C = tensor.empty(...)</span></span>
<span class="line"><span>%2 = linalg.generic #trait</span></span>
<span class="line"><span>  ins(%A: tensor&lt;?x?xf64, #CSR&gt;, %B: tensor&lt;?x?xi32, #CSR&gt;</span></span>
<span class="line"><span>  outs(%C: tensor&lt;?x?xf64, #CSR&gt; {</span></span>
<span class="line"><span>  ^bb0(%a: f64, %b: i32, %c: f64) :</span></span>
<span class="line"><span>    %result = sparse_tensor.binary %a, %b : f64, i32 to f64</span></span>
<span class="line"><span>      overlap={}</span></span>
<span class="line"><span>      left=identity</span></span>
<span class="line"><span>      right={}</span></span>
<span class="line"><span>    linalg.yield %result : f64</span></span>
<span class="line"><span>} -&gt; tensor&lt;?x?xf64, #CSR&gt;</span></span></code></pre></div><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/9ea23073f744053d4d0215296dadcac5d8c3c434/src/mlir/Dialects/SparseTensor.jl#L257-L362" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.sparse_tensor.coiterate-Tuple{Vector{Reactant.MLIR.IR.Value}, Vector{Reactant.MLIR.IR.Value}}" href="#Reactant.MLIR.Dialects.sparse_tensor.coiterate-Tuple{Vector{Reactant.MLIR.IR.Value}, Vector{Reactant.MLIR.IR.Value}}"><span class="jlbinding">Reactant.MLIR.Dialects.sparse_tensor.coiterate</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>coiterate</code></p><p>The <code>sparse_tensor.coiterate</code> operation represents a loop (nest) over a set of iteration spaces. The operation can have multiple regions, with each of them defining a case to compute a result at the current iterations. The case condition is defined solely based on the pattern of specified iterators. For example:</p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>%ret = sparse_tensor.coiterate (%sp1, %sp2) at(%coord) iter_args(%arg = %init)</span></span>
<span class="line"><span>     : (!sparse_tensor.iter_space&lt;#CSR, lvls = 0&gt;,</span></span>
<span class="line"><span>        !sparse_tensor.iter_space&lt;#COO, lvls = 0&gt;)</span></span>
<span class="line"><span>     -&gt; index</span></span>
<span class="line"><span>case %it1, _ {</span></span>
<span class="line"><span>  // %coord is specifed in space %sp1 but *NOT* specified in space %sp2.</span></span>
<span class="line"><span>}</span></span>
<span class="line"><span>case %it1, %it2 {</span></span>
<span class="line"><span>  // %coord is specifed in *BOTH* spaces %sp1 and %sp2.</span></span>
<span class="line"><span>}</span></span></code></pre></div><p><code>sparse_tensor.coiterate</code> can also operate on loop-carried variables. It returns the final value for each loop-carried variable after loop termination. The initial values of the variables are passed as additional SSA operands to the iterator SSA value and used coordinate SSA values. Each operation region has variadic arguments for specified (used), one argument for each loop-carried variable, representing the value of the variable at the current iteration, followed by a list of arguments for iterators. The body region must contain exactly one block that terminates with <code>sparse_tensor.yield</code>.</p><p>The results of an <code>sparse_tensor.coiterate</code> hold the final values after the last iteration. If the <code>sparse_tensor.coiterate</code> defines any values, a yield must be explicitly present in every region defined in the operation. The number and types of the <code>sparse_tensor.coiterate</code> results must match the initial values in the iter_args binding and the yield operands.</p><p>A <code>sparse_tensor.coiterate</code> example that does elementwise addition between two sparse vectors.</p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>%ret = sparse_tensor.coiterate (%sp1, %sp2) at(%coord) iter_args(%arg = %init)</span></span>
<span class="line"><span>     : (!sparse_tensor.iter_space&lt;#CSR, lvls = 0&gt;,</span></span>
<span class="line"><span>        !sparse_tensor.iter_space&lt;#CSR, lvls = 0&gt;)</span></span>
<span class="line"><span>     -&gt; tensor&lt;?xindex, #CSR&gt;</span></span>
<span class="line"><span>case %it1, _ {</span></span>
<span class="line"><span>   // v = v1 + 0 = v1</span></span>
<span class="line"><span>   %v1 = sparse_tensor.extract_value %t1 at %it1 : index</span></span>
<span class="line"><span>   %yield = sparse_tensor.insert %v1 into %arg[%coord]</span></span>
<span class="line"><span>   sparse_tensor.yield %yield</span></span>
<span class="line"><span>}</span></span>
<span class="line"><span>case _, %it2 {</span></span>
<span class="line"><span>   // v = v2 + 0 = v2</span></span>
<span class="line"><span>   %v2 = sparse_tensor.extract_value %t2 at %it2 : index</span></span>
<span class="line"><span>   %yield = sparse_tensor.insert %v1 into %arg[%coord]</span></span>
<span class="line"><span>   sparse_tensor.yield %yield</span></span>
<span class="line"><span>}</span></span>
<span class="line"><span>case %it1, %it2 {</span></span>
<span class="line"><span>   // v = v1 + v2</span></span>
<span class="line"><span>   %v1 = sparse_tensor.extract_value %t1 at %it1 : index</span></span>
<span class="line"><span>   %v2 = sparse_tensor.extract_value %t2 at %it2 : index</span></span>
<span class="line"><span>   %v = arith.addi %v1, %v2 : index</span></span>
<span class="line"><span>   %yield = sparse_tensor.insert %v into %arg[%coord]</span></span>
<span class="line"><span>   sparse_tensor.yield %yield</span></span>
<span class="line"><span>}</span></span></code></pre></div><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/9ea23073f744053d4d0215296dadcac5d8c3c434/src/mlir/Dialects/SparseTensor.jl#L396-L464" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.sparse_tensor.compress-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value, Vector{Reactant.MLIR.IR.Value}}" href="#Reactant.MLIR.Dialects.sparse_tensor.compress-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value, Vector{Reactant.MLIR.IR.Value}}"><span class="jlbinding">Reactant.MLIR.Dialects.sparse_tensor.compress</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>compress</code></p><p>Finishes a single access pattern expansion by moving inserted elements into the sparse storage scheme of the given tensor with the given level-coordinates. The arity of <code>lvlCoords</code> is one less than the level-rank of the tensor, with the coordinate of the innermost level defined through the <code>added</code> array. The <code>values</code> and <code>filled</code> arrays are reset in a <em>sparse</em> fashion by only iterating over set elements through an indirection using the <code>added</code> array, so that the operations are kept proportional to the number of nonzeros. See the <code>sparse_tensor.expand</code> operation for more details.</p><p>Note that this operation is &quot;impure&quot; in the sense that even though the result is modeled through an SSA value, the insertion is eventually done &quot;in place&quot;, and referencing the old SSA value is undefined behavior.</p><p><strong>Example</strong></p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>%result = sparse_tensor.compress %values, %filled, %added, %count into %tensor[%i]</span></span>
<span class="line"><span>  : memref&lt;?xf64&gt;, memref&lt;?xi1&gt;, memref&lt;?xindex&gt;, tensor&lt;4x4xf64, #CSR&gt;</span></span></code></pre></div><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/9ea23073f744053d4d0215296dadcac5d8c3c434/src/mlir/Dialects/SparseTensor.jl#L495-L518" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.sparse_tensor.concatenate-Tuple{Vector{Reactant.MLIR.IR.Value}}" href="#Reactant.MLIR.Dialects.sparse_tensor.concatenate-Tuple{Vector{Reactant.MLIR.IR.Value}}"><span class="jlbinding">Reactant.MLIR.Dialects.sparse_tensor.concatenate</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>concatenate</code></p><p>Concatenates a list input tensors and the output tensor with the same dimension-rank. The concatenation happens on the specified <code>dimension</code> (0 &lt;= dimension &lt; dimRank). The resulting <code>dimension</code> size is the sum of all the input sizes for that dimension, while all the other dimensions should have the same size in the input and output tensors.</p><p>Only statically-sized input tensors are accepted, while the output tensor can be dynamically-sized.</p><p><strong>Example</strong></p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>%0 = sparse_tensor.concatenate %1, %2 { dimension = 0 : index }</span></span>
<span class="line"><span>  : tensor&lt;64x64xf64, #CSR&gt;, tensor&lt;64x64xf64, #CSR&gt; to tensor&lt;128x64xf64, #CSR&gt;</span></span></code></pre></div><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/9ea23073f744053d4d0215296dadcac5d8c3c434/src/mlir/Dialects/SparseTensor.jl#L548-L566" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.sparse_tensor.convert-Tuple{Reactant.MLIR.IR.Value}" href="#Reactant.MLIR.Dialects.sparse_tensor.convert-Tuple{Reactant.MLIR.IR.Value}"><span class="jlbinding">Reactant.MLIR.Dialects.sparse_tensor.convert</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>convert</code></p><p>Converts one sparse or dense tensor type to another tensor type. The rank of the source and destination types must match exactly, and the dimension sizes must either match exactly or relax from a static to a dynamic size. The sparse encoding of the two types can obviously be completely different. The name <code>convert</code> was preferred over <code>cast</code>, since the operation may incur a non-trivial cost.</p><p>When converting between two different sparse tensor types, only explicitly stored values are moved from one underlying sparse storage format to the other. When converting from an unannotated dense tensor type to a sparse tensor type, an explicit test for nonzero values is used. When converting to an unannotated dense tensor type, implicit zeroes in the sparse storage format are made explicit. Note that the conversions can have non-trivial costs associated with them, since they may involve elaborate data structure transformations. Also, conversions from sparse tensor types into dense tensor types may be infeasible in terms of storage requirements.</p><p>Trivial dense-to-dense convert will be removed by canonicalization while trivial sparse-to-sparse convert will be removed by the sparse codegen. This is because we use trivial sparse-to-sparse convert to tell bufferization that the sparse codegen will expand the tensor buffer into sparse tensor storage.</p><p>Examples:</p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>%0 = sparse_tensor.convert %a : tensor&lt;32x32xf32&gt; to tensor&lt;32x32xf32, #CSR&gt;</span></span>
<span class="line"><span>%1 = sparse_tensor.convert %a : tensor&lt;32x32xf32&gt; to tensor&lt;?x?xf32, #CSR&gt;</span></span>
<span class="line"><span>%2 = sparse_tensor.convert %b : tensor&lt;8x8xi32, #CSC&gt; to tensor&lt;8x8xi32, #CSR&gt;</span></span>
<span class="line"><span>%3 = sparse_tensor.convert %c : tensor&lt;4x8xf64, #CSR&gt; to tensor&lt;4x?xf64, #CSC&gt;</span></span>
<span class="line"><span></span></span>
<span class="line"><span>// The following conversion is not allowed (since it would require a</span></span>
<span class="line"><span>// runtime assertion that the source&#39;s dimension size is actually 100).</span></span>
<span class="line"><span>%4 = sparse_tensor.convert %d : tensor&lt;?xf64&gt; to tensor&lt;100xf64, #SV&gt;</span></span></code></pre></div><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/9ea23073f744053d4d0215296dadcac5d8c3c434/src/mlir/Dialects/SparseTensor.jl#L586-L624" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.sparse_tensor.coordinates-Tuple{Reactant.MLIR.IR.Value}" href="#Reactant.MLIR.Dialects.sparse_tensor.coordinates-Tuple{Reactant.MLIR.IR.Value}"><span class="jlbinding">Reactant.MLIR.Dialects.sparse_tensor.coordinates</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>coordinates</code></p><p>Returns the coordinates array of the tensor&#39;s storage at the given level. This is similar to the <code>bufferization.to_buffer</code> operation in the sense that it provides a bridge between a tensor world view and a bufferized world view. Unlike the <code>bufferization.to_buffer</code> operation, however, this sparse operation actually lowers into code that extracts the coordinates array from the sparse storage itself (either by calling a support library or through direct code).</p><p>Writing into the result of this operation is undefined behavior.</p><p><strong>Example</strong></p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>%1 = sparse_tensor.coordinates %0 { level = 1 : index }</span></span>
<span class="line"><span>   : tensor&lt;64x64xf64, #CSR&gt; to memref&lt;?xindex&gt;</span></span></code></pre></div><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/9ea23073f744053d4d0215296dadcac5d8c3c434/src/mlir/Dialects/SparseTensor.jl#L1744-L1763" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.sparse_tensor.coordinates_buffer-Tuple{Reactant.MLIR.IR.Value}" href="#Reactant.MLIR.Dialects.sparse_tensor.coordinates_buffer-Tuple{Reactant.MLIR.IR.Value}"><span class="jlbinding">Reactant.MLIR.Dialects.sparse_tensor.coordinates_buffer</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>coordinates_buffer</code></p><p>Returns the linear coordinates array for a sparse tensor with a trailing COO region with at least two levels. It is an error if the tensor doesn&#39;t contain such a COO region. This is similar to the <code>bufferization.to_buffer</code> operation in the sense that it provides a bridge between a tensor world view and a bufferized world view. Unlike the <code>bufferization.to_buffer</code> operation, however, this operation actually lowers into code that extracts the linear coordinates array from the sparse storage scheme that stores the coordinates for the COO region as an array of structures. For example, a 2D COO sparse tensor with two non-zero elements at coordinates (1, 3) and (4, 6) are stored in a linear buffer as (1, 4, 3, 6) instead of two buffer as (1, 4) and (3, 6).</p><p>Writing into the result of this operation is undefined behavior.</p><p><strong>Example</strong></p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>%1 = sparse_tensor.coordinates_buffer %0</span></span>
<span class="line"><span>   : tensor&lt;64x64xf64, #COO&gt; to memref&lt;?xindex&gt;</span></span></code></pre></div><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/9ea23073f744053d4d0215296dadcac5d8c3c434/src/mlir/Dialects/SparseTensor.jl#L1697-L1721" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.sparse_tensor.crd_translate-Tuple{Vector{Reactant.MLIR.IR.Value}}" href="#Reactant.MLIR.Dialects.sparse_tensor.crd_translate-Tuple{Vector{Reactant.MLIR.IR.Value}}"><span class="jlbinding">Reactant.MLIR.Dialects.sparse_tensor.crd_translate</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>crd_translate</code></p><p>Performs coordinate translation between level and dimension coordinate space according to the affine maps defined by encoder.</p><p><strong>Example</strong></p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>%l0, %l1, %l2, %l3 = sparse_tensor.crd_translate dim_to_lvl [%d0, %d1] as #BSR</span></span>
<span class="line"><span>                   : index, index, index, index</span></span></code></pre></div><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/9ea23073f744053d4d0215296dadcac5d8c3c434/src/mlir/Dialects/SparseTensor.jl#L644-L656" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.sparse_tensor.disassemble-Tuple{Reactant.MLIR.IR.Value, Vector{Reactant.MLIR.IR.Value}, Reactant.MLIR.IR.Value}" href="#Reactant.MLIR.Dialects.sparse_tensor.disassemble-Tuple{Reactant.MLIR.IR.Value, Vector{Reactant.MLIR.IR.Value}, Reactant.MLIR.IR.Value}"><span class="jlbinding">Reactant.MLIR.Dialects.sparse_tensor.disassemble</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>disassemble</code></p><p>The disassemble operation is the inverse of <code>sparse_tensor::assemble</code>. It copies the per-level position and coordinate arrays together with the values array of the given sparse tensor into the user-supplied buffers along with the actual length of the memory used in each returned buffer.</p><p>This operation can be used for returning a disassembled MLIR sparse tensor; e.g., copying the sparse tensor contents into pre-allocated numpy arrays back to Python. It is the user&#39;s responsibility to allocate large enough buffers of the appropriate types to hold the sparse tensor contents. The sparsifier simply copies all fields of the sparse tensor into the user-supplied buffers without any sanity test to verify data integrity.</p><p><strong>Example</strong></p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>// input COO format |1.1, 0.0, 0.0, 0.0|</span></span>
<span class="line"><span>//    of 3x4 matrix |0.0, 0.0, 2.2, 3.3|</span></span>
<span class="line"><span>//                  |0.0, 0.0, 0.0, 0.0|</span></span>
<span class="line"><span>%p, %c, %v, %p_len, %c_len, %v_len =</span></span>
<span class="line"><span>  sparse_tensor.disassemble %s : tensor&lt;3x4xf64, #COO&gt;</span></span>
<span class="line"><span>     out_lvls(%op, %oi : tensor&lt;2xindex&gt;, tensor&lt;3x2xindex&gt;)</span></span>
<span class="line"><span>     out_vals(%od : tensor&lt;3xf64&gt;) -&gt;</span></span>
<span class="line"><span>       (tensor&lt;2xindex&gt;, tensor&lt;3x2xindex&gt;), tensor&lt;3xf64&gt;, (index, index), index</span></span>
<span class="line"><span>// %p = arith.constant dense&lt;[ 0,              3 ]&gt; : tensor&lt;2xindex&gt;</span></span>
<span class="line"><span>// %c = arith.constant dense&lt;[[0,0], [1,2], [1,3]]&gt; : tensor&lt;3x2xindex&gt;</span></span>
<span class="line"><span>// %v = arith.constant dense&lt;[ 1.1,   2.2,   3.3 ]&gt; : tensor&lt;3xf64&gt;</span></span>
<span class="line"><span>// %p_len = 2</span></span>
<span class="line"><span>// %c_len = 6 (3x2)</span></span>
<span class="line"><span>// %v_len = 3</span></span></code></pre></div><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/9ea23073f744053d4d0215296dadcac5d8c3c434/src/mlir/Dialects/SparseTensor.jl#L684-L717" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.sparse_tensor.expand-Tuple{Reactant.MLIR.IR.Value}" href="#Reactant.MLIR.Dialects.sparse_tensor.expand-Tuple{Reactant.MLIR.IR.Value}"><span class="jlbinding">Reactant.MLIR.Dialects.sparse_tensor.expand</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>expand</code></p><p>Performs an access pattern expansion for the innermost levels of the given tensor. This operation is useful to implement kernels in which a sparse tensor appears as output. This technique is known under several different names and using several alternative implementations, for example, phase counter [Gustavson72], expanded or switch array [Pissanetzky84], in phase scan [Duff90], access pattern expansion [Bik96], and workspaces [Kjolstad19].</p><p>The <code>values</code> and <code>filled</code> arrays must have lengths equal to the level-size of the innermost level (i.e., as if the innermost level were <em>dense</em>). The <code>added</code> array and <code>count</code> are used to store new level-coordinates when a false value is encountered in the <code>filled</code> array. All arrays should be allocated before the loop (possibly even shared between loops in a future optimization) so that their <em>dense</em> initialization can be amortized over many iterations. Setting and resetting the dense arrays in the loop nest itself is kept <em>sparse</em> by only iterating over set elements through an indirection using the added array, so that the operations are kept proportional to the number of nonzeros.</p><p>Note that this operation is &quot;impure&quot; in the sense that even though the results are modeled through SSA values, the operation relies on a proper side-effecting context that sets and resets the expanded arrays.</p><p><strong>Example</strong></p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>%values, %filled, %added, %count = sparse_tensor.expand %tensor</span></span>
<span class="line"><span>  : tensor&lt;4x4xf64, #CSR&gt; to memref&lt;?xf64&gt;, memref&lt;?xi1&gt;, memref&lt;?xindex&gt;</span></span></code></pre></div><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/9ea23073f744053d4d0215296dadcac5d8c3c434/src/mlir/Dialects/SparseTensor.jl#L746-L779" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.sparse_tensor.extract_iteration_space" href="#Reactant.MLIR.Dialects.sparse_tensor.extract_iteration_space"><span class="jlbinding">Reactant.MLIR.Dialects.sparse_tensor.extract_iteration_space</span></a> <span class="VPBadge info jlObjectType jlFunction"><!--[-->Function<!--]--></span></summary><p><code>extract_iteration_space</code></p><p>Extracts a <code>!sparse_tensor.iter_space</code> from a sparse tensor between certain (consecutive) levels. For sparse levels, it is usually done by loading a postion range from the underlying sparse tensor storage. E.g., for a compressed level, the iteration space is extracted by [pos[i], pos[i+1]) supposing the the parent iterator points at <code>i</code>.</p><p><code>tensor</code>: the input sparse tensor that defines the iteration space. <code>parentIter</code>: the iterator for the previous level, at which the iteration space at the current levels will be extracted. <code>loLvl</code>, <code>hiLvl</code>: the level range between [loLvl, hiLvl) in the input tensor that the returned iteration space covers. <code>hiLvl - loLvl</code> defines the dimension of the iteration space.</p><p>The type of returned the value is must be <code>!sparse_tensor.iter_space&lt;#INPUT_ENCODING, lvls = $loLvl to $hiLvl&gt;</code>. The returned iteration space can then be iterated over by <code>sparse_tensor.iterate</code> operations to visit every stored element (usually nonzeros) in the input sparse tensor.</p><p><strong>Example</strong></p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>// Extracts a 1-D iteration space from a COO tensor at level 1.</span></span>
<span class="line"><span>%space = sparse_tensor.iteration.extract_space %sp at %it1 lvls = 1</span></span>
<span class="line"><span>  : tensor&lt;4x8xf32, #COO&gt;, !sparse_tensor.iterator&lt;#COO, lvls = 0&gt;</span></span>
<span class="line"><span> -&gt;!sparse_tensor.iter_space&lt;#COO, lvls = 1&gt;</span></span></code></pre></div><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/9ea23073f744053d4d0215296dadcac5d8c3c434/src/mlir/Dialects/SparseTensor.jl#L16-L45" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.sparse_tensor.extract_value-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}" href="#Reactant.MLIR.Dialects.sparse_tensor.extract_value-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}"><span class="jlbinding">Reactant.MLIR.Dialects.sparse_tensor.extract_value</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>extract_value</code></p><p>The <code>sparse_tensor.extract_value</code> operation extracts the value pointed to by a sparse iterator from a sparse tensor.</p><p><strong>Example</strong></p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>%val = sparse_tensor.extract_value %sp at %it</span></span>
<span class="line"><span>     : tensor&lt;?x?xf32, #CSR&gt;, !sparse_tensor.iterator&lt;#CSR, lvl = 1&gt;</span></span></code></pre></div><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/9ea23073f744053d4d0215296dadcac5d8c3c434/src/mlir/Dialects/SparseTensor.jl#L76-L88" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.sparse_tensor.foreach-Tuple{Reactant.MLIR.IR.Value, Vector{Reactant.MLIR.IR.Value}}" href="#Reactant.MLIR.Dialects.sparse_tensor.foreach-Tuple{Reactant.MLIR.IR.Value, Vector{Reactant.MLIR.IR.Value}}"><span class="jlbinding">Reactant.MLIR.Dialects.sparse_tensor.foreach</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>foreach</code></p><p>Iterates over stored elements in a tensor (which are typically, but not always, non-zero for sparse tensors) and executes the block.</p><p><code>tensor</code>: the input tensor to iterate over. <code>initArgs</code>: the initial loop argument to carry and update during each iteration. <code>order</code>: an optional permutation affine map that specifies the order in which the dimensions are visited (e.g., row first or column first). This is only applicable when the input tensor is a non-annotated dense tensor.</p><p>For an input tensor with dim-rank <code>n</code>, the block must take <code>n + 1</code> arguments (plus additional loop-carried variables as described below). The first <code>n</code> arguments provide the dimension-coordinates of the element being visited, and must all have <code>index</code> type. The <code>(n+1)</code>-th argument provides the element&#39;s value, and must have the tensor&#39;s element type.</p><p><code>sparse_tensor.foreach</code> can also operate on loop-carried variables and returns the final values after loop termination. The initial values of the variables are passed as additional SSA operands to the &quot;sparse_tensor.foreach&quot; following the n + 1 SSA values mentioned above (n coordinates and 1 value).</p><p>The region must terminate with a &quot;sparse_tensor.yield&quot; that passes the current values of all loop-carried variables to the next iteration, or to the result, if at the last iteration. The number and static types of loop-carried variables may not change with iterations.</p><p>For example:</p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>%c0 = arith.constant 0 : i32</span></span>
<span class="line"><span>%ret = sparse_tensor.foreach in %0 init(%c0): tensor&lt;?x?xi32, #DCSR&gt;, i32 -&gt; i32 do {</span></span>
<span class="line"><span> ^bb0(%arg1: index, %arg2: index, %arg3: i32, %iter: i32):</span></span>
<span class="line"><span>   %sum = arith.add %iter, %arg3</span></span>
<span class="line"><span>   sparse_tensor.yield %sum</span></span>
<span class="line"><span>}</span></span></code></pre></div><p>It is important to note that the generated loop iterates over elements in their storage order. However, regardless of the storage scheme used by the tensor, the block is always given the dimension-coordinates.</p><p>For example:</p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>#COL_MAJOR = #sparse_tensor.encoding&lt;{</span></span>
<span class="line"><span>  map = (d0, d1) -&gt; (d1 : compressed, d0 : compressed)</span></span>
<span class="line"><span>}&gt;</span></span>
<span class="line"><span></span></span>
<span class="line"><span>// foreach on a column-major sparse tensor</span></span>
<span class="line"><span>sparse_tensor.foreach in %0 : tensor&lt;2x3xf64, #COL_MAJOR&gt; do {</span></span>
<span class="line"><span> ^bb0(%row: index, %col: index, %arg3: f64):</span></span>
<span class="line"><span>    // [%row, %col] -&gt; [0, 0], [1, 0], [2, 0], [0, 1], [1, 1], [2, 1]</span></span>
<span class="line"><span>}</span></span>
<span class="line"><span></span></span>
<span class="line"><span>#ROW_MAJOR = #sparse_tensor.encoding&lt;{</span></span>
<span class="line"><span>  map = (d0, d1) -&gt; (d0 : compressed, d1 : compressed)</span></span>
<span class="line"><span>}&gt;</span></span>
<span class="line"><span></span></span>
<span class="line"><span>// foreach on a row-major sparse tensor</span></span>
<span class="line"><span>sparse_tensor.foreach in %0 : tensor&lt;2x3xf64, #ROW_MAJOR&gt; do {</span></span>
<span class="line"><span> ^bb0(%row: index, %col: index, %arg3: f64):</span></span>
<span class="line"><span>    // [%row, %col] -&gt; [0, 0], [0, 1], [1, 0], [1, 1], [2, 0], [2, 1]</span></span>
<span class="line"><span>}</span></span>
<span class="line"><span></span></span>
<span class="line"><span>// foreach on a row-major dense tensor but visit column first</span></span>
<span class="line"><span>sparse_tensor.foreach in %0 {order=affine_map&lt;(i,j)-&gt;(j,i)&gt;}: tensor&lt;2x3xf64&gt; do {</span></span>
<span class="line"><span> ^bb0(%row: index, %col: index, %arg3: f64):</span></span>
<span class="line"><span>    // [%row, %col] -&gt; [0, 0], [1, 0], [2, 0], [0, 1], [1, 1], [2, 1]</span></span>
<span class="line"><span>}</span></span></code></pre></div><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/9ea23073f744053d4d0215296dadcac5d8c3c434/src/mlir/Dialects/SparseTensor.jl#L806-L878" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.sparse_tensor.has_runtime_library-Tuple{}" href="#Reactant.MLIR.Dialects.sparse_tensor.has_runtime_library-Tuple{}"><span class="jlbinding">Reactant.MLIR.Dialects.sparse_tensor.has_runtime_library</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>has_runtime_library</code></p><p>Returns a boolean value that indicates whether the sparsifier runs in runtime library mode or not. For testing only! This operation is useful for writing test cases that require different code depending on runtime/codegen mode.</p><p><strong>Example</strong></p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>%has_runtime = sparse_tensor.has_runtime_library</span></span>
<span class="line"><span>scf.if %has_runtime {</span></span>
<span class="line"><span>  ...</span></span>
<span class="line"><span>}</span></span></code></pre></div><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/9ea23073f744053d4d0215296dadcac5d8c3c434/src/mlir/Dialects/SparseTensor.jl#L945-L961" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.sparse_tensor.iterate-Tuple{Reactant.MLIR.IR.Value, Vector{Reactant.MLIR.IR.Value}}" href="#Reactant.MLIR.Dialects.sparse_tensor.iterate-Tuple{Reactant.MLIR.IR.Value, Vector{Reactant.MLIR.IR.Value}}"><span class="jlbinding">Reactant.MLIR.Dialects.sparse_tensor.iterate</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>iterate</code></p><p>The <code>sparse_tensor.iterate</code> operation represents a loop (nest) over the provided iteration space extracted from a specific sparse tensor. The operation defines an SSA value for a sparse iterator that points to the current stored element in the sparse tensor and SSA values for coordinates of the stored element. The coordinates are always converted to <code>index</code> type despite of the underlying sparse tensor storage. When coordinates are not used, the SSA values can be skipped by <code>_</code> symbols, which usually leads to simpler generated code after sparsification. For example:</p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>// The coordinate for level 0 is not used when iterating over a 2-D</span></span>
<span class="line"><span>// iteration space.</span></span>
<span class="line"><span>%sparse_tensor.iterate %iterator in %space at(_, %crd_1)</span></span>
<span class="line"><span>  : !sparse_tensor.iter_space&lt;#CSR, lvls = 0 to 2&gt;</span></span></code></pre></div><p><code>sparse_tensor.iterate</code> can also operate on loop-carried variables. It returns the final values after loop termination. The initial values of the variables are passed as additional SSA operands to the iterator SSA value and used coordinate SSA values mentioned above. The operation region has an argument for the iterator, variadic arguments for specified (used) coordiates and followed by one argument for each loop-carried variable, representing the value of the variable at the current iteration. The body region must contain exactly one block that terminates with <code>sparse_tensor.yield</code>.</p><p>The results of an <code>sparse_tensor.iterate</code> hold the final values after the last iteration. If the <code>sparse_tensor.iterate</code> defines any values, a yield must be explicitly present. The number and types of the <code>sparse_tensor.iterate</code> results must match the initial values in the iter_args binding and the yield operands.</p><p>A nested <code>sparse_tensor.iterate</code> example that prints all the coordinates stored in the sparse input:</p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>func.func @nested_iterate(%sp : tensor&lt;4x8xf32, #COO&gt;) {</span></span>
<span class="line"><span>  // Iterates over the first level of %sp</span></span>
<span class="line"><span>  %l1 = sparse_tensor.extract_iteration_space %sp lvls = 0</span></span>
<span class="line"><span>      : tensor&lt;4x8xf32, #COO&gt; -&gt; !sparse_tensor.iter_space&lt;#COO, lvls = 0 to 1&gt;</span></span>
<span class="line"><span>  %r1 = sparse_tensor.iterate %it1 in %l1 at (%coord0)</span></span>
<span class="line"><span>      : !sparse_tensor.iter_space&lt;#COO, lvls = 0 to 1&gt;  {</span></span>
<span class="line"><span>    // Iterates over the second level of %sp</span></span>
<span class="line"><span>    %l2 = sparse_tensor.extract_iteration_space %sp at %it1 lvls = 1</span></span>
<span class="line"><span>        : tensor&lt;4x8xf32, #COO&gt;, !sparse_tensor.iterator&lt;#COO, lvls = 0 to 1&gt;</span></span>
<span class="line"><span>       -&gt; !sparse_tensor.iter_space&lt;#COO, lvls = 1 to 2&gt;</span></span>
<span class="line"><span>    %r2 = sparse_tensor.iterate %it2 in %l2 at (coord1)</span></span>
<span class="line"><span>        : !sparse_tensor.iter_space&lt;#COO, lvls = 1 to 2&gt;  {</span></span>
<span class="line"><span>       vector.print %coord0 : index</span></span>
<span class="line"><span>       vector.print %coord1 : index</span></span>
<span class="line"><span>    }</span></span>
<span class="line"><span>  }</span></span>
<span class="line"><span>}</span></span></code></pre></div><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/9ea23073f744053d4d0215296dadcac5d8c3c434/src/mlir/Dialects/SparseTensor.jl#L114-L175" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.sparse_tensor.load-Tuple{Reactant.MLIR.IR.Value}" href="#Reactant.MLIR.Dialects.sparse_tensor.load-Tuple{Reactant.MLIR.IR.Value}"><span class="jlbinding">Reactant.MLIR.Dialects.sparse_tensor.load</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>load</code></p><p>Rematerializes a tensor from the underlying sparse storage format of the given tensor. This is similar to the <code>bufferization.to_tensor</code> operation in the sense that it provides a bridge between a bufferized world view and a tensor world view. Unlike the <code>bufferization.to_tensor</code> operation, however, this sparse operation is used only temporarily to maintain a correctly typed intermediate representation during progressive bufferization.</p><p>The <code>hasInserts</code> attribute denote whether insertions to the underlying sparse storage format may have occurred, in which case the underlying sparse storage format needs to be finalized. Otherwise, the operation simply folds away.</p><p>Note that this operation is &quot;impure&quot; in the sense that even though the result is modeled through an SSA value, the operation relies on a proper context of materializing and inserting the tensor value.</p><p>Examples:</p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>%result = sparse_tensor.load %tensor : tensor&lt;8xf64, #SV&gt;</span></span>
<span class="line"><span></span></span>
<span class="line"><span>%1 = sparse_tensor.load %0 hasInserts : tensor&lt;16x32xf32, #CSR&gt;</span></span></code></pre></div><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/9ea23073f744053d4d0215296dadcac5d8c3c434/src/mlir/Dialects/SparseTensor.jl#L982-L1009" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.sparse_tensor.lvl-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}" href="#Reactant.MLIR.Dialects.sparse_tensor.lvl-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}"><span class="jlbinding">Reactant.MLIR.Dialects.sparse_tensor.lvl</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>lvl</code></p><p>The <code>sparse_tensor.lvl</code> behaves similar to <code>tensor.dim</code> operation. It takes a sparse tensor and a level operand of type <code>index</code> and returns the size of the requested level of the given sparse tensor. If the sparse tensor has an identity dimension to level mapping, it returns the same result as <code>tensor.dim</code>. If the level index is out of bounds, the behavior is undefined.</p><p><strong>Example</strong></p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>#BSR = #sparse_tensor.encoding&lt;{</span></span>
<span class="line"><span>  map = ( i, j ) -&gt;</span></span>
<span class="line"><span>    ( i floordiv 2 : dense,</span></span>
<span class="line"><span>      j floordiv 3 : compressed,</span></span>
<span class="line"><span>      i mod 2      : dense,</span></span>
<span class="line"><span>      j mod 3      : dense</span></span>
<span class="line"><span>    )</span></span>
<span class="line"><span>}&gt;</span></span>
<span class="line"><span></span></span>
<span class="line"><span>// Always returns 2 (4 floordiv 2), can be constant folded:</span></span>
<span class="line"><span>%c0 = arith.constant 0 : index</span></span>
<span class="line"><span>%x = sparse_tensor.lvl %A, %c0 : tensor&lt;4x?xf32, #BSR&gt;</span></span>
<span class="line"><span></span></span>
<span class="line"><span>// Return the dynamic dimension of %A computed by %j mod 3.</span></span>
<span class="line"><span>%c1 = arith.constant 1 : index</span></span>
<span class="line"><span>%y = sparse_tensor.lvl %A, %c1 : tensor&lt;4x?xf32, #BSR&gt;</span></span>
<span class="line"><span></span></span>
<span class="line"><span>// Always return 3 (since j mod 3 &lt; 3), can be constant fold</span></span>
<span class="line"><span>%c3 = arith.constant 3 : index</span></span>
<span class="line"><span>%y = sparse_tensor.lvl %A, %c3 : tensor&lt;4x?xf32, #BSR&gt;</span></span></code></pre></div><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/9ea23073f744053d4d0215296dadcac5d8c3c434/src/mlir/Dialects/SparseTensor.jl#L1036-L1070" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.sparse_tensor.new-Tuple{Reactant.MLIR.IR.Value}" href="#Reactant.MLIR.Dialects.sparse_tensor.new-Tuple{Reactant.MLIR.IR.Value}"><span class="jlbinding">Reactant.MLIR.Dialects.sparse_tensor.new</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>new</code></p><p>Materializes a sparse tensor with contents taken from an opaque pointer provided by <code>source</code>. For targets that have access to a file system, for example, this pointer may be a filename (or file) of a sparse tensor in a particular external storage format. The form of the operation is kept deliberately very general to allow for alternative implementations in the future, such as pointers to buffers or runnable initialization code. The operation is provided as an anchor that materializes a properly typed sparse tensor with inital contents into a computation.</p><p>Reading in a symmetric matrix will result in just the lower/upper triangular part of the matrix (so that only relevant information is stored). Proper symmetry support for operating on symmetric matrices is still TBD.</p><p><strong>Example</strong></p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>sparse_tensor.new %source : !Source to tensor&lt;1024x1024xf64, #CSR&gt;</span></span></code></pre></div><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/9ea23073f744053d4d0215296dadcac5d8c3c434/src/mlir/Dialects/SparseTensor.jl#L1093-L1114" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.sparse_tensor.number_of_entries-Tuple{Reactant.MLIR.IR.Value}" href="#Reactant.MLIR.Dialects.sparse_tensor.number_of_entries-Tuple{Reactant.MLIR.IR.Value}"><span class="jlbinding">Reactant.MLIR.Dialects.sparse_tensor.number_of_entries</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>number_of_entries</code></p><p>Returns the number of entries that are stored in the given sparse tensor. Note that this is typically the number of nonzero elements in the tensor, but since explicit zeros may appear in the storage formats, the more accurate nomenclature is used.</p><p><strong>Example</strong></p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>%noe = sparse_tensor.number_of_entries %tensor : tensor&lt;64x64xf64, #CSR&gt;</span></span></code></pre></div><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/9ea23073f744053d4d0215296dadcac5d8c3c434/src/mlir/Dialects/SparseTensor.jl#L1134-L1147" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.sparse_tensor.out-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}" href="#Reactant.MLIR.Dialects.sparse_tensor.out-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}"><span class="jlbinding">Reactant.MLIR.Dialects.sparse_tensor.out</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>out</code></p><p>Outputs the contents of a sparse tensor to the destination defined by an opaque pointer provided by <code>dest</code>. For targets that have access to a file system, for example, this pointer may specify a filename (or file) for output. The form of the operation is kept deliberately very general to allow for alternative implementations in the future, such as sending the contents to a buffer defined by a pointer.</p><p>Note that this operation is &quot;impure&quot; in the sense that its behavior is solely defined by side-effects and not SSA values.</p><p><strong>Example</strong></p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>sparse_tensor.out %t, %dest : tensor&lt;1024x1024xf64, #CSR&gt;, !Dest</span></span></code></pre></div><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/9ea23073f744053d4d0215296dadcac5d8c3c434/src/mlir/Dialects/SparseTensor.jl#L1170-L1188" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.sparse_tensor.positions-Tuple{Reactant.MLIR.IR.Value}" href="#Reactant.MLIR.Dialects.sparse_tensor.positions-Tuple{Reactant.MLIR.IR.Value}"><span class="jlbinding">Reactant.MLIR.Dialects.sparse_tensor.positions</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>positions</code></p><p>Returns the positions array of the tensor&#39;s storage at the given level. This is similar to the <code>bufferization.to_buffer</code> operation in the sense that it provides a bridge between a tensor world view and a bufferized world view. Unlike the <code>bufferization.to_buffer</code> operation, however, this sparse operation actually lowers into code that extracts the positions array from the sparse storage itself (either by calling a support library or through direct code).</p><p>Writing into the result of this operation is undefined behavior.</p><p><strong>Example</strong></p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>%1 = sparse_tensor.positions %0 { level = 1 : index }</span></span>
<span class="line"><span>   : tensor&lt;64x64xf64, #CSR&gt; to memref&lt;?xindex&gt;</span></span></code></pre></div><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/9ea23073f744053d4d0215296dadcac5d8c3c434/src/mlir/Dialects/SparseTensor.jl#L1786-L1805" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.sparse_tensor.print-Tuple{Reactant.MLIR.IR.Value}" href="#Reactant.MLIR.Dialects.sparse_tensor.print-Tuple{Reactant.MLIR.IR.Value}"><span class="jlbinding">Reactant.MLIR.Dialects.sparse_tensor.print</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>print</code></p><p>Prints the individual components of a sparse tensors (the positions, coordinates, and values components) to stdout for testing and debugging purposes. This operation lowers to just a few primitives in a light-weight runtime support to simplify supporting this operation on new platforms.</p><p><strong>Example</strong></p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>sparse_tensor.print %tensor : tensor&lt;1024x1024xf64, #CSR&gt;</span></span></code></pre></div><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/9ea23073f744053d4d0215296dadcac5d8c3c434/src/mlir/Dialects/SparseTensor.jl#L1208-L1221" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.sparse_tensor.push_back" href="#Reactant.MLIR.Dialects.sparse_tensor.push_back"><span class="jlbinding">Reactant.MLIR.Dialects.sparse_tensor.push_back</span></a> <span class="VPBadge info jlObjectType jlFunction"><!--[-->Function<!--]--></span></summary><p><code>push_back</code></p><p>Pushes <code>value</code> to the end of the given sparse tensor storage buffer <code>inBuffer</code> as indicated by the value of <code>curSize</code> and returns the new size of the buffer in <code>newSize</code> (<code>newSize = curSize + n</code>). The capacity of the buffer is recorded in the memref type of <code>inBuffer</code>. If the current buffer is full, then <code>inBuffer.realloc</code> is called before pushing the data to the buffer. This is similar to std::vector push_back.</p><p>The optional input <code>n</code> specifies the number of times to repeately push the value to the back of the tensor. When <code>n</code> is a compile-time constant, its value can&#39;t be less than 1. If <code>n</code> is a runtime value that is less than 1, the behavior is undefined. Although using input <code>n</code> is semantically equivalent to calling push_back n times, it gives compiler more chances to to optimize the memory reallocation and the filling of the memory with the same value.</p><p>The <code>inbounds</code> attribute tells the compiler that the insertion won&#39;t go beyond the current storage buffer. This allows the compiler to not generate the code for capacity check and reallocation. The typical usage will be for &quot;dynamic&quot; sparse tensors for which a capacity can be set beforehand.</p><p>Note that this operation is &quot;impure&quot; in the sense that even though the result is modeled through an SSA value, referencing the memref through the old SSA value after this operation is undefined behavior.</p><p><strong>Example</strong></p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>%buf, %newSize = sparse_tensor.push_back %curSize, %buffer, %val</span></span>
<span class="line"><span>   : index, memref&lt;?xf64&gt;, f64</span></span></code></pre></div><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>%buf, %newSize = sparse_tensor.push_back inbounds %curSize, %buffer, %val</span></span>
<span class="line"><span>   : xindex, memref&lt;?xf64&gt;, f64</span></span></code></pre></div><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>%buf, %newSize = sparse_tensor.push_back inbounds %curSize, %buffer, %val, %n</span></span>
<span class="line"><span>   : xindex, memref&lt;?xf64&gt;, f64</span></span></code></pre></div><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/9ea23073f744053d4d0215296dadcac5d8c3c434/src/mlir/Dialects/SparseTensor.jl#L1241-L1284" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.sparse_tensor.reduce-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}" href="#Reactant.MLIR.Dialects.sparse_tensor.reduce-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}"><span class="jlbinding">Reactant.MLIR.Dialects.sparse_tensor.reduce</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>reduce</code></p><p>Defines a computation with a <code>linalg.generic</code> operation that takes two operands and an identity value and reduces all stored values down to a single result based on the computation in the region.</p><p>The region must contain exactly one block taking two arguments. The block must end with a sparse_tensor.yield and the output must match the input argument types.</p><p>Note that this operation is only required for custom reductions beyond the standard reduction operations (add, sub, or, xor) that can be sparsified by merely reducing the stored values. More elaborate reduction operations (mul, and, min, max, etc.) would need to account for implicit zeros as well. They can still be handled using this custom reduction operation. The <code>linalg.generic</code> <code>iterator_types</code> defines which indices are being reduced. When the associated operands are used in an operation, a reduction will occur. The use of this explicit <code>reduce</code> operation is not required in most cases.</p><p>Example of Matrix-&gt;Vector reduction using max(product(x_i), 100):</p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>%cf1 = arith.constant 1.0 : f64</span></span>
<span class="line"><span>%cf100 = arith.constant 100.0 : f64</span></span>
<span class="line"><span>%C = tensor.empty(...)</span></span>
<span class="line"><span>%0 = linalg.generic #trait</span></span>
<span class="line"><span>   ins(%A: tensor&lt;?x?xf64, #SparseMatrix&gt;)</span></span>
<span class="line"><span>  outs(%C: tensor&lt;?xf64, #SparseVector&gt;) {</span></span>
<span class="line"><span>  ^bb0(%a: f64, %c: f64) :</span></span>
<span class="line"><span>    %result = sparse_tensor.reduce %c, %a, %cf1 : f64 {</span></span>
<span class="line"><span>        ^bb0(%arg0: f64, %arg1: f64):</span></span>
<span class="line"><span>          %0 = arith.mulf %arg0, %arg1 : f64</span></span>
<span class="line"><span>          %cmp = arith.cmpf &quot;ogt&quot;, %0, %cf100 : f64</span></span>
<span class="line"><span>          %ret = arith.select %cmp, %cf100, %0 : f64</span></span>
<span class="line"><span>          sparse_tensor.yield %ret : f64</span></span>
<span class="line"><span>      }</span></span>
<span class="line"><span>    linalg.yield %result : f64</span></span>
<span class="line"><span>} -&gt; tensor&lt;?xf64, #SparseVector&gt;</span></span></code></pre></div><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/9ea23073f744053d4d0215296dadcac5d8c3c434/src/mlir/Dialects/SparseTensor.jl#L1317-L1358" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.sparse_tensor.reinterpret_map-Tuple{Reactant.MLIR.IR.Value}" href="#Reactant.MLIR.Dialects.sparse_tensor.reinterpret_map-Tuple{Reactant.MLIR.IR.Value}"><span class="jlbinding">Reactant.MLIR.Dialects.sparse_tensor.reinterpret_map</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>reinterpret_map</code></p><p>Reinterprets the dimension-to-level and level-to-dimension map specified in <code>source</code> according to the type of <code>dest</code>. <code>reinterpret_map</code> is a no-op and is introduced merely to resolve type conflicts. It does not make any modification to the source tensor and source/dest tensors are considered to be aliases.</p><p><code>source</code> and <code>dest</code> tensors are &quot;reinterpretable&quot; if and only if they have the exactly same storage at a low level. That is, both <code>source</code> and <code>dest</code> has the same number of levels and level types, and their shape is consistent before and after <code>reinterpret_map</code>.</p><p><strong>Example</strong></p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>#CSC = #sparse_tensor.encoding&lt;{</span></span>
<span class="line"><span>  map = (d0, d1) -&gt; (d1: dense, d0: compressed)</span></span>
<span class="line"><span>}&gt;</span></span>
<span class="line"><span>#CSR = #sparse_tensor.encoding&lt;{</span></span>
<span class="line"><span>  map = (d0, d1) -&gt; (d0: dense, d1: compressed)</span></span>
<span class="line"><span>}&gt;</span></span>
<span class="line"><span>%t1 = sparse_tensor.reinterpret_map %t0 : tensor&lt;3x4xi32, #CSC&gt; to tensor&lt;4x3xi32, #CSR&gt;</span></span>
<span class="line"><span></span></span>
<span class="line"><span>#BSR = #sparse_tensor.encoding&lt;{</span></span>
<span class="line"><span>  map = ( i, j ) -&gt; ( i floordiv 2 : dense,</span></span>
<span class="line"><span>                      j floordiv 3 : compressed,</span></span>
<span class="line"><span>                      i mod 2      : dense,</span></span>
<span class="line"><span>                      j mod 3      : dense</span></span>
<span class="line"><span>  )</span></span>
<span class="line"><span>}&gt;</span></span>
<span class="line"><span>#DSDD = #sparse_tensor.encoding&lt;{</span></span>
<span class="line"><span>  map = (i, j, k, l) -&gt; (i: dense, j: compressed, k: dense, l: dense)</span></span>
<span class="line"><span>}&gt;</span></span>
<span class="line"><span>%t1 = sparse_tensor.reinterpret_map %t0 : tensor&lt;6x12xi32, #BSR&gt; to tensor&lt;3x4x2x3xi32, #DSDD&gt;</span></span></code></pre></div><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/9ea23073f744053d4d0215296dadcac5d8c3c434/src/mlir/Dialects/SparseTensor.jl#L1386-L1422" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.sparse_tensor.reorder_coo-Tuple{Reactant.MLIR.IR.Value}" href="#Reactant.MLIR.Dialects.sparse_tensor.reorder_coo-Tuple{Reactant.MLIR.IR.Value}"><span class="jlbinding">Reactant.MLIR.Dialects.sparse_tensor.reorder_coo</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>reorder_coo</code></p><p>Reorders the input COO to the same order as specified by the output format. E.g., reorder an unordered COO into an ordered one.</p><p>The input and result COO tensor must have the same element type, position type and coordinate type. At the moment, the operation also only supports ordering input and result COO with the same dim2lvl map.</p><p><strong>Example</strong></p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>%res = sparse_tensor.reorder_coo quick_sort %coo : tensor&lt;?x?xf64 : #Unordered_COO&gt; to</span></span>
<span class="line"><span>                                                   tensor&lt;?x?xf64 : #Ordered_COO&gt;</span></span></code></pre></div><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/9ea23073f744053d4d0215296dadcac5d8c3c434/src/mlir/Dialects/SparseTensor.jl#L1442-L1459" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.sparse_tensor.select-Tuple{Reactant.MLIR.IR.Value}" href="#Reactant.MLIR.Dialects.sparse_tensor.select-Tuple{Reactant.MLIR.IR.Value}"><span class="jlbinding">Reactant.MLIR.Dialects.sparse_tensor.select</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>select</code></p><p>Defines an evaluation within a <code>linalg.generic</code> operation that takes a single operand and decides whether or not to keep that operand in the output.</p><p>A single region must contain exactly one block taking one argument. The block must end with a sparse_tensor.yield and the output type must be boolean.</p><p>Value threshold is an obvious usage of the select operation. However, by using <code>linalg.index</code>, other useful selection can be achieved, such as selecting the upper triangle of a matrix.</p><p>Example of selecting A &gt;= 4.0:</p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>%C = tensor.empty(...)</span></span>
<span class="line"><span>%0 = linalg.generic #trait</span></span>
<span class="line"><span>   ins(%A: tensor&lt;?xf64, #SparseVector&gt;)</span></span>
<span class="line"><span>  outs(%C: tensor&lt;?xf64, #SparseVector&gt;) {</span></span>
<span class="line"><span>  ^bb0(%a: f64, %c: f64) :</span></span>
<span class="line"><span>    %result = sparse_tensor.select %a : f64 {</span></span>
<span class="line"><span>        ^bb0(%arg0: f64):</span></span>
<span class="line"><span>          %cf4 = arith.constant 4.0 : f64</span></span>
<span class="line"><span>          %keep = arith.cmpf &quot;uge&quot;, %arg0, %cf4 : f64</span></span>
<span class="line"><span>          sparse_tensor.yield %keep : i1</span></span>
<span class="line"><span>      }</span></span>
<span class="line"><span>    linalg.yield %result : f64</span></span>
<span class="line"><span>} -&gt; tensor&lt;?xf64, #SparseVector&gt;</span></span></code></pre></div><p>Example of selecting lower triangle of a matrix:</p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>%C = tensor.empty(...)</span></span>
<span class="line"><span>%1 = linalg.generic #trait</span></span>
<span class="line"><span>   ins(%A: tensor&lt;?x?xf64, #CSR&gt;)</span></span>
<span class="line"><span>  outs(%C: tensor&lt;?x?xf64, #CSR&gt;) {</span></span>
<span class="line"><span>  ^bb0(%a: f64, %c: f64) :</span></span>
<span class="line"><span>    %row = linalg.index 0 : index</span></span>
<span class="line"><span>    %col = linalg.index 1 : index</span></span>
<span class="line"><span>    %result = sparse_tensor.select %a : f64 {</span></span>
<span class="line"><span>        ^bb0(%arg0: f64):</span></span>
<span class="line"><span>          %keep = arith.cmpf &quot;olt&quot;, %col, %row : f64</span></span>
<span class="line"><span>          sparse_tensor.yield %keep : i1</span></span>
<span class="line"><span>      }</span></span>
<span class="line"><span>    linalg.yield %result : f64</span></span>
<span class="line"><span>} -&gt; tensor&lt;?x?xf64, #CSR&gt;</span></span></code></pre></div><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/9ea23073f744053d4d0215296dadcac5d8c3c434/src/mlir/Dialects/SparseTensor.jl#L1479-L1528" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.sparse_tensor.slice_offset-Tuple{Reactant.MLIR.IR.Value}" href="#Reactant.MLIR.Dialects.sparse_tensor.slice_offset-Tuple{Reactant.MLIR.IR.Value}"><span class="jlbinding">Reactant.MLIR.Dialects.sparse_tensor.slice_offset</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>slice_offset</code></p><p>Extracts the offset of the sparse tensor slice at the given dimension.</p><p>Currently, sparse tensor slices are still a work in progress, and only works when runtime library is disabled (i.e., running the sparsifier with <code>enable-runtime-library=false</code>).</p><p><strong>Example</strong></p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>%0 = tensor.extract_slice %s[%v1, %v2][64, 64][1, 1] : tensor&lt;128x128xf64, #DCSR&gt;</span></span>
<span class="line"><span>                                                    to tensor&lt;64x64xf64, #Slice&gt;</span></span>
<span class="line"><span></span></span>
<span class="line"><span>%1 = sparse_tensor.slice.offset %0 at 0 : tensor&lt;64x64xf64, #Slice&gt;</span></span>
<span class="line"><span>%2 = sparse_tensor.slice.offset %0 at 1 : tensor&lt;64x64xf64, #Slice&gt;</span></span>
<span class="line"><span>// %1 = %v1</span></span>
<span class="line"><span>// %2 = %v2</span></span></code></pre></div><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/9ea23073f744053d4d0215296dadcac5d8c3c434/src/mlir/Dialects/SparseTensor.jl#L1828-L1848" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.sparse_tensor.slice_stride-Tuple{Reactant.MLIR.IR.Value}" href="#Reactant.MLIR.Dialects.sparse_tensor.slice_stride-Tuple{Reactant.MLIR.IR.Value}"><span class="jlbinding">Reactant.MLIR.Dialects.sparse_tensor.slice_stride</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>slice_stride</code></p><p>Extracts the stride of the sparse tensor slice at the given dimension.</p><p>Currently, sparse tensor slices are still a work in progress, and only works when runtime library is disabled (i.e., running the sparsifier with <code>enable-runtime-library=false</code>).</p><p><strong>Example</strong></p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>%0 = tensor.extract_slice %s[%v1, %v2][64, 64][%s1, %s2] : tensor&lt;128x128xf64, #DCSR&gt;</span></span>
<span class="line"><span>                                                        to tensor&lt;64x64xf64, #Slice&gt;</span></span>
<span class="line"><span></span></span>
<span class="line"><span>%1 = sparse_tensor.slice.stride %0 at 0 : tensor&lt;64x64xf64, #Slice&gt;</span></span>
<span class="line"><span>%2 = sparse_tensor.slice.stride %0 at 1 : tensor&lt;64x64xf64, #Slice&gt;</span></span>
<span class="line"><span>// %1 = %s1</span></span>
<span class="line"><span>// %2 = %s2</span></span></code></pre></div><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/9ea23073f744053d4d0215296dadcac5d8c3c434/src/mlir/Dialects/SparseTensor.jl#L1871-L1892" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.sparse_tensor.sort-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value, Vector{Reactant.MLIR.IR.Value}}" href="#Reactant.MLIR.Dialects.sparse_tensor.sort-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value, Vector{Reactant.MLIR.IR.Value}}"><span class="jlbinding">Reactant.MLIR.Dialects.sparse_tensor.sort</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>sort</code></p><p>Sorts the <code>xs</code> values along with some <code>ys</code> values that are put in a single linear buffer <code>xy</code>. The affine map attribute <code>perm_map</code> specifies the permutation to be applied on the <code>xs</code> before comparison, the rank of the permutation map also specifies the number of <code>xs</code> values in <code>xy</code>. The optional index attribute <code>ny</code> provides the number of <code>ys</code> values in <code>xy</code>. When <code>ny</code> is not explicitly specified, its value is 0. This instruction supports a more efficient way to store the COO definition in sparse tensor type.</p><p>The buffer xy should have a dimension not less than n * (rank(perm_map) + ny) while the buffers in <code>ys</code> should have a dimension not less than <code>n</code>. The behavior of the operator is undefined if this condition is not met.</p><p><strong>Example</strong></p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>sparse_tensor.sort insertion_sort_stable %n, %x { perm_map = affine_map&lt;(i,j) -&gt; (j,i)&gt; }</span></span>
<span class="line"><span>  : memref&lt;?xindex&gt;</span></span></code></pre></div><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/9ea23073f744053d4d0215296dadcac5d8c3c434/src/mlir/Dialects/SparseTensor.jl#L1592-L1614" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.sparse_tensor.storage_specifier_get-Tuple{Reactant.MLIR.IR.Value}" href="#Reactant.MLIR.Dialects.sparse_tensor.storage_specifier_get-Tuple{Reactant.MLIR.IR.Value}"><span class="jlbinding">Reactant.MLIR.Dialects.sparse_tensor.storage_specifier_get</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>storage_specifier_get</code></p><p>Returns the requested field of the given storage_specifier.</p><p>Example of querying the size of the coordinates array for level 0:</p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>%0 = sparse_tensor.storage_specifier.get %arg0 crd_mem_sz at 0</span></span>
<span class="line"><span>     : !sparse_tensor.storage_specifier&lt;#COO&gt;</span></span></code></pre></div><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/9ea23073f744053d4d0215296dadcac5d8c3c434/src/mlir/Dialects/SparseTensor.jl#L906-L917" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.sparse_tensor.storage_specifier_init" href="#Reactant.MLIR.Dialects.sparse_tensor.storage_specifier_init"><span class="jlbinding">Reactant.MLIR.Dialects.sparse_tensor.storage_specifier_init</span></a> <span class="VPBadge info jlObjectType jlFunction"><!--[-->Function<!--]--></span></summary><p><code>storage_specifier_init</code></p><p>Returns an initial storage specifier value. A storage specifier value holds the level-sizes, position arrays, coordinate arrays, and the value array. If this is a specifier for slices, it also holds the extra strides/offsets for each tensor dimension.</p><p>TODO: The sparse tensor slice support is currently in a unstable state, and is subject to change in the future.</p><p><strong>Example</strong></p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>#CSR = #sparse_tensor.encoding&lt;{</span></span>
<span class="line"><span>  map = (i, j) -&gt; (i : dense, j : compressed)</span></span>
<span class="line"><span>}&gt;</span></span>
<span class="line"><span>#CSR_SLICE = #sparse_tensor.encoding&lt;{</span></span>
<span class="line"><span>  map = (d0 : #sparse_tensor&lt;slice(1, 4, 1)&gt;,</span></span>
<span class="line"><span>         d1 : #sparse_tensor&lt;slice(1, 4, 2)&gt;) -&gt;</span></span>
<span class="line"><span>        (d0 : dense, d1 : compressed)</span></span>
<span class="line"><span>}&gt;</span></span>
<span class="line"><span></span></span>
<span class="line"><span>%0 = sparse_tensor.storage_specifier.init :  !sparse_tensor.storage_specifier&lt;#CSR&gt;</span></span>
<span class="line"><span>%1 = sparse_tensor.storage_specifier.init with %src</span></span>
<span class="line"><span>     : !sparse_tensor.storage_specifier&lt;#CSR&gt; to</span></span>
<span class="line"><span>       !sparse_tensor.storage_specifier&lt;#CSR_SLICE&gt;</span></span></code></pre></div><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/9ea23073f744053d4d0215296dadcac5d8c3c434/src/mlir/Dialects/SparseTensor.jl#L1645-L1674" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.sparse_tensor.storage_specifier_set-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}" href="#Reactant.MLIR.Dialects.sparse_tensor.storage_specifier_set-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}"><span class="jlbinding">Reactant.MLIR.Dialects.sparse_tensor.storage_specifier_set</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>storage_specifier_set</code></p><p>Set the field of the storage specifier to the given input value. Returns the updated storage_specifier as a new SSA value.</p><p>Example of updating the sizes of the coordinates array for level 0:</p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>%0 = sparse_tensor.storage_specifier.set %arg0 crd_mem_sz at 0 with %new_sz</span></span>
<span class="line"><span>   : !sparse_tensor.storage_specifier&lt;#COO&gt;</span></span></code></pre></div><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/9ea23073f744053d4d0215296dadcac5d8c3c434/src/mlir/Dialects/SparseTensor.jl#L1551-L1563" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.sparse_tensor.unary-Tuple{Reactant.MLIR.IR.Value}" href="#Reactant.MLIR.Dialects.sparse_tensor.unary-Tuple{Reactant.MLIR.IR.Value}"><span class="jlbinding">Reactant.MLIR.Dialects.sparse_tensor.unary</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>unary</code></p><p>Defines a computation with a <code>linalg.generic</code> operation that takes a single operand and executes one of two regions depending on whether the operand is nonzero (i.e. stored explicitly in the sparse storage format).</p><p>Two regions are defined for the operation must appear in this order:</p><ul><li><p>present (elements present in the sparse tensor)</p></li><li><p>absent (elements not present in the sparse tensor)</p></li></ul><p>Each region contains a single block describing the computation and result. A non-empty block must end with a sparse_tensor.yield and the return type must match the type of <code>output</code>. The primary region&#39;s block has one argument, while the missing region&#39;s block has zero arguments. The absent region may only generate constants or values already computed on entry of the <code>linalg.generic</code> operation.</p><p>A region may also be declared empty (i.e. <code>absent={}</code>), indicating that the region does not contribute to the output.</p><p>Due to the possibility of empty regions, i.e. lack of a value for certain cases, the result of this operation may only feed directly into the output of the <code>linalg.generic</code> operation or into into a custom reduction <code>sparse_tensor.reduce</code> operation that follows in the same region.</p><p>Example of A+1, restricted to existing elements:</p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>%C = tensor.empty(...) : tensor&lt;?xf64, #SparseVector&gt;</span></span>
<span class="line"><span>%0 = linalg.generic #trait</span></span>
<span class="line"><span>   ins(%A: tensor&lt;?xf64, #SparseVector&gt;)</span></span>
<span class="line"><span>  outs(%C: tensor&lt;?xf64, #SparseVector&gt;) {</span></span>
<span class="line"><span>  ^bb0(%a: f64, %c: f64) :</span></span>
<span class="line"><span>    %result = sparse_tensor.unary %a : f64 to f64</span></span>
<span class="line"><span>      present={</span></span>
<span class="line"><span>      ^bb0(%arg0: f64):</span></span>
<span class="line"><span>        %cf1 = arith.constant 1.0 : f64</span></span>
<span class="line"><span>        %ret = arith.addf %arg0, %cf1 : f64</span></span>
<span class="line"><span>        sparse_tensor.yield %ret : f64</span></span>
<span class="line"><span>      }</span></span>
<span class="line"><span>      absent={}</span></span>
<span class="line"><span>    linalg.yield %result : f64</span></span>
<span class="line"><span>} -&gt; tensor&lt;?xf64, #SparseVector&gt;</span></span></code></pre></div><p>Example returning +1 for existing values and -1 for missing values:</p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>%p1 = arith.constant  1 : i32</span></span>
<span class="line"><span>%m1 = arith.constant -1 : i32</span></span>
<span class="line"><span>%C = tensor.empty(...) : tensor&lt;?xi32, #SparseVector&gt;</span></span>
<span class="line"><span>%1 = linalg.generic #trait</span></span>
<span class="line"><span>   ins(%A: tensor&lt;?xf64, #SparseVector&gt;)</span></span>
<span class="line"><span>  outs(%C: tensor&lt;?xi32, #SparseVector&gt;) {</span></span>
<span class="line"><span>  ^bb0(%a: f64, %c: i32) :</span></span>
<span class="line"><span>    %result = sparse_tensor.unary %a : f64 to i32</span></span>
<span class="line"><span>      present={</span></span>
<span class="line"><span>      ^bb0(%x: f64):</span></span>
<span class="line"><span>        sparse_tensor.yield %p1 : i32</span></span>
<span class="line"><span>      }</span></span>
<span class="line"><span>      absent={</span></span>
<span class="line"><span>        sparse_tensor.yield %m1 : i32</span></span>
<span class="line"><span>      }</span></span>
<span class="line"><span>    linalg.yield %result : i32</span></span>
<span class="line"><span>} -&gt; tensor&lt;?xi32, #SparseVector&gt;</span></span></code></pre></div><p>Example showing a structural inversion (existing values become missing in the output, while missing values are filled with 1):</p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>%c1 = arith.constant 1 : i64</span></span>
<span class="line"><span>%C = tensor.empty(...) : tensor&lt;?xi64, #SparseVector&gt;</span></span>
<span class="line"><span>%2 = linalg.generic #trait</span></span>
<span class="line"><span>   ins(%A: tensor&lt;?xf64, #SparseVector&gt;)</span></span>
<span class="line"><span>  outs(%C: tensor&lt;?xi64, #SparseVector&gt;) {</span></span>
<span class="line"><span>  ^bb0(%a: f64, %c: i64) :</span></span>
<span class="line"><span>    %result = sparse_tensor.unary %a : f64 to i64</span></span>
<span class="line"><span>      present={}</span></span>
<span class="line"><span>      absent={</span></span>
<span class="line"><span>        sparse_tensor.yield %c1 : i64</span></span>
<span class="line"><span>      }</span></span>
<span class="line"><span>    linalg.yield %result : i64</span></span>
<span class="line"><span>} -&gt; tensor&lt;?xi64, #SparseVector&gt;</span></span></code></pre></div><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/9ea23073f744053d4d0215296dadcac5d8c3c434/src/mlir/Dialects/SparseTensor.jl#L1954-L2040" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.sparse_tensor.values-Tuple{Reactant.MLIR.IR.Value}" href="#Reactant.MLIR.Dialects.sparse_tensor.values-Tuple{Reactant.MLIR.IR.Value}"><span class="jlbinding">Reactant.MLIR.Dialects.sparse_tensor.values</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>values</code></p><p>Returns the values array of the sparse storage format for the given sparse tensor, independent of the actual dimension. This is similar to the <code>bufferization.to_buffer</code> operation in the sense that it provides a bridge between a tensor world view and a bufferized world view. Unlike the <code>bufferization.to_buffer</code> operation, however, this sparse operation actually lowers into code that extracts the values array from the sparse storage scheme (either by calling a support library or through direct code).</p><p>Writing into the result of this operation is undefined behavior.</p><p><strong>Example</strong></p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>%1 = sparse_tensor.values %0 : tensor&lt;64x64xf64, #CSR&gt; to memref&lt;?xf64&gt;</span></span></code></pre></div><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/9ea23073f744053d4d0215296dadcac5d8c3c434/src/mlir/Dialects/SparseTensor.jl#L1915-L1933" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.sparse_tensor.yield-Tuple{Vector{Reactant.MLIR.IR.Value}}" href="#Reactant.MLIR.Dialects.sparse_tensor.yield-Tuple{Vector{Reactant.MLIR.IR.Value}}"><span class="jlbinding">Reactant.MLIR.Dialects.sparse_tensor.yield</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>yield</code></p><p>Yields a value from within a <code>binary</code>, <code>unary</code>, <code>reduce</code>, <code>select</code> or <code>foreach</code> block.</p><p><strong>Example</strong></p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>%0 = sparse_tensor.unary %a : i64 to i64 {</span></span>
<span class="line"><span>  present={</span></span>
<span class="line"><span>    ^bb0(%arg0: i64):</span></span>
<span class="line"><span>      %cst = arith.constant 1 : i64</span></span>
<span class="line"><span>      %ret = arith.addi %arg0, %cst : i64</span></span>
<span class="line"><span>      sparse_tensor.yield %ret : i64</span></span>
<span class="line"><span>  }</span></span>
<span class="line"><span>}</span></span></code></pre></div><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/9ea23073f744053d4d0215296dadcac5d8c3c434/src/mlir/Dialects/SparseTensor.jl#L2066-L2084" target="_blank" rel="noreferrer">source</a><!--]--></span></details></div></div></main><footer class="VPDocFooter" data-v-83890dd9 data-v-4f9813fa><!--[--><!--]--><div class="edit-info" data-v-4f9813fa><div class="edit-link" data-v-4f9813fa><a class="VPLink link vp-external-link-icon no-icon edit-link-button" href="https://github.com/EnzymeAD/Reactant.jl/edit/main/docs/src/api/dialects/sparsetensor.md" target="_blank" rel="noreferrer" data-v-4f9813fa><!--[--><span class="vpi-square-pen edit-link-icon" data-v-4f9813fa></span> Edit this page on GitHub<!--]--></a></div><!----></div><nav class="prev-next" aria-labelledby="doc-footer-aria-label" data-v-4f9813fa><span class="visually-hidden" id="doc-footer-aria-label" data-v-4f9813fa>Pager</span><div class="pager" data-v-4f9813fa><a class="VPLink link pager-link prev" href="/Reactant.jl/previews/PR1385/api/dialects/shardy" data-v-4f9813fa><!--[--><span class="desc" data-v-4f9813fa>Previous page</span><span class="title" data-v-4f9813fa>Shardy</span><!--]--></a></div><div class="pager" data-v-4f9813fa><a class="VPLink link pager-link next" href="/Reactant.jl/previews/PR1385/api/dialects/stablehlo" data-v-4f9813fa><!--[--><span class="desc" data-v-4f9813fa>Next page</span><span class="title" data-v-4f9813fa>StableHLO</span><!--]--></a></div></nav></footer><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><footer class="VPFooter has-sidebar" data-v-a9a9e638 data-v-c970a860><div class="container" data-v-c970a860><p class="message" data-v-c970a860>Made with <a href="https://documenter.juliadocs.org/stable/" target="_blank"><strong>Documenter.jl</strong></a>, <a href="https://vitepress.dev" target="_blank"><strong>VitePress</strong></a> and <a href="https://luxdl.github.io/DocumenterVitepress.jl/stable" target="_blank"><strong>DocumenterVitepress.jl</strong></a><br>Released under the MIT License. Powered by the <a href="https://www.julialang.org">Julia Programming Language</a>.<br></p><p class="copyright" data-v-c970a860>© Copyright 2025 Reactant Development Team.</p></div></footer><!--[--><!--]--></div></div>
    <script>window.__VP_HASH_MAP__=JSON.parse("{\"api_api.md\":\"CvgjVCq9\",\"api_config.md\":\"1igqPjnW\",\"api_dialects_affine.md\":\"D-8hJbHo\",\"api_dialects_arith.md\":\"C8kX3KmN\",\"api_dialects_builtin.md\":\"D6DD5MPX\",\"api_dialects_chlo.md\":\"BRFtG4XV\",\"api_dialects_enzyme.md\":\"B6ZSd7Yx\",\"api_dialects_enzymexla.md\":\"-DIxIbXC\",\"api_dialects_func.md\":\"DNAae5rN\",\"api_dialects_gpu.md\":\"D6NTY22g\",\"api_dialects_llvm.md\":\"Dqn57itT\",\"api_dialects_memref.md\":\"ClUZKXRz\",\"api_dialects_mpi.md\":\"BS3xj-Tu\",\"api_dialects_nvvm.md\":\"DDvHTNwe\",\"api_dialects_shardy.md\":\"DOcZozFM\",\"api_dialects_sparsetensor.md\":\"4uqqTpex\",\"api_dialects_stablehlo.md\":\"cVirhPIV\",\"api_dialects_tpu.md\":\"BvNduVUq\",\"api_dialects_triton.md\":\"BpbGyJOB\",\"api_dialects_vhlo.md\":\"BMtYHOHF\",\"api_internal.md\":\"L20_wpJy\",\"api_mlirc.md\":\"DBevUhOu\",\"api_ops.md\":\"C196B2Wg\",\"api_sharding.md\":\"D8xxDzjc\",\"api_xla.md\":\"CCLtrkXt\",\"index.md\":\"ABpVZq2l\",\"introduction_configuration.md\":\"CBlAtHaL\",\"introduction_faqs.md\":\"B_KN3KY4\",\"introduction_index.md\":\"DTAFAG_D\",\"tutorials_control-flow.md\":\"BV0sLZS9\",\"tutorials_index.md\":\"Du5DPOiX\",\"tutorials_local-build.md\":\"BTkHSGGO\",\"tutorials_multihost.md\":\"C0BwY53p\",\"tutorials_profiling.md\":\"BU5uTp54\"}");window.__VP_SITE_DATA__=JSON.parse("{\"lang\":\"en-US\",\"dir\":\"ltr\",\"title\":\"Reactant.jl\",\"description\":\"Documentation for Reactant.jl\",\"base\":\"/Reactant.jl/previews/PR1385/\",\"head\":[],\"router\":{\"prefetchLinks\":true},\"appearance\":true,\"themeConfig\":{\"outline\":\"deep\",\"logo\":{\"light\":\"/logo.svg\",\"dark\":\"/logo.svg\"},\"search\":{\"provider\":\"local\",\"options\":{\"detailedView\":true}},\"nav\":[{\"text\":\"Home\",\"link\":\"/\"},{\"text\":\"Getting Started\",\"items\":[{\"text\":\"Introduction\",\"link\":\"/introduction\"},{\"text\":\"Configuration\",\"link\":\"/introduction/configuration\"},{\"text\":\"FAQs\",\"link\":\"/introduction/FAQs\"}]},{\"text\":\"Benchmarks\",\"link\":\"https://enzymead.github.io/Reactant.jl/benchmarks/\"},{\"text\":\"Tutorials\",\"items\":[{\"text\":\"Overview\",\"link\":\"/tutorials/\"},{\"text\":\"Profiling\",\"link\":\"/tutorials/profiling\"},{\"text\":\"Distributed\",\"link\":\"/tutorials/multihost\"},{\"text\":\"Local build\",\"link\":\"/tutorials/local-build\"},{\"text\":\"Control Flow\",\"link\":\"/tutorials/control-flow\"}]},{\"text\":\"API\",\"items\":[{\"text\":\"Core Reactant API\",\"link\":\"/api/api\"},{\"text\":\"Sharding\",\"link\":\"/api/sharding\"},{\"text\":\"Ops\",\"link\":\"/api/ops\"},{\"text\":\"Configuration\",\"link\":\"/api/config\"},{\"text\":\"MLIR Dialects\",\"items\":[{\"text\":\"ArithOps\",\"link\":\"/api/dialects/arith\"},{\"text\":\"Affine\",\"link\":\"/api/dialects/affine\"},{\"text\":\"Builtin\",\"link\":\"/api/dialects/builtin\"},{\"text\":\"Chlo\",\"link\":\"/api/dialects/chlo\"},{\"text\":\"Enzyme\",\"link\":\"/api/dialects/enzyme\"},{\"text\":\"EnzymeXLA\",\"link\":\"/api/dialects/enzymexla\"},{\"text\":\"Func\",\"link\":\"/api/dialects/func\"},{\"text\":\"GPU\",\"link\":\"/api/dialects/gpu\"},{\"text\":\"LLVM\",\"link\":\"/api/dialects/llvm\"},{\"text\":\"MPI\",\"link\":\"/api/dialects/mpi\"},{\"text\":\"MemRef\",\"link\":\"/api/dialects/memref\"},{\"text\":\"NVVM\",\"link\":\"/api/dialects/nvvm\"},{\"text\":\"Shardy\",\"link\":\"/api/dialects/shardy\"},{\"text\":\"SparseTensor\",\"link\":\"/api/dialects/sparsetensor\"},{\"text\":\"StableHLO\",\"link\":\"/api/dialects/stablehlo\"},{\"text\":\"Triton\",\"link\":\"/api/dialects/triton\"},{\"text\":\"TPU\",\"link\":\"/api/dialects/tpu\"},{\"text\":\"VHLO\",\"link\":\"/api/dialects/vhlo\"}]},{\"text\":\"Low-Level API\",\"items\":[{\"text\":\"MLIR API\",\"link\":\"/api/mlirc\"},{\"text\":\"XLA\",\"link\":\"/api/xla\"}]},{\"text\":\"Internal API\",\"link\":\"/api/internal\"}]},{\"component\":\"VersionPicker\"}],\"sidebar\":{\"/introduction/\":[{\"text\":\"Getting Started\",\"collapsed\":false,\"items\":[{\"text\":\"Introduction\",\"link\":\"/introduction\"},{\"text\":\"Configuration\",\"link\":\"/introduction/configuration\"},{\"text\":\"FAQs\",\"link\":\"/introduction/FAQs\"}]}],\"/tutorials/\":[{\"text\":\"Tutorials\",\"collapsed\":false,\"items\":[{\"text\":\"Overview\",\"link\":\"/tutorials/\"},{\"text\":\"Profiling\",\"link\":\"/tutorials/profiling\"},{\"text\":\"Distributed\",\"link\":\"/tutorials/multihost\"},{\"text\":\"Local build\",\"link\":\"/tutorials/local-build\"},{\"text\":\"Control Flow\",\"link\":\"/tutorials/control-flow\"}]}],\"/api/\":[{\"text\":\"API Reference\",\"collapsed\":false,\"items\":[{\"text\":\"Reactant API\",\"link\":\"/api/api\"},{\"text\":\"Sharding\",\"link\":\"/api/sharding\"},{\"text\":\"Ops\",\"link\":\"/api/ops\"},{\"text\":\"Configuration\",\"link\":\"/api/config\"},{\"text\":\"MLIR Dialects\",\"collapsed\":false,\"items\":[{\"text\":\"ArithOps\",\"link\":\"/api/dialects/arith\"},{\"text\":\"Affine\",\"link\":\"/api/dialects/affine\"},{\"text\":\"Builtin\",\"link\":\"/api/dialects/builtin\"},{\"text\":\"Chlo\",\"link\":\"/api/dialects/chlo\"},{\"text\":\"Enzyme\",\"link\":\"/api/dialects/enzyme\"},{\"text\":\"EnzymeXLA\",\"link\":\"/api/dialects/enzymexla\"},{\"text\":\"Func\",\"link\":\"/api/dialects/func\"},{\"text\":\"GPU\",\"link\":\"/api/dialects/gpu\"},{\"text\":\"LLVM\",\"link\":\"/api/dialects/llvm\"},{\"text\":\"MPI\",\"link\":\"/api/dialects/mpi\"},{\"text\":\"MemRef\",\"link\":\"/api/dialects/memref\"},{\"text\":\"NVVM\",\"link\":\"/api/dialects/nvvm\"},{\"text\":\"Shardy\",\"link\":\"/api/dialects/shardy\"},{\"text\":\"SparseTensor\",\"link\":\"/api/dialects/sparsetensor\"},{\"text\":\"StableHLO\",\"link\":\"/api/dialects/stablehlo\"},{\"text\":\"Triton\",\"link\":\"/api/dialects/triton\"},{\"text\":\"TPU\",\"link\":\"/api/dialects/tpu\"},{\"text\":\"VHLO\",\"link\":\"/api/dialects/vhlo\"}]},{\"text\":\"Low-Level API\",\"collapsed\":false,\"items\":[{\"text\":\"MLIR API\",\"link\":\"/api/mlirc\"},{\"text\":\"XLA\",\"link\":\"/api/xla\"}]},{\"text\":\"Internal API\",\"link\":\"/api/internal\"}]}]},\"editLink\":{\"pattern\":\"https://github.com/EnzymeAD/Reactant.jl/edit/main/docs/src/:path\",\"text\":\"Edit this page on GitHub\"},\"socialLinks\":[{\"icon\":\"slack\",\"link\":\"https://julialang.org/slack/\"}],\"footer\":{\"message\":\"Made with <a href=\\\"https://documenter.juliadocs.org/stable/\\\" target=\\\"_blank\\\"><strong>Documenter.jl</strong></a>, <a href=\\\"https://vitepress.dev\\\" target=\\\"_blank\\\"><strong>VitePress</strong></a> and <a href=\\\"https://luxdl.github.io/DocumenterVitepress.jl/stable\\\" target=\\\"_blank\\\"><strong>DocumenterVitepress.jl</strong></a><br>Released under the MIT License. Powered by the <a href=\\\"https://www.julialang.org\\\">Julia Programming Language</a>.<br>\",\"copyright\":\"© Copyright 2025 Reactant Development Team.\"},\"lastUpdated\":{\"text\":\"Updated at\",\"formatOptions\":{\"dateStyle\":\"full\",\"timeStyle\":\"medium\"}}},\"locales\":{},\"scrollOffset\":134,\"cleanUrls\":true}");</script>
    
  </body>
</html>