<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Tensor Dialect | Reactant.jl</title>
    <meta name="description" content="Documentation for Reactant.jl">
    <meta name="generator" content="VitePress v1.6.4">
    <link rel="preload stylesheet" href="/Reactant.jl/previews/PR2571/assets/style.C9ORk6Bz.css" as="style">
    <link rel="preload stylesheet" href="/Reactant.jl/previews/PR2571/vp-icons.css" as="style">
    
    <script type="module" src="/Reactant.jl/previews/PR2571/assets/app.B-YLMdSj.js"></script>
    <link rel="preload" href="/Reactant.jl/previews/PR2571/assets/inter-roman-latin.Di8DUHzh.woff2" as="font" type="font/woff2" crossorigin="">
    <link rel="modulepreload" href="/Reactant.jl/previews/PR2571/assets/chunks/theme.2Hgney8c.js">
    <link rel="modulepreload" href="/Reactant.jl/previews/PR2571/assets/chunks/framework.B0LxqlOb.js">
    <link rel="modulepreload" href="/Reactant.jl/previews/PR2571/assets/api_dialects_tensor.md.Btz0De3z.lean.js">
    <link rel="icon" href="/Reactant.jl/previews/PR2571/favicon.ico">
    <script src="/versions.js"></script>
    <script src="/Reactant.jl/previews/PR2571/siteinfo.js"></script>
    <script id="check-dark-mode">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"auto",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
    <script id="check-mac-os">document.documentElement.classList.toggle("mac",/Mac|iPhone|iPod|iPad/i.test(navigator.platform));</script>
  </head>
  <body>
    <div id="app"><div class="Layout" data-v-a9a9e638><!--[--><!--]--><!--[--><span tabindex="-1" data-v-492508fc></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-492508fc>Skip to content</a><!--]--><!----><header class="VPNav" data-v-a9a9e638 data-v-f1e365da><div class="VPNavBar" data-v-f1e365da data-v-822684d1><div class="wrapper" data-v-822684d1><div class="container" data-v-822684d1><div class="title" data-v-822684d1><div class="VPNavBarTitle has-sidebar" data-v-822684d1 data-v-0f4f798b><a class="title" href="/Reactant.jl/previews/PR2571/" data-v-0f4f798b><!--[--><!--]--><!--[--><!--[--><!--[--><img class="VPImage dark logo" src="/Reactant.jl/previews/PR2571/logo.svg" alt data-v-35a7d0b8><!--]--><!--[--><img class="VPImage light logo" src="/Reactant.jl/previews/PR2571/logo.svg" alt data-v-35a7d0b8><!--]--><!--]--><!--]--><span data-v-0f4f798b>Reactant.jl</span><!--[--><!--]--></a></div></div><div class="content" data-v-822684d1><div class="content-body" data-v-822684d1><!--[--><!--]--><div class="VPNavBarSearch search" data-v-822684d1><!--[--><!----><div id="local-search"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><span class="vp-icon DocSearch-Search-Icon"></span><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"><kbd class="DocSearch-Button-Key"></kbd><kbd class="DocSearch-Button-Key">K</kbd></span></button></div><!--]--></div><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-822684d1 data-v-e6d46098><span id="main-nav-aria-label" class="visually-hidden" data-v-e6d46098> Main Navigation </span><!--[--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/Reactant.jl/previews/PR2571/" tabindex="0" data-v-e6d46098 data-v-956ec74c><!--[--><span data-v-956ec74c>Home</span><!--]--></a><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-e6d46098 data-v-04f5c5e9><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-04f5c5e9><span class="text" data-v-04f5c5e9><!----><span data-v-04f5c5e9>Getting Started</span><span class="vpi-chevron-down text-icon" data-v-04f5c5e9></span></span></button><div class="menu" data-v-04f5c5e9><div class="VPMenu" data-v-04f5c5e9 data-v-7dd3104a><div class="items" data-v-7dd3104a><!--[--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2571/introduction" data-v-acbfed09><!--[--><span data-v-acbfed09>Introduction</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2571/introduction/configuration" data-v-acbfed09><!--[--><span data-v-acbfed09>Configuration</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2571/introduction/FAQs" data-v-acbfed09><!--[--><span data-v-acbfed09>FAQs</span><!--]--></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><a class="VPLink link vp-external-link-icon VPNavBarMenuLink" href="https://enzymead.github.io/Reactant.jl/benchmarks/" target="_blank" rel="noreferrer" tabindex="0" data-v-e6d46098 data-v-956ec74c><!--[--><span data-v-956ec74c>Benchmarks</span><!--]--></a><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-e6d46098 data-v-04f5c5e9><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-04f5c5e9><span class="text" data-v-04f5c5e9><!----><span data-v-04f5c5e9>Tutorials</span><span class="vpi-chevron-down text-icon" data-v-04f5c5e9></span></span></button><div class="menu" data-v-04f5c5e9><div class="VPMenu" data-v-04f5c5e9 data-v-7dd3104a><div class="items" data-v-7dd3104a><!--[--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2571/tutorials/" data-v-acbfed09><!--[--><span data-v-acbfed09>Overview</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2571/tutorials/partial-evaluation" data-v-acbfed09><!--[--><span data-v-acbfed09>Partial Evaluation</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2571/tutorials/control-flow" data-v-acbfed09><!--[--><span data-v-acbfed09>Control Flow</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2571/tutorials/automatic-differentiation" data-v-acbfed09><!--[--><span data-v-acbfed09>Automatic Differentiation</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2571/tutorials/sharding" data-v-acbfed09><!--[--><span data-v-acbfed09>Sharding</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2571/tutorials/profiling" data-v-acbfed09><!--[--><span data-v-acbfed09>Profiling</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2571/tutorials/multihost" data-v-acbfed09><!--[--><span data-v-acbfed09>Multi-Host Environments</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2571/tutorials/local-build" data-v-acbfed09><!--[--><span data-v-acbfed09>Local build</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2571/tutorials/persistent_compile_cache" data-v-acbfed09><!--[--><span data-v-acbfed09>Persistent Compilation Cache</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2571/tutorials/raising" data-v-acbfed09><!--[--><span data-v-acbfed09>Raising</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2571/tutorials/kernels" data-v-acbfed09><!--[--><span data-v-acbfed09>Computational kernels</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2571/tutorials/debugging" data-v-acbfed09><!--[--><span data-v-acbfed09>Debugging compilation errors</span><!--]--></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup active" data-v-e6d46098 data-v-04f5c5e9><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-04f5c5e9><span class="text" data-v-04f5c5e9><!----><span data-v-04f5c5e9>API</span><span class="vpi-chevron-down text-icon" data-v-04f5c5e9></span></span></button><div class="menu" data-v-04f5c5e9><div class="VPMenu" data-v-04f5c5e9 data-v-7dd3104a><div class="items" data-v-7dd3104a><!--[--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2571/api/api" data-v-acbfed09><!--[--><span data-v-acbfed09>Core Reactant API</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2571/api/sharding" data-v-acbfed09><!--[--><span data-v-acbfed09>Sharding</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2571/api/serialization" data-v-acbfed09><!--[--><span data-v-acbfed09>Serialization</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2571/api/ops" data-v-acbfed09><!--[--><span data-v-acbfed09>Ops</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2571/api/config" data-v-acbfed09><!--[--><span data-v-acbfed09>Configuration</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuGroup" data-v-7dd3104a data-v-48c802d0><p class="title" data-v-48c802d0>MLIR Dialects</p><!--[--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2571/api/dialects/arith" data-v-acbfed09><!--[--><span data-v-acbfed09>ArithOps</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2571/api/dialects/affine" data-v-acbfed09><!--[--><span data-v-acbfed09>Affine</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2571/api/dialects/builtin" data-v-acbfed09><!--[--><span data-v-acbfed09>Builtin</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2571/api/dialects/chlo" data-v-acbfed09><!--[--><span data-v-acbfed09>Chlo</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2571/api/dialects/complex" data-v-acbfed09><!--[--><span data-v-acbfed09>Complex</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2571/api/dialects/cuda_tile" data-v-acbfed09><!--[--><span data-v-acbfed09>CUDA Tile</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2571/api/dialects/enzyme" data-v-acbfed09><!--[--><span data-v-acbfed09>Enzyme</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2571/api/dialects/enzymexla" data-v-acbfed09><!--[--><span data-v-acbfed09>EnzymeXLA</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2571/api/dialects/func" data-v-acbfed09><!--[--><span data-v-acbfed09>Func</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2571/api/dialects/gpu" data-v-acbfed09><!--[--><span data-v-acbfed09>GPU</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2571/api/dialects/llvm" data-v-acbfed09><!--[--><span data-v-acbfed09>LLVM</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2571/api/dialects/mpi" data-v-acbfed09><!--[--><span data-v-acbfed09>MPI</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2571/api/dialects/memref" data-v-acbfed09><!--[--><span data-v-acbfed09>MemRef</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2571/api/dialects/mosaicgpu" data-v-acbfed09><!--[--><span data-v-acbfed09>Mosaic GPU</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2571/api/dialects/nvvm" data-v-acbfed09><!--[--><span data-v-acbfed09>NVVM</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2571/api/dialects/shape" data-v-acbfed09><!--[--><span data-v-acbfed09>Shape</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2571/api/dialects/shardy" data-v-acbfed09><!--[--><span data-v-acbfed09>Shardy</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2571/api/dialects/sparsetensor" data-v-acbfed09><!--[--><span data-v-acbfed09>SparseTensor</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2571/api/dialects/stablehlo" data-v-acbfed09><!--[--><span data-v-acbfed09>StableHLO</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link active" href="/Reactant.jl/previews/PR2571/api/dialects/tensor" data-v-acbfed09><!--[--><span data-v-acbfed09>Tensor</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2571/api/dialects/triton" data-v-acbfed09><!--[--><span data-v-acbfed09>Triton</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2571/api/dialects/tritonext" data-v-acbfed09><!--[--><span data-v-acbfed09>TritonExt</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2571/api/dialects/tpu" data-v-acbfed09><!--[--><span data-v-acbfed09>TPU</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2571/api/dialects/vhlo" data-v-acbfed09><!--[--><span data-v-acbfed09>VHLO</span><!--]--></a></div><!--]--><!--]--></div><!--]--><!--[--><div class="VPMenuGroup" data-v-7dd3104a data-v-48c802d0><p class="title" data-v-48c802d0>Low-Level API</p><!--[--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2571/api/mlirc" data-v-acbfed09><!--[--><span data-v-acbfed09>MLIR API</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2571/api/xla" data-v-acbfed09><!--[--><span data-v-acbfed09>XLA</span><!--]--></a></div><!--]--><!--]--></div><!--]--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2571/api/internal" data-v-acbfed09><!--[--><span data-v-acbfed09>Internal API</span><!--]--></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><!----><!--]--><!--]--></nav><!----><div class="VPNavBarAppearance appearance" data-v-822684d1 data-v-af096f4a><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-af096f4a data-v-e40a8bb6 data-v-4a1c76db><span class="check" data-v-4a1c76db><span class="icon" data-v-4a1c76db><!--[--><span class="vpi-sun sun" data-v-e40a8bb6></span><span class="vpi-moon moon" data-v-e40a8bb6></span><!--]--></span></span></button></div><div class="VPSocialLinks VPNavBarSocialLinks social-links" data-v-822684d1 data-v-164c457f data-v-ee7a9424><!--[--><a class="VPSocialLink no-icon" href="https://julialang.org/slack/" aria-label="slack" target="_blank" rel="noopener" data-v-ee7a9424 data-v-d26d30cb><span class="vpi-social-slack"></span></a><!--]--></div><div class="VPFlyout VPNavBarExtra extra" data-v-822684d1 data-v-925effce data-v-04f5c5e9><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-04f5c5e9><span class="vpi-more-horizontal icon" data-v-04f5c5e9></span></button><div class="menu" data-v-04f5c5e9><div class="VPMenu" data-v-04f5c5e9 data-v-7dd3104a><!----><!--[--><!--[--><!----><div class="group" data-v-925effce><div class="item appearance" data-v-925effce><p class="label" data-v-925effce>Appearance</p><div class="appearance-action" data-v-925effce><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-925effce data-v-e40a8bb6 data-v-4a1c76db><span class="check" data-v-4a1c76db><span class="icon" data-v-4a1c76db><!--[--><span class="vpi-sun sun" data-v-e40a8bb6></span><span class="vpi-moon moon" data-v-e40a8bb6></span><!--]--></span></span></button></div></div></div><div class="group" data-v-925effce><div class="item social-links" data-v-925effce><div class="VPSocialLinks social-links-list" data-v-925effce data-v-ee7a9424><!--[--><a class="VPSocialLink no-icon" href="https://julialang.org/slack/" aria-label="slack" target="_blank" rel="noopener" data-v-ee7a9424 data-v-d26d30cb><span class="vpi-social-slack"></span></a><!--]--></div></div></div><!--]--><!--]--></div></div></div><!--[--><!--[--><!--[--><!--[--><a target="_blank" data-decoration="★" title="305 GitHub stars" href="https://github.com/EnzymeAD/Reactant.jl" data-v-b4d08338><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="20" height="20" fill="currentColor" style="vertical-align:middle;margin-right:0.25rem;margin-left:0.5rem;" data-v-b4d08338><path d="M12 .297C5.375.297 0 5.673 0 12.3c0 5.292 3.438 9.8 8.207 11.387.6.11.793-.26.793-.577 0-.285-.01-1.04-.015-2.04-3.338.727-4.042-1.61-4.042-1.61-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.807 1.305 3.493.997.107-.774.42-1.305.762-1.605-2.665-.3-5.467-1.333-5.467-5.931 0-1.31.47-2.382 1.236-3.222-.123-.303-.535-1.52.117-3.166 0 0 1.01-.323 3.31 1.23.96-.267 1.98-.4 3-.405 1.02.005 2.04.138 3 .405 2.3-1.553 3.31-1.23 3.31-1.23.653 1.646.24 2.863.117 3.166.765.84 1.236 1.912 1.236 3.222 0 4.61-2.807 5.625-5.477 5.921.43.372.823 1.102.823 2.222 0 1.606-.015 2.902-.015 3.293 0 .32.192.693.8.577C20.565 22.1 24 17.588 24 12.297 24 5.673 18.627.297 12 .297z" data-v-b4d08338></path></svg><span data-v-b4d08338>0.3k</span></a><a class="mobile" target="_blank" title="305 GitHub stars" href="https://github.com/EnzymeAD/Reactant.jl" data-v-b4d08338><svg xmlns="http://www.w3.org/2000/svg" width="21" height="21" viewBox="0 0 21 21" fill="none" data-v-b4d08338><path d="M19.625 5.60534C18.7083 4.03477 17.4649 2.79135 15.8945 1.87479C14.3238 0.958185 12.6091 0.5 10.7492 0.5C8.88947 0.5 7.17422 0.958325 5.60388 1.87479C4.0333 2.7913 2.78997 4.03477 1.87332 5.60534C0.956814 7.17587 0.498535 8.89089 0.498535 10.7504C0.498535 12.984 1.15021 14.9926 2.4539 16.7766C3.75744 18.5607 5.44142 19.7952 7.50571 20.4803C7.746 20.5249 7.92388 20.4936 8.03954 20.387C8.15524 20.2804 8.21302 20.1467 8.21302 19.9868C8.21302 19.9601 8.21073 19.7199 8.20629 19.266C8.20171 18.8122 8.19956 18.4162 8.19956 18.0783L7.89256 18.1315C7.69682 18.1673 7.44989 18.1825 7.15178 18.1782C6.8538 18.174 6.54446 18.1428 6.22419 18.0847C5.90377 18.0272 5.60575 17.8937 5.32988 17.6846C5.05416 17.4755 4.85842 17.2018 4.74272 16.8639L4.60925 16.5568C4.52029 16.3523 4.38023 16.1251 4.18888 15.8761C3.99754 15.6269 3.80405 15.458 3.60831 15.369L3.51486 15.3021C3.45259 15.2577 3.39481 15.204 3.34138 15.1418C3.28799 15.0796 3.24802 15.0173 3.22132 14.955C3.19458 14.8926 3.21674 14.8414 3.28804 14.8012C3.35933 14.761 3.48817 14.7416 3.67512 14.7416L3.94196 14.7814C4.11993 14.8171 4.34007 14.9236 4.60266 15.1017C4.86511 15.2796 5.08085 15.5109 5.24994 15.7956C5.4547 16.1605 5.7014 16.4385 5.99072 16.6299C6.27982 16.8212 6.5713 16.9167 6.86488 16.9167C7.15846 16.9167 7.41203 16.8945 7.62567 16.8502C7.83908 16.8057 8.0393 16.7388 8.22625 16.6499C8.30633 16.0535 8.52437 15.5953 8.88017 15.275C8.37304 15.2217 7.9171 15.1414 7.51212 15.0347C7.10736 14.9278 6.6891 14.7544 6.25761 14.5139C5.82589 14.2738 5.46774 13.9756 5.18309 13.6198C4.89839 13.2639 4.66474 12.7966 4.48247 12.2183C4.3001 11.6399 4.20889 10.9726 4.20889 10.2163C4.20889 9.13941 4.56044 8.22304 5.26341 7.46665C4.93411 6.65705 4.96519 5.74947 5.35676 4.744C5.61482 4.66382 5.9975 4.72399 6.50463 4.92412C7.01186 5.12434 7.38323 5.29587 7.61912 5.43808C7.85502 5.58024 8.04402 5.70071 8.18642 5.79842C9.01411 5.56715 9.86825 5.45149 10.7491 5.45149C11.6299 5.45149 12.4843 5.56715 13.312 5.79842L13.8192 5.47823C14.166 5.26459 14.5756 5.06881 15.0469 4.89083C15.5185 4.71295 15.8791 4.66396 16.1284 4.74414C16.5286 5.74966 16.5643 6.65719 16.2349 7.46679C16.9378 8.22318 17.2895 9.13978 17.2895 10.2164C17.2895 10.9727 17.198 11.6421 17.0159 12.225C16.8336 12.808 16.5979 13.2749 16.3088 13.6265C16.0194 13.9781 15.659 14.274 15.2275 14.5141C14.7959 14.7544 14.3775 14.9278 13.9728 15.0347C13.5678 15.1415 13.1119 15.2219 12.6047 15.2752C13.0673 15.6755 13.2986 16.3073 13.2986 17.1704V19.9864C13.2986 20.1464 13.3542 20.2799 13.4656 20.3867C13.5768 20.4932 13.7524 20.5246 13.9927 20.4799C16.0573 19.7949 17.7413 18.5603 19.0448 16.7762C20.3481 14.9922 21 12.9837 21 10.75C20.9996 8.89075 20.541 7.17587 19.625 5.60534Z" fill="currentColor" data-v-b4d08338></path></svg></a><!--]--><div class="VPFlyout VPNolebaseEnhancedReadabilitiesMenu VPNolebaseEnhancedReadabilitiesMenuFlyout" aria-label="Enhanced Readability" role="menuitem" data-v-04f5c5e9><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-04f5c5e9><span class="text" data-v-04f5c5e9><span class="i-icon-park-outline:book-open option-icon" data-v-04f5c5e9></span><!----><span class="vpi-chevron-down text-icon" data-v-04f5c5e9></span></span></button><div class="menu" data-v-04f5c5e9><div class="VPMenu" data-v-04f5c5e9 data-v-7dd3104a><!----><!--[--><!--]--></div></div></div><!--]--><!--]--><!--]--><button type="button" class="VPNavBarHamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-822684d1 data-v-5dea55bf><span class="container" data-v-5dea55bf><span class="top" data-v-5dea55bf></span><span class="middle" data-v-5dea55bf></span><span class="bottom" data-v-5dea55bf></span></span></button></div></div></div></div><div class="divider" data-v-822684d1><div class="divider-line" data-v-822684d1></div></div></div><!----></header><div class="VPLocalNav has-sidebar empty" data-v-a9a9e638 data-v-070ab83d><div class="container" data-v-070ab83d><button class="menu" aria-expanded="false" aria-controls="VPSidebarNav" data-v-070ab83d><span class="vpi-align-left menu-icon" data-v-070ab83d></span><span class="menu-text" data-v-070ab83d>Menu</span></button><div class="VPLocalNavOutlineDropdown" style="--vp-vh:0px;" data-v-070ab83d data-v-168ddf5d><button data-v-168ddf5d>Return to top</button><!----></div></div></div><aside class="VPSidebar" data-v-a9a9e638 data-v-18756405><div class="curtain" data-v-18756405></div><nav class="nav" id="VPSidebarNav" aria-labelledby="sidebar-aria-label" tabindex="-1" data-v-18756405><span class="visually-hidden" id="sidebar-aria-label" data-v-18756405> Sidebar Navigation </span><!--[--><!--]--><!--[--><div class="no-transition group" data-v-9e426adc><section class="VPSidebarItem level-0 collapsible has-active" data-v-9e426adc data-v-a4b0d9bf><div class="item" role="button" tabindex="0" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><h2 class="text" data-v-a4b0d9bf>API Reference</h2><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-a4b0d9bf><span class="vpi-chevron-right caret-icon" data-v-a4b0d9bf></span></div></div><div class="items" data-v-a4b0d9bf><!--[--><div class="VPSidebarItem level-1 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR2571/api/api" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Reactant API</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR2571/api/sharding" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Sharding</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR2571/api/serialization" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Serialization</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR2571/api/ops" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Ops</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR2571/api/config" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Configuration</p><!--]--></a><!----></div><!----></div><section class="VPSidebarItem level-1 collapsible has-active" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" role="button" tabindex="0" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><h3 class="text" data-v-a4b0d9bf>MLIR Dialects</h3><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-a4b0d9bf><span class="vpi-chevron-right caret-icon" data-v-a4b0d9bf></span></div></div><div class="items" data-v-a4b0d9bf><!--[--><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR2571/api/dialects/arith" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>ArithOps</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR2571/api/dialects/affine" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Affine</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR2571/api/dialects/builtin" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Builtin</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR2571/api/dialects/chlo" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Chlo</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR2571/api/dialects/complex" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Complex</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR2571/api/dialects/cuda_tile" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>CUDA Tile</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR2571/api/dialects/enzyme" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Enzyme</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR2571/api/dialects/enzymexla" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>EnzymeXLA</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR2571/api/dialects/func" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Func</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR2571/api/dialects/gpu" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>GPU</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR2571/api/dialects/llvm" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>LLVM</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR2571/api/dialects/mpi" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>MPI</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR2571/api/dialects/memref" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>MemRef</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR2571/api/dialects/mosaicgpu" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Mosaic GPU</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR2571/api/dialects/nvvm" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>NVVM</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR2571/api/dialects/shape" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Shape</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR2571/api/dialects/shardy" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Shardy</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR2571/api/dialects/sparsetensor" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>SparseTensor</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR2571/api/dialects/stablehlo" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>StableHLO</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR2571/api/dialects/tensor" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Tensor</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR2571/api/dialects/triton" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Triton</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR2571/api/dialects/tritonext" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>TritonExt</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR2571/api/dialects/tpu" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>TPU</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR2571/api/dialects/vhlo" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>VHLO</p><!--]--></a><!----></div><!----></div><!--]--></div></section><section class="VPSidebarItem level-1 collapsible" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" role="button" tabindex="0" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><h3 class="text" data-v-a4b0d9bf>Low-Level API</h3><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-a4b0d9bf><span class="vpi-chevron-right caret-icon" data-v-a4b0d9bf></span></div></div><div class="items" data-v-a4b0d9bf><!--[--><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR2571/api/mlirc" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>MLIR API</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR2571/api/xla" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>XLA</p><!--]--></a><!----></div><!----></div><!--]--></div></section><div class="VPSidebarItem level-1 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR2571/api/internal" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Internal API</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><!--]--><!--[--><!--]--></nav></aside><div class="VPContent has-sidebar" id="VPContent" data-v-a9a9e638 data-v-91765379><div class="VPDoc has-sidebar has-aside" data-v-91765379 data-v-83890dd9><!--[--><!--]--><div class="container" data-v-83890dd9><div class="aside" data-v-83890dd9><div class="aside-curtain" data-v-83890dd9></div><div class="aside-container" data-v-83890dd9><div class="aside-content" data-v-83890dd9><div class="VPDocAside" data-v-83890dd9 data-v-6d7b3c46><!--[--><!--]--><!--[--><!--]--><nav aria-labelledby="doc-outline-aria-label" class="VPDocAsideOutline" data-v-6d7b3c46 data-v-b38bf2ff><div class="content" data-v-b38bf2ff><div class="outline-marker" data-v-b38bf2ff></div><div aria-level="2" class="outline-title" id="doc-outline-aria-label" role="heading" data-v-b38bf2ff>On this page</div><ul class="VPDocOutlineItem root" data-v-b38bf2ff data-v-3f927ebe><!--[--><!--]--></ul></div></nav><!--[--><!--]--><div class="spacer" data-v-6d7b3c46></div><!--[--><!--[--><!--[--><!--[--><!--[--><br><h2> Trusted by </h2><a class="enjoyer" href="https://lux.csail.mit.edu/" target="_blank"><img width="32" height="32" src="https://raw.githubusercontent.com/LuxDL/Lux.jl/refs/heads/main/assets/lux-logo.svg"><span><p class="extra-info">Scientific Computing</p><p class="heading">Lux.jl</p><p class="extra-info">Machine Learning</p></span></a><!--]--><!--]--><!--]--><!--]--><!--]--><!----><!--[--><!--]--><!--[--><!--]--></div></div></div></div><div class="content" data-v-83890dd9><div class="content-container" data-v-83890dd9><!--[--><!--]--><main class="main" data-v-83890dd9><div style="position:relative;" class="vp-doc _Reactant_jl_previews_PR2571_api_dialects_tensor" data-v-83890dd9><div><h1 id="Tensor-Dialect" tabindex="-1">Tensor Dialect <a class="header-anchor" href="#Tensor-Dialect" aria-label="Permalink to &quot;Tensor Dialect {#Tensor-Dialect}&quot;">​</a></h1><p>Refer to the <a href="https://mlir.llvm.org/docs/Dialects/TensorOps/" target="_blank" rel="noreferrer">official documentation</a> for more details.</p><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.tensor.bitcast-Tuple{Reactant.MLIR.IR.Value}" href="#Reactant.MLIR.Dialects.tensor.bitcast-Tuple{Reactant.MLIR.IR.Value}"><span class="jlbinding">Reactant.MLIR.Dialects.tensor.bitcast</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>bitcast</code></p><p>Bitcast a tensor from one type to another type of equivalent element width. If both are ranked, then the rank should be the same and static dimensions should match.</p><p><strong>Example</strong></p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>// Bitcast from unsigned to signed or signless integer.</span></span>
<span class="line"><span>%2 = tensor.bitcast %1 : tensor&lt;4xui32&gt; to tensor&lt;4xi32&gt;</span></span></code></pre></div><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/e81f65c2f980d0e1afc308dcdb424251184cefc1/src/mlir/Dialects/Tensor.jl#L16-L29" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.tensor.cast-Tuple{Reactant.MLIR.IR.Value}" href="#Reactant.MLIR.Dialects.tensor.cast-Tuple{Reactant.MLIR.IR.Value}"><span class="jlbinding">Reactant.MLIR.Dialects.tensor.cast</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>cast</code></p><p>Convert a tensor from one type to an equivalent type without changing any data elements. The source and destination types must both be tensor types with the same element type. If both are ranked, then the rank should be the same and static dimensions should match. The operation is invalid if converting to a mismatching constant dimension.</p><p><strong>Example</strong></p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>// Convert from unknown rank to rank 2 with unknown dimension sizes.</span></span>
<span class="line"><span>%2 = tensor.cast %1 : tensor&lt;*xf32&gt; to tensor&lt;?x?xf32&gt;</span></span>
<span class="line"><span></span></span>
<span class="line"><span>// Convert to a type with more known dimensions.</span></span>
<span class="line"><span>%3 = tensor.cast %2 : tensor&lt;?x?xf32&gt; to tensor&lt;4x?xf32&gt;</span></span>
<span class="line"><span></span></span>
<span class="line"><span>// Discard static dimension and rank information.</span></span>
<span class="line"><span>%4 = tensor.cast %3 : tensor&lt;4x?xf32&gt; to tensor&lt;?x?xf32&gt;</span></span>
<span class="line"><span>%5 = tensor.cast %4 : tensor&lt;?x?xf32&gt; to tensor&lt;*xf32&gt;</span></span></code></pre></div><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/e81f65c2f980d0e1afc308dcdb424251184cefc1/src/mlir/Dialects/Tensor.jl#L49-L71" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.tensor.collapse_shape-Tuple{Reactant.MLIR.IR.Value}" href="#Reactant.MLIR.Dialects.tensor.collapse_shape-Tuple{Reactant.MLIR.IR.Value}"><span class="jlbinding">Reactant.MLIR.Dialects.tensor.collapse_shape</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>collapse_shape</code></p><p>The <code>tensor.collapse_shape</code> op produces a new tensor of lower (or equal) rank whose dimension sizes are a reassociation of the original <code>src</code> dimensions.</p><p>A reassociation is defined as a continuous grouping of dimensions and is represented by an array of DenseI64ArrayAttr attribute. The reassociation maps are applied to the operand shape to obtain the result shape.</p><p><strong>Example</strong></p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>// Dimension collapse (i, j) -&gt; i&#39; and k -&gt; k&#39;</span></span>
<span class="line"><span>%b = tensor.collapse_shape %a [[0, 1], [2]]</span></span>
<span class="line"><span>    : tensor&lt;?x?x?xf32&gt; into tensor&lt;?x?xf32&gt;</span></span></code></pre></div><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/e81f65c2f980d0e1afc308dcdb424251184cefc1/src/mlir/Dialects/Tensor.jl#L91-L109" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.tensor.concat-Tuple{Vector{Reactant.MLIR.IR.Value}}" href="#Reactant.MLIR.Dialects.tensor.concat-Tuple{Vector{Reactant.MLIR.IR.Value}}"><span class="jlbinding">Reactant.MLIR.Dialects.tensor.concat</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>concat</code></p><p>The &quot;concat&quot; operation constructs a tensor out of a variadic list of input tensors, concatenated along a static dimension number. All inputs and the result type must share the same rank.</p><p><code>dim</code> specifies the dimension along which to concatenate. The size of the concatenated dimension in the result must be equal to the sum of the sizes of the inputs along that dimension. All other dimensions in both the inputs and result must be the same size.</p><p><strong>Example</strong></p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>%0 = tensor.concat dim(0) %0, %1, %2 :</span></span>
<span class="line"><span>    (tensor&lt;3x6xf32&gt;, tensor&lt;3x6xf32&gt;, tensor&lt;1x6xf32) -&gt; tensor&lt;7x6xf32&gt;</span></span>
<span class="line"><span></span></span>
<span class="line"><span>// Dynamic + dynamic -&gt; static</span></span>
<span class="line"><span>%0 = tensor.concat dim(1) %0, %1, %2 :</span></span>
<span class="line"><span>    (tensor&lt;3x?xf32&gt;, tensor&lt;3x2xf32&gt;, tensor&lt;3x?xf32) -&gt; tensor&lt;3x10xf32&gt;</span></span></code></pre></div><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/e81f65c2f980d0e1afc308dcdb424251184cefc1/src/mlir/Dialects/Tensor.jl#L129-L151" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.tensor.dim-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}" href="#Reactant.MLIR.Dialects.tensor.dim-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}"><span class="jlbinding">Reactant.MLIR.Dialects.tensor.dim</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>dim</code></p><p>The <code>tensor.dim</code> operation takes a tensor and a dimension operand of type <code>index</code>. It returns the size of the requested dimension of the given tensor. If the dimension index is out of bounds, the behavior is undefined.</p><p>The specified tensor type is that of the first operand.</p><p><strong>Example</strong></p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>// Always returns 4, can be constant folded:</span></span>
<span class="line"><span>%c0 = arith.constant 0 : index</span></span>
<span class="line"><span>%x = tensor.dim %A, %c0 : tensor&lt;4x?xf32&gt;</span></span>
<span class="line"><span></span></span>
<span class="line"><span>// Return the dynamic dimension of %A.</span></span>
<span class="line"><span>%c1 = arith.constant 1 : index</span></span>
<span class="line"><span>%y = tensor.dim %A, %c1 : tensor&lt;4x?xf32&gt;</span></span>
<span class="line"><span></span></span>
<span class="line"><span>// Equivalent generic form:</span></span>
<span class="line"><span>%x = &quot;tensor.dim&quot;(%A, %c0) : (tensor&lt;4x?xf32&gt;, index) -&gt; index</span></span>
<span class="line"><span>%y = &quot;tensor.dim&quot;(%A, %c1) : (tensor&lt;4x?xf32&gt;, index) -&gt; index</span></span></code></pre></div><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/e81f65c2f980d0e1afc308dcdb424251184cefc1/src/mlir/Dialects/Tensor.jl#L171-L195" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.tensor.empty-Tuple{Vector{Reactant.MLIR.IR.Value}}" href="#Reactant.MLIR.Dialects.tensor.empty-Tuple{Vector{Reactant.MLIR.IR.Value}}"><span class="jlbinding">Reactant.MLIR.Dialects.tensor.empty</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>empty</code></p><p><code>tensor.empty</code> is an operation that defines a tensor of a particular shape. The shape could be dynamic or static. The contents of the tensor are unspecified and the only purpose of the op result is to materialize the specified shape in IR and make it available to other transformations.</p><p><code>tensor.empty</code> is useful in transformations that expect destination style ops. I.e., ops that implement <code>DestinationStyleOpInterface</code>. Ops that are not in destination style can be made compatible with such transformations with a <code>tensor.empty</code> destination.</p><p>Note: This op can be lowered to a <code>bufferization.alloc_tensor</code>, at which point it turns into an explicit buffer allocation.</p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/e81f65c2f980d0e1afc308dcdb424251184cefc1/src/mlir/Dialects/Tensor.jl#L218-L233" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.tensor.expand_shape-Tuple{Reactant.MLIR.IR.Value, Vector{Reactant.MLIR.IR.Value}}" href="#Reactant.MLIR.Dialects.tensor.expand_shape-Tuple{Reactant.MLIR.IR.Value, Vector{Reactant.MLIR.IR.Value}}"><span class="jlbinding">Reactant.MLIR.Dialects.tensor.expand_shape</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>expand_shape</code></p><p>The <code>tensor.expand_shape</code> op produces a tensor of higher (or equal) rank than the operand <code>src</code> whose dimension sizes are a reassociation of <code>src</code>.</p><p>A reassociation is defined as a continuous grouping of dimensions and is represented with an array of DenseI64ArrayAttr attribute. The reassociation maps applied to the result tensor with the higher rank must result in the operand tensor with the smaller rank.</p><p>The representation for the output shape supports a partially-static specification via attributes specified through the <code>static_output_shape</code> argument. A special sentinel value <code>ShapedType::kDynamic</code> encodes that the corresponding entry has a dynamic value. There must be exactly as many SSA inputs in <code>output_shape</code> as there are <code>ShapedType::kDynamic</code> entries in <code>static_output_shape</code>.</p><p><strong>Example</strong></p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>// Dimension expansion i -&gt; (i&#39;, j&#39;) and (k) -&gt; (k&#39;)</span></span>
<span class="line"><span>%b = tensor.expand_shape %a [[0, 1], [2]] output_shape [%sz0, %sz1, 32]</span></span>
<span class="line"><span>    : tensor&lt;?x32xf32&gt; into tensor&lt;?x?x32xf32&gt;</span></span></code></pre></div><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/e81f65c2f980d0e1afc308dcdb424251184cefc1/src/mlir/Dialects/Tensor.jl#L253-L279" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.tensor.extract-Tuple{Reactant.MLIR.IR.Value, Vector{Reactant.MLIR.IR.Value}}" href="#Reactant.MLIR.Dialects.tensor.extract-Tuple{Reactant.MLIR.IR.Value, Vector{Reactant.MLIR.IR.Value}}"><span class="jlbinding">Reactant.MLIR.Dialects.tensor.extract</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>extract</code></p><p>The <code>tensor.extract</code> op reads a ranked tensor and returns one element as specified by the given indices. The result of the op is a value with the same type as the elements of the tensor. The arity of indices must match the rank of the accessed value. All indices should all be of <code>index</code> type.</p><p><strong>Example</strong></p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>%4 = tensor.extract %t[%1, %2] : tensor&lt;4x4xi32&gt;</span></span>
<span class="line"><span>%5 = tensor.extract %rt[%1, %2] : tensor&lt;?x?xi32&gt;</span></span></code></pre></div><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/e81f65c2f980d0e1afc308dcdb424251184cefc1/src/mlir/Dialects/Tensor.jl#L309-L323" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.tensor.extract_slice-Tuple{Reactant.MLIR.IR.Value, Vector{Reactant.MLIR.IR.Value}, Vector{Reactant.MLIR.IR.Value}, Vector{Reactant.MLIR.IR.Value}}" href="#Reactant.MLIR.Dialects.tensor.extract_slice-Tuple{Reactant.MLIR.IR.Value, Vector{Reactant.MLIR.IR.Value}, Vector{Reactant.MLIR.IR.Value}, Vector{Reactant.MLIR.IR.Value}}"><span class="jlbinding">Reactant.MLIR.Dialects.tensor.extract_slice</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>extract_slice</code></p><p>The &quot;extract_slice&quot; operation extract a tensor from another tensor as specified by the operation&#39;s offsets, sizes and strides arguments.</p><p>The extract_slice operation supports the following arguments:</p><ul><li><p>source: the &quot;base&quot; tensor from which to extract a slice.</p></li><li><p>offsets: tensor-rank number of offsets into the &quot;base&quot; tensor from which to extract the slice.</p></li><li><p>sizes: tensor-rank number of sizes which specify the sizes of the result tensor type.</p></li><li><p>strides: tensor-rank number of strides specifying subsampling in each dimension.</p></li></ul><p>The representation based on offsets, sizes and strides support a partially-static specification via attributes specified through the <code>static_offsets</code>, <code>static_sizes</code> and <code>static_strides</code> arguments. A special sentinel value ShapedType::kDynamic encodes that the corresponding entry has a dynamic value.</p><p>After buffer allocation, the &quot;extract_slice&quot; op is expected to lower into a memref.subview op.</p><p>An extract_slice operation may additionally reduce the rank of the resulting tensor by removing dimensions that are statically known to be of size 1. This rank-reduction behavior is not required by the op semantics: this flexibility allows to progressively drop unit dimensions while lowering between different flavors of ops on that operate on tensors.</p><p><strong>Verification vs Inference in the rank-reduced case</strong></p><p>Note that there may be multiple ways to infer a resulting rank-reduced type. e.g. 1x6x1 could potentially rank-reduce to either 1x6 or 6x1 2-D shapes.</p><p>To disambiguate, the inference helpers <code>inferCanonicalRankReducedResultType</code> only drop the first unit dimensions, in order: e.g. 1x6x1 rank-reduced to 2-D will infer the 6x1 2-D shape, but not 1x6.</p><p>Verification however has access to result type and does not need to infer. The verifier calls <code>isRankReducedType(getSource(), getResult())</code> to determine whether the result type is rank-reduced from the source type. This computes a so-called rank-reduction mask, consisting of dropped unit dims, to map the rank-reduced type to the source type by dropping ones: e.g. 1x6 is a rank-reduced version of 1x6x1 by mask {2} 6x1 is a rank-reduced version of 1x6x1 by mask {0} 1x2x1x4 is a rank-reduced version of 1x1x2x1x1x4x1 by mask {1, 4, 6} (remaining common 1 dimensions are matched eagerly)</p><p><strong>Example</strong></p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>// Rank-reducing extract_slice.</span></span>
<span class="line"><span>%1 = tensor.extract_slice %0[0, 0, 0][1, 16, 4][1, 1, 1] :</span></span>
<span class="line"><span>  tensor&lt;8x16x4xf32&gt; to tensor&lt;16x4xf32&gt;</span></span>
<span class="line"><span>%3 = tensor.extract_slice %2[%o0, 4, %o2][1, %sz1, 1][1, %st1, 1] :</span></span>
<span class="line"><span>  tensor&lt;8x16x4xf32&gt; to tensor&lt;1x?xf32&gt;</span></span></code></pre></div><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/e81f65c2f980d0e1afc308dcdb424251184cefc1/src/mlir/Dialects/Tensor.jl#L349-L408" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.tensor.from_elements-Tuple{Vector{Reactant.MLIR.IR.Value}}" href="#Reactant.MLIR.Dialects.tensor.from_elements-Tuple{Vector{Reactant.MLIR.IR.Value}}"><span class="jlbinding">Reactant.MLIR.Dialects.tensor.from_elements</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>from_elements</code></p><p>Create a N-D tensor from a range of same-type arguments. The number of provided <code>elements</code> should equal to the number of the elements in the result type. The <code>elements</code> correspond to a flattened tensor.</p><p><strong>Example</strong></p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>tensor.from_elements %a, %b, %c, %d, %e, %f :  tensor&lt;2x3xindex&gt;</span></span></code></pre></div><p>will result in a tensor</p><p>[[%a, %b, %c] [%d, %e, %f]]</p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/e81f65c2f980d0e1afc308dcdb424251184cefc1/src/mlir/Dialects/Tensor.jl#L446-L463" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.tensor.gather-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}" href="#Reactant.MLIR.Dialects.tensor.gather-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}"><span class="jlbinding">Reactant.MLIR.Dialects.tensor.gather</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>gather</code></p><p>The <code>gather</code> operation extracts a subset of the elements from a <code>source</code> tensor at the given indices.</p><p>In its most general form, the tensor of indices specifies all the coordinates of every element to extract (i.e. COO format, without the payload). The indices are expected to be confined to coordinate values that fit the range of the <code>source</code> tensor, otherwise the behavior is undefined.</p><p>The leading dimensions of the index tensor give the result tensor its leading dimensions. The trailing dimensions of the result tensor are obtained from the source tensor by omitting the dimensions specified in <code>gather_dims</code> (rank-reducing semantics) or setting them to <code>1</code> (rank-preserving semantics) (see examples). The trailing dimension of the index tensor contains the coordinates and is expected to have its size equal to the number of dimensions being gathered. This convention allows an idiomatic specification and lowering of &quot;gathering multiple N-D slices from the source tensor&quot;.</p><p>Note: in the examples below, we separate out the indexing part of the tensor type by a whitespace for readability purposes.</p><p><strong>Example</strong></p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>    // For each 1x2 triple of coordinates in %indices, extract the</span></span>
<span class="line"><span>    // element (i.e. 0-D subset) at the coordinates triple in %source.</span></span>
<span class="line"><span>    //</span></span>
<span class="line"><span>    %out = tensor.gather %source[%indices] gather_dims([0, 1, 2]) :</span></span>
<span class="line"><span>      (tensor&lt;4x4x4xf32&gt;, tensor&lt;1x2x 3xindex&gt;) -&gt; tensor&lt;1x2x 1x1x1xf32&gt;</span></span>
<span class="line"><span></span></span>
<span class="line"><span>    // Note: result type may be further rank-reduced to tensor&lt;1x2x f32&gt;.</span></span></code></pre></div><p>A slice variant is provided to allow specifying whole slices of the source tensor.</p><p><strong>Example</strong></p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>    // For each 5x6 singleton of coordinates in %indices, extract the 2-D</span></span>
<span class="line"><span>    // slice %source[*, %indices[...]:%indices[...] + 1, *] with the indices</span></span>
<span class="line"><span>    // corresponding to the `gather_dims` attribute specified by %indices.</span></span>
<span class="line"><span>    //</span></span>
<span class="line"><span>    %out = tensor.gather %source[%indices] gather_dims([1]) :</span></span>
<span class="line"><span>      (tensor&lt;3x4x5xf32&gt;, tensor&lt;6x7x 1xindex&gt;) -&gt; tensor&lt;6x7x 3x1x5xf32&gt;</span></span>
<span class="line"><span></span></span>
<span class="line"><span>    // Note: result type may be further rank-reduced to tensor&lt;6x7x 3x5xf32&gt;.</span></span></code></pre></div><p>The dimensions specified in the gather_dims attribute are ones for which the result tensor has size <code>1</code>. I.e. if the source type is <code>axbxcxd</code> and the coordinates are [1, 3], then the shape suffix is <code>ax1xcx1</code>. Gather also allows rank-reducing semantics where the shape <code>ax1xcx1</code> can be further simplified to <code>axc</code>.</p><p>The elemental type of the indices tensor can be any integer type. In the absence of target-specific or problem specific information the default type one should use is <code>index</code>.</p><p>This operation does not support unranked tensors.</p><p>An optional <code>unique</code> unit attribute may be specified to indicate that the coordinates in <code>indices</code> are statically guaranteed to be unique at runtime. Incorrectly setting the <code>unique</code> attribute when the coordinates are not truly unique is undefined behavior.</p><p>Only full slices are meant to be supported by this op, if one desires partial slices (e.g. strided windows) one should compose this op with other tensor ops (e.g. tensor.extract_slice). This is to avoid a slippery slope of complexity that would make the op unusable in practice.</p><p>At the tensor-level, the index tensor is specified in an AoS form (i.e. coordinate tuple is the most minor). It is the responsibility of further lowerings and bufferization to implement various concrete layouts.</p><p>Note: As currently specified, the operation must lower to an abstraction that performs copies to the output tensor. This is because the buffer type system is currently not rich enough to allow multiple non-contiguous views in the same type. This is visible more clearly in a notional buffer version of the op:</p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>    // memref&lt;?x4x1xf32&gt; is a contiguous buffer of ?x4x1 elements.</span></span>
<span class="line"><span>    // gather from random source slices must copy to the contiguous output.</span></span>
<span class="line"><span>    %out = memref.gather %source[%indices] gather_dims([1]) :</span></span>
<span class="line"><span>      (memref&lt;4x4xf32&gt;, memref&lt;?x 1xindex&gt;) -&gt; memref&lt;?x 4x1xf32&gt;</span></span>
<span class="line"><span></span></span>
<span class="line"><span>    // Nested buffer support would allow gather to directly index into the</span></span>
<span class="line"><span>    // source buffer (i.e. represent a jagged view into the source).</span></span>
<span class="line"><span>    %out = memref.gather %source[%indices] gather_dims([1]) :</span></span>
<span class="line"><span>      (memref&lt;4x4xf32&gt;, memref&lt;?x 1xindex&gt;) -&gt; memref&lt;? x memref&lt;4x1xf32&gt;&gt;</span></span></code></pre></div><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/e81f65c2f980d0e1afc308dcdb424251184cefc1/src/mlir/Dialects/Tensor.jl#L483-L579" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.tensor.generate-Tuple{Vector{Reactant.MLIR.IR.Value}}" href="#Reactant.MLIR.Dialects.tensor.generate-Tuple{Vector{Reactant.MLIR.IR.Value}}"><span class="jlbinding">Reactant.MLIR.Dialects.tensor.generate</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>generate</code></p><p>This operation creates a dynamically sized tensor with elements of any type. It expects one index operand per dynamic extent of the result tensor.</p><p>The body region defines the tensor&#39;s elements. It takes index operands as its region arguments that span the index space. The element at the given position is yielded with the <code>yield</code> operation (see <code>YieldOp</code>). There is no defined ordering to the invocations of the body. It is conceptually a &quot;parallel map&quot; operation.</p><p><strong>Example</strong></p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>  %tnsr = tensor.generate %m, %n {</span></span>
<span class="line"><span>  ^bb0(%i : index, %j : index, %k : index):</span></span>
<span class="line"><span>    ...</span></span>
<span class="line"><span>    yield %elem : f32</span></span>
<span class="line"><span>  } : tensor&lt;?x3x?f32&gt;</span></span></code></pre></div><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/e81f65c2f980d0e1afc308dcdb424251184cefc1/src/mlir/Dialects/Tensor.jl#L607-L628" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.tensor.insert-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value, Vector{Reactant.MLIR.IR.Value}}" href="#Reactant.MLIR.Dialects.tensor.insert-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value, Vector{Reactant.MLIR.IR.Value}}"><span class="jlbinding">Reactant.MLIR.Dialects.tensor.insert</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>insert</code></p><p>The <code>tensor.insert</code> op inserts a scalar into a ranked tensor <code>dest</code> as specified by the operation&#39;s indices.</p><p>It returns a copy of <code>dest</code> with the indexed position updated to the value of <code>scalar</code>.</p><p>The arity of <code>indices</code>must match the rank of the tensor <code>dest</code>. All indices should be of <code>index</code> type.</p><p><strong>Example</strong></p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>%4 = tensor.insert %t into %dest[%1, %2] : tensor&lt;4x4xi32&gt;</span></span>
<span class="line"><span>%5 = tensor.insert %rt into %dest[%1, %2] : tensor&lt;?x?xi32&gt;</span></span></code></pre></div><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/e81f65c2f980d0e1afc308dcdb424251184cefc1/src/mlir/Dialects/Tensor.jl#L650-L668" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.tensor.insert_slice-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value, Vector{Reactant.MLIR.IR.Value}, Vector{Reactant.MLIR.IR.Value}, Vector{Reactant.MLIR.IR.Value}}" href="#Reactant.MLIR.Dialects.tensor.insert_slice-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value, Vector{Reactant.MLIR.IR.Value}, Vector{Reactant.MLIR.IR.Value}, Vector{Reactant.MLIR.IR.Value}}"><span class="jlbinding">Reactant.MLIR.Dialects.tensor.insert_slice</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>insert_slice</code></p><p>The &quot;insert_slice&quot; operation insert a tensor <code>source</code> into another tensor <code>dest</code> as specified by the operation&#39;s offsets, sizes and strides arguments.</p><p>It returns a copy of <code>dest</code> with the proper slice updated with the value of <code>source</code>.</p><p>The insert_slice operation supports the following arguments:</p><ul><li><p>source: the tensor that is inserted.</p></li><li><p>dest: the tensor into which the source tensor is inserted.</p></li><li><p>offsets: tensor-rank number of offsets into the <code>dest</code> tensor into which the slice is inserted.</p></li><li><p>sizes: tensor-rank number of sizes which specify the sizes of the source tensor type.</p></li><li><p>strides: tensor-rank number of strides that specify subsampling in each dimension.</p></li></ul><p>The representation based on offsets, sizes and strides support a partially-static specification via attributes specified through the <code>static_offsets</code>, <code>static_sizes</code> and <code>static_strides</code> arguments. A special sentinel value ShapedType::kDynamic encodes that the corresponding entry has a dynamic value.</p><p>After buffer allocation, the &quot;insert_slice&quot; op is expected to lower into a memref.subview op.</p><p>An insert_slice operation may additionally specify insertion into a tensor of higher rank than the source tensor, along dimensions that are statically known to be of size 1. This rank-altering behavior is not required by the op semantics: this flexibility allows to progressively drop unit dimensions while lowering between different flavors of ops on that operate on tensors. The rank-altering behavior of tensor.insert_slice matches the rank-reducing behavior of tensor.extract_slice.</p><p><strong>Verification in the rank-reduced case</strong></p><p>The same verification discussion and mechanisms apply as for ExtractSliceOp. Unlike ExtractSliceOp however, there is no need for a specific inference.</p><p><strong>Example</strong></p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>// Rank-altering insert_slice.</span></span>
<span class="line"><span>%1 = tensor.insert_slice %t into %0[0, 0, 0][1, 16, 4][1, 1, 1] :</span></span>
<span class="line"><span>  tensor&lt;16x4xf32&gt; into tensor&lt;8x16x4xf32&gt;</span></span>
<span class="line"><span>%3 = tensor.insert_slice %tt into %2[%o0, 4, %o2][1, %sz1, 1][1, %st1, 1] :</span></span>
<span class="line"><span>  tensor&lt;1x?xf32&gt; into tensor&lt;8x16x4xf32&gt;</span></span></code></pre></div><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/e81f65c2f980d0e1afc308dcdb424251184cefc1/src/mlir/Dialects/Tensor.jl#L695-L748" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.tensor.pad-Tuple{Reactant.MLIR.IR.Value, Vector{Reactant.MLIR.IR.Value}, Vector{Reactant.MLIR.IR.Value}}" href="#Reactant.MLIR.Dialects.tensor.pad-Tuple{Reactant.MLIR.IR.Value, Vector{Reactant.MLIR.IR.Value}, Vector{Reactant.MLIR.IR.Value}}"><span class="jlbinding">Reactant.MLIR.Dialects.tensor.pad</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>pad</code></p><p><code>tensor.pad</code> is an operation that pads the <code>source</code> tensor with given <code>low</code> and <code>high</code> padding config.</p><p>The PadOp operation supports the following arguments:</p><ul><li><p>source: the &quot;base&quot; tensor on which to pad.</p></li><li><p>low: A list contains the padding along the start of each dimension, i.e., how many padded values are prepended to the beginning of the tensor in each dimension.</p></li><li><p>high: A list contains the padding along the end of each dimension, i.e., how many padded values are appended to the end of the tensor in each dimension.</p></li><li><p>nofold: indicates that the operation should not be folded when source and result types are equal.</p></li></ul><p>The result tensor dimensions are <code>low[i]</code> + <code>dim[i]</code> + <code>high[i]</code> for each dimension <code>i</code>. The number of elements of <code>low</code> and <code>high</code> must match the rank of the input tensor. They can be either a constant or a dynamic value.</p><p>The region of the <code>tensor.pad</code> operation returns the value to use for the padding. The arguments of the region represent the index of the source being accessed. There should be as many arguments as the rank of the <code>source</code> tensor. The value <code>yield</code>-ed by the region is used as the value of the view at the given position.</p><p>If <code>nofold</code> is set, the padding operation will not be folded away even if the source type and the padded type have the same static shape. This can be used, e.g., for packing or promotion to faster memory.</p><p>Example 1: add 3 zeros to the beginning and 5 zeros to the end of a 1D tensor.</p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>  %arg0 = ... : tensor&lt;10xi32&gt;</span></span>
<span class="line"><span>  %c0_i32 = arith.constant 0 : i32</span></span>
<span class="line"><span>  %padded = tensor.pad %arg0 low[3] high[5] {</span></span>
<span class="line"><span>  ^bb0(%arg1: index):</span></span>
<span class="line"><span>    tensor.yield %c0_i32 : i32</span></span>
<span class="line"><span>  } : tensor&lt;10xi32&gt; to tensor&lt;18xi32&gt;</span></span></code></pre></div><p>Example 2: add 1 value to the beginning of dimension 0, 2 values to the end of dimension 0, 2 values to the start of dimension 1, and 3 values to the end of dimension 1.</p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>  %pad_value = ... : f32</span></span>
<span class="line"><span>  %0 = tensor.pad %0 low[1, 2] high[2, 3] {</span></span>
<span class="line"><span>  ^bb0(%arg0 : index, %arg1 : index):</span></span>
<span class="line"><span>    tensor.yield %pad_value : f32</span></span>
<span class="line"><span>  } : tensor&lt;?x?xf32&gt; to tensor&lt;?x?xf32&gt;</span></span></code></pre></div><p>Example 3:</p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>  %pad_value = ... : f32</span></span>
<span class="line"><span>  %0 = tensor.pad %arg0 low[2, %arg1, 3, 3] high[3, 3, %arg1, 2] {</span></span>
<span class="line"><span>  ^bb0(%arg2: index, %arg3: index, %arg4: index, %arg5: index):</span></span>
<span class="line"><span>      tensor.yield %pad_value : f32</span></span>
<span class="line"><span>  } : tensor&lt;1x2x2x?xf32&gt; to tensor&lt;6x?x?x?xf32&gt;</span></span></code></pre></div><p>Example 4:</p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>  %pad_value = ... : f32</span></span>
<span class="line"><span>  %0 = tensor.pad %arg0 low[0, 0] high[%ub0, %ub1] {</span></span>
<span class="line"><span>  ^bb0(%arg1: index, %arg2: index):</span></span>
<span class="line"><span>    tensor.yield %pad_value : f32</span></span>
<span class="line"><span>  } : tensor&lt;2x3xf32&gt; to tensor&lt;?x?xf32&gt;</span></span></code></pre></div><p>Example 5: Force a padded value to be always exist with <code>nofold</code>, even though the padding config specifies that no new elements will be added to the tensor.</p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>  %pad_value = ... : f32</span></span>
<span class="line"><span>  %0 = tensor.pad %arg0 nofold low[0, 0] high[0, 0] {</span></span>
<span class="line"><span>  ^bb0(%arg1: index, %arg2: index):</span></span>
<span class="line"><span>    tensor.yield %pad_value : f32</span></span>
<span class="line"><span>  } : tensor&lt;2x3xf32&gt; to tensor&lt;2x3xf32&gt;</span></span></code></pre></div><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/e81f65c2f980d0e1afc308dcdb424251184cefc1/src/mlir/Dialects/Tensor.jl#L788-L875" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.tensor.parallel_insert_slice-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value, Vector{Reactant.MLIR.IR.Value}, Vector{Reactant.MLIR.IR.Value}, Vector{Reactant.MLIR.IR.Value}}" href="#Reactant.MLIR.Dialects.tensor.parallel_insert_slice-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value, Vector{Reactant.MLIR.IR.Value}, Vector{Reactant.MLIR.IR.Value}, Vector{Reactant.MLIR.IR.Value}}"><span class="jlbinding">Reactant.MLIR.Dialects.tensor.parallel_insert_slice</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>parallel_insert_slice</code></p><p>The <code>parallel_insert_slice</code> yields a subset tensor value to its parent InParallelOpInterface. These subset tensor values are aggregated to in some unspecified order into a full tensor value returned by the parent parallel iterating op. The <code>parallel_insert_slice</code> is one such op allowed in the InParallelOpInterface op.</p><p>Conflicting writes result in undefined semantics, in that the indices written to by multiple parallel updates might contain data from any of the updates, or even a malformed bit pattern.</p><p>If an index is updated exactly once, the value contained at that index in the resulting tensor will be equal to the value at a corresponding index of a slice that was used for the updated. If an index is not updated at all, its value will be equal to the one in the original tensor.</p><p>This op does not create a new value, which allows maintaining a clean separation between the subset and full tensor.</p><p>Note that we cannot mark this operation as pure (Pures), even though it has no side effects, because it will get DCEd during canonicalization.</p><p>The parallel_insert_slice operation supports the following arguments:</p><ul><li><p>source: the tensor that is inserted.</p></li><li><p>dest: the tensor into which the source tensor is inserted.</p></li><li><p>offsets: tensor-rank number of offsets into the <code>dest</code> tensor into which the slice is inserted.</p></li><li><p>sizes: tensor-rank number of sizes which specify the sizes of the source tensor type.</p></li><li><p>strides: tensor-rank number of strides that specify subsampling in each dimension.</p></li></ul><p>The representation based on offsets, sizes and strides support a partially-static specification via attributes specified through the <code>static_offsets</code>, <code>static_sizes</code> and <code>static_strides</code> arguments. A special sentinel value ShapedType::kDynamic encodes that the corresponding entry has a dynamic value.</p><p>After buffer allocation, the &quot;parallel_insert_slice&quot; op is expected to lower into a memref.subview op.</p><p>A parallel_insert_slice operation may additionally specify insertion into a tensor of higher rank than the source tensor, along dimensions that are statically known to be of size 1. This rank-altering behavior is not required by the op semantics: this flexibility allows to progressively drop unit dimensions while lowering between different flavors of ops on that operate on tensors. The rank-altering behavior of tensor.parallel_insert_slice matches the rank-reducing behavior of tensor.insert_slice and tensor.extract_slice.</p><p><strong>Verification in the rank-reduced case</strong></p><p>The same verification discussion and mechanisms apply as for ExtractSliceOp. Unlike ExtractSliceOp however, there is no need for a specific inference.</p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/e81f65c2f980d0e1afc308dcdb424251184cefc1/src/mlir/Dialects/Tensor.jl#L909-L968" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.tensor.rank-Tuple{Reactant.MLIR.IR.Value}" href="#Reactant.MLIR.Dialects.tensor.rank-Tuple{Reactant.MLIR.IR.Value}"><span class="jlbinding">Reactant.MLIR.Dialects.tensor.rank</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>rank</code></p><p>The <code>tensor.rank</code> operation takes a tensor operand and returns its rank.</p><p><strong>Example</strong></p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>%0 = tensor.rank %arg0 : tensor&lt;*xf32&gt;</span></span>
<span class="line"><span>%1 = tensor.rank %arg1 : tensor&lt;?x?xf32&gt;</span></span></code></pre></div><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/e81f65c2f980d0e1afc308dcdb424251184cefc1/src/mlir/Dialects/Tensor.jl#L1006-L1017" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.tensor.reshape-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}" href="#Reactant.MLIR.Dialects.tensor.reshape-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}"><span class="jlbinding">Reactant.MLIR.Dialects.tensor.reshape</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>reshape</code></p><p>The <code>reshape</code> operation converts a tensor from one type to an equivalent type with a provided shape. The source and destination types are compatible if both have the same element type, same number of elements. The following combinations are possible:</p><p>a. Source type is ranked or unranked. Shape argument has static size. Result type is ranked.</p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>// Reshape statically-shaped tensor.</span></span>
<span class="line"><span>%dst = tensor.reshape %src(%shape)</span></span>
<span class="line"><span>         : (tensor&lt;4x1xf32&gt;, tensor&lt;1xi32&gt;) -&gt; tensor&lt;4xf32&gt;</span></span>
<span class="line"><span>%dst0 = tensor.reshape %src(%shape0)</span></span>
<span class="line"><span>         : (tensor&lt;4x1xf32&gt;, tensor&lt;2xi32&gt;) -&gt; tensor&lt;2x2xf32&gt;</span></span>
<span class="line"><span>// Flatten unranked tensor.</span></span>
<span class="line"><span>%dst = tensor.reshape %src(%shape)</span></span>
<span class="line"><span>         : (tensor&lt;*xf32&gt;, tensor&lt;1xi32&gt;) -&gt; tensor&lt;?xf32&gt;</span></span></code></pre></div><p>b. Source type is ranked or unranked. Shape argument has dynamic size. Result type is unranked.</p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>// Reshape dynamically-shaped 1D tensor.</span></span>
<span class="line"><span>%dst = tensor.reshape %src(%shape)</span></span>
<span class="line"><span>         : (tensor&lt;?xf32&gt;, tensor&lt;?xi32&gt;) -&gt; tensor&lt;*xf32&gt;</span></span>
<span class="line"><span>// Reshape unranked tensor.</span></span>
<span class="line"><span>%dst = tensor.reshape %src(%shape)</span></span>
<span class="line"><span>         : (tensor&lt;*xf32&gt;, tensor&lt;?xi32&gt;) -&gt; tensor&lt;*xf32&gt;</span></span></code></pre></div><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/e81f65c2f980d0e1afc308dcdb424251184cefc1/src/mlir/Dialects/Tensor.jl#L1038-L1071" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.tensor.scatter-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}" href="#Reactant.MLIR.Dialects.tensor.scatter-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}"><span class="jlbinding">Reactant.MLIR.Dialects.tensor.scatter</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>scatter</code></p><p>The <code>scatter</code> operation inserts a <code>source</code> tensor into a <code>dest</code> tensor at the given indices.</p><p>In its most general form, the tensor of indices specifies all the coordinates of every element to insert (i.e. COO format, without the payload). The indices are expected to be confined to coordinate values that fit the range of the <code>dest</code> tensor, otherwise the behavior is undefined.</p><p>The leading dimensions of the index tensor must match that of the dest tensor. The trailing dimensions of the dest tensor must match those of the source tensor by omitting the dimensions specified in scatter_dims (rank-reducing semantics) or setting them to <code>1</code> (rank-preserving semantics) (see examples). This convention allows an idiomatic specification and lowering of &quot;scattering multiple N-D slices into the dest tensor&quot;. The result type must match the type of the dest tensor.</p><p>Note: in the examples below, we separate out the indexing part of the tensor type by a whitespace for readability purposes.</p><p><strong>Example</strong></p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>    // For each 1x2 triple of coordinates in %indices, insert the</span></span>
<span class="line"><span>    // element (i.e. 0-D subset) at the coordinates triple in %dest.</span></span>
<span class="line"><span>    //</span></span>
<span class="line"><span>    %out = tensor.scatter %source into %dest[%indices]</span></span>
<span class="line"><span>        scatter_dims([0, 1, 2]) unique :</span></span>
<span class="line"><span>      (tensor&lt;1x2x 1x1x1xf32&gt;, tensor&lt;4x4x4xf32&gt;, tensor&lt;1x2x 3xindex&gt;)</span></span>
<span class="line"><span>        -&gt; tensor&lt;4x4x4xf32&gt;</span></span>
<span class="line"><span></span></span>
<span class="line"><span>    // Note: source type may be further rank-reduced to tensor&lt;1x2x f32&gt;.</span></span></code></pre></div><p>A slice variant is provided to allow specifying insertion of whole tensor slices into the <code>dest</code> tensor.</p><p><strong>Example</strong></p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>    // For each 3 singleton of coordinates in %indices, insert the 2-D</span></span>
<span class="line"><span>    // slice into %dest[*, %indices[...]:%indices[...] + 1, *] with the</span></span>
<span class="line"><span>    // indices corresponding to the scatter_dims attribute specified by</span></span>
<span class="line"><span>    // %indices.</span></span>
<span class="line"><span>    //</span></span>
<span class="line"><span>    %out = tensor.scatter %source into %dest[%indices] scatter_dims([1]) unique :</span></span>
<span class="line"><span>      (tensor&lt;3x 4x1x6xf32&gt;, tensor&lt;4x5x6xf32&gt;, tensor&lt;3x 1xindex&gt;)</span></span>
<span class="line"><span>        -&gt; tensor&lt;4x5x6xf32&gt;</span></span></code></pre></div><p>The dimensions specified in the scatter_dims attribute are ones for which the source tensor has size <code>1</code>. I.e. if the dest type is <code>axbxcxd</code> and the coordinates are [1, 3], then the source type suffix is <code>ax1xcx1</code>. Scatter also allows rank-reducing semantics where the shape <code>ax1xcx1</code> can be further simplified to <code>axc</code>.</p><p>The elemental type of the indices tensor can be any integer type. In the absence of target-specific or problem specific information the default type one should use is <code>index</code>.</p><p>This operation does not support unranked tensors.</p><p>A <code>unique</code> unit attribute must be be specified to indicate that the coordinates are statically guaranteed to be unique at runtime. If coordinates are not truly unique at runtime, the behavior is undefined.</p><p>Only full slices are meant to be supported by this op, if one desires partial slices (e.g. strided windows) one should compose this op with other tensor ops (e.g. tensor.insert_slice). This is to avoid a slippery slope of complexity that would make the op unusable in practice.</p><p>At the tensor-level, the index tensor is specified in an AoS form (i.e. coordinate tuple is the most minor). It is the responsibility of further lowerings and bufferization to implement various concrete layouts.</p><p>Note: As currently specified, the operation must lower to an abstraction that performs copies to the output tensor. This is because the buffer type system is currently not rich enough to allow multiple non-contiguous views in the same type. This is visible more clearly in a notional buffer version of the op:</p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>    // memref&lt;?x 4xf32&gt; is a contiguous buffer of ?x4 elements, scatter into</span></span>
<span class="line"><span>    // random dest slices must copy to the contiguous dest.</span></span>
<span class="line"><span>    //</span></span>
<span class="line"><span>    some_side_effecting_op_writing_into %source, ...: memref&lt;3x 4xf32&gt;</span></span>
<span class="line"><span>    memref.scatter %source into %dest[%indices] scatter_dims([1]) unique :</span></span>
<span class="line"><span>      (memref&lt;3x 4xf32&gt;, memref&lt;?x 4xf32&gt;, memref&lt;?x 1xindex&gt;)</span></span>
<span class="line"><span></span></span>
<span class="line"><span>    // Nested buffer support in the producing op would allow writing directly</span></span>
<span class="line"><span>    // into the dest buffer.</span></span>
<span class="line"><span>    %v = some_nested_buffer_view_op %dest[%indices] scatter_dims([1]) unique :</span></span>
<span class="line"><span>      memref&lt;? x memref&lt;4xf32&gt;&gt;</span></span>
<span class="line"><span>    some_side_effecting_op_writing_into %v, ...: memref&lt;? x memref&lt;4xf32&gt;&gt;</span></span></code></pre></div><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/e81f65c2f980d0e1afc308dcdb424251184cefc1/src/mlir/Dialects/Tensor.jl#L1091-L1190" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.tensor.splat-Tuple{Reactant.MLIR.IR.Value, Vector{Reactant.MLIR.IR.Value}}" href="#Reactant.MLIR.Dialects.tensor.splat-Tuple{Reactant.MLIR.IR.Value, Vector{Reactant.MLIR.IR.Value}}"><span class="jlbinding">Reactant.MLIR.Dialects.tensor.splat</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>splat</code></p><p>Broadcast the operand to all elements of the result tensor.</p><p>An additional argument of type <code>index</code> must be provided for each dynamic dimension present in the result type.</p><p>Example for a statically shaped tensor:</p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>%s = arith.constant 1.0 : f32</span></span>
<span class="line"><span>%t = tensor.splat %s : tensor&lt;8x16xf32&gt;</span></span></code></pre></div><p>Example for a tensor containing dynamic dimensions:</p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>// Broadcasts %s to a 3D dynamically shaped tensor, with %m and %n binding</span></span>
<span class="line"><span>// to dimensions 0 and 2 of the resulting tensor, respectively.</span></span>
<span class="line"><span>%m = arith.constant 10 : index</span></span>
<span class="line"><span>%n = arith.constant 30 : index</span></span>
<span class="line"><span>%t = tensor.splat %s[%m, %n] : tensor&lt;?x20x?xf32&gt;</span></span></code></pre></div><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/e81f65c2f980d0e1afc308dcdb424251184cefc1/src/mlir/Dialects/Tensor.jl#L1219-L1243" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.tensor.yield-Tuple{Reactant.MLIR.IR.Value}" href="#Reactant.MLIR.Dialects.tensor.yield-Tuple{Reactant.MLIR.IR.Value}"><span class="jlbinding">Reactant.MLIR.Dialects.tensor.yield</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>yield</code></p><p>This operation is used to yield a single value from a within a region. It is used to create dynamically sized tensors (see <code>tensor.generate</code> and <code>tensor.pad</code> ops).</p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/e81f65c2f980d0e1afc308dcdb424251184cefc1/src/mlir/Dialects/Tensor.jl#L1265-L1271" target="_blank" rel="noreferrer">source</a><!--]--></span></details></div></div></main><footer class="VPDocFooter" data-v-83890dd9 data-v-4f9813fa><!--[--><!--]--><div class="edit-info" data-v-4f9813fa><div class="edit-link" data-v-4f9813fa><a class="VPLink link vp-external-link-icon no-icon edit-link-button" href="https://github.com/EnzymeAD/Reactant.jl/edit/main/docs/src/api/dialects/tensor.md" target="_blank" rel="noreferrer" data-v-4f9813fa><!--[--><span class="vpi-square-pen edit-link-icon" data-v-4f9813fa></span> Edit this page on GitHub<!--]--></a></div><!----></div><nav class="prev-next" aria-labelledby="doc-footer-aria-label" data-v-4f9813fa><span class="visually-hidden" id="doc-footer-aria-label" data-v-4f9813fa>Pager</span><div class="pager" data-v-4f9813fa><a class="VPLink link pager-link prev" href="/Reactant.jl/previews/PR2571/api/dialects/stablehlo" data-v-4f9813fa><!--[--><span class="desc" data-v-4f9813fa>Previous page</span><span class="title" data-v-4f9813fa>StableHLO</span><!--]--></a></div><div class="pager" data-v-4f9813fa><a class="VPLink link pager-link next" href="/Reactant.jl/previews/PR2571/api/dialects/triton" data-v-4f9813fa><!--[--><span class="desc" data-v-4f9813fa>Next page</span><span class="title" data-v-4f9813fa>Triton</span><!--]--></a></div></nav></footer><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><footer class="VPFooter has-sidebar" data-v-a9a9e638 data-v-c970a860><div class="container" data-v-c970a860><p class="message" data-v-c970a860>Made with <a href="https://documenter.juliadocs.org/stable/" target="_blank"><strong>Documenter.jl</strong></a>, <a href="https://vitepress.dev" target="_blank"><strong>VitePress</strong></a> and <a href="https://luxdl.github.io/DocumenterVitepress.jl/stable" target="_blank"><strong>DocumenterVitepress.jl</strong></a><br>Released under the MIT License. Powered by the <a href="https://www.julialang.org">Julia Programming Language</a>.<br></p><p class="copyright" data-v-c970a860>© Copyright 2026 Reactant Development Team.</p></div></footer><!--[--><!--]--></div></div>
    <script>window.__VP_HASH_MAP__=JSON.parse("{\"api_api.md\":\"Bb07o-3w\",\"api_config.md\":\"BIBuYwuu\",\"api_dialects_affine.md\":\"DH_t-ZSG\",\"api_dialects_arith.md\":\"Pv5eiB2l\",\"api_dialects_builtin.md\":\"DiEjZHsZ\",\"api_dialects_chlo.md\":\"8_A6f-qf\",\"api_dialects_complex.md\":\"BM8cDEql\",\"api_dialects_cuda_tile.md\":\"BfTCZH8c\",\"api_dialects_enzyme.md\":\"Uq5nHsc1\",\"api_dialects_enzymexla.md\":\"DvNFHxEZ\",\"api_dialects_func.md\":\"Ck9Q7O5O\",\"api_dialects_gpu.md\":\"3-wm0BBB\",\"api_dialects_llvm.md\":\"Cxku-vPh\",\"api_dialects_memref.md\":\"DMS1D_UI\",\"api_dialects_mosaicgpu.md\":\"CE_Wb-oM\",\"api_dialects_mpi.md\":\"DtLO0eMp\",\"api_dialects_nvvm.md\":\"Bzs41_TB\",\"api_dialects_shape.md\":\"Dspu3E5-\",\"api_dialects_shardy.md\":\"sFqG48sA\",\"api_dialects_sparsetensor.md\":\"BV6SSY1u\",\"api_dialects_stablehlo.md\":\"BxytMyEt\",\"api_dialects_tensor.md\":\"Btz0De3z\",\"api_dialects_tpu.md\":\"C_SMBO1t\",\"api_dialects_triton.md\":\"D6tQOUhs\",\"api_dialects_tritonext.md\":\"CuUbYs1f\",\"api_dialects_vhlo.md\":\"DHxp-fI0\",\"api_internal.md\":\"zF-AHlMu\",\"api_mlirc.md\":\"DXQvpPsc\",\"api_ops.md\":\"DvYbTsTF\",\"api_serialization.md\":\"R1Gcypp3\",\"api_sharding.md\":\"C5AMao4W\",\"api_xla.md\":\"6Vugu5c9\",\"index.md\":\"BxxNUSTg\",\"introduction_configuration.md\":\"hykkH0lW\",\"introduction_faqs.md\":\"Q5mgKgmP\",\"introduction_index.md\":\"BH9JAULn\",\"tutorials_automatic-differentiation.md\":\"CqoIUdFt\",\"tutorials_control-flow.md\":\"CkyfYmtU\",\"tutorials_debugging.md\":\"DwtyoJgG\",\"tutorials_index.md\":\"QjnTZYDP\",\"tutorials_kernels.md\":\"B8Af4Vvj\",\"tutorials_local-build.md\":\"DjuatApA\",\"tutorials_multihost.md\":\"CdVBd_bw\",\"tutorials_partial-evaluation.md\":\"k68zkl42\",\"tutorials_persistent_compile_cache.md\":\"BmYC1jfV\",\"tutorials_profiling.md\":\"CSks-8iV\",\"tutorials_raising.md\":\"CvM_e_SG\",\"tutorials_sharding.md\":\"RkNLcmh6\"}");window.__VP_SITE_DATA__=JSON.parse("{\"lang\":\"en-US\",\"dir\":\"ltr\",\"title\":\"Reactant.jl\",\"description\":\"Documentation for Reactant.jl\",\"base\":\"/Reactant.jl/previews/PR2571/\",\"head\":[],\"router\":{\"prefetchLinks\":true},\"appearance\":true,\"themeConfig\":{\"outline\":\"deep\",\"logo\":{\"light\":\"/logo.svg\",\"dark\":\"/logo.svg\"},\"search\":{\"provider\":\"local\",\"options\":{\"detailedView\":true}},\"nav\":[{\"text\":\"Home\",\"link\":\"/\"},{\"text\":\"Getting Started\",\"items\":[{\"text\":\"Introduction\",\"link\":\"/introduction\"},{\"text\":\"Configuration\",\"link\":\"/introduction/configuration\"},{\"text\":\"FAQs\",\"link\":\"/introduction/FAQs\"}]},{\"text\":\"Benchmarks\",\"link\":\"https://enzymead.github.io/Reactant.jl/benchmarks/\"},{\"text\":\"Tutorials\",\"items\":[{\"text\":\"Overview\",\"link\":\"/tutorials/\"},{\"text\":\"Partial Evaluation\",\"link\":\"/tutorials/partial-evaluation\"},{\"text\":\"Control Flow\",\"link\":\"/tutorials/control-flow\"},{\"text\":\"Automatic Differentiation\",\"link\":\"/tutorials/automatic-differentiation\"},{\"text\":\"Sharding\",\"link\":\"/tutorials/sharding\"},{\"text\":\"Profiling\",\"link\":\"/tutorials/profiling\"},{\"text\":\"Multi-Host Environments\",\"link\":\"/tutorials/multihost\"},{\"text\":\"Local build\",\"link\":\"/tutorials/local-build\"},{\"text\":\"Persistent Compilation Cache\",\"link\":\"/tutorials/persistent_compile_cache\"},{\"text\":\"Raising\",\"link\":\"/tutorials/raising\"},{\"text\":\"Computational kernels\",\"link\":\"/tutorials/kernels\"},{\"text\":\"Debugging compilation errors\",\"link\":\"/tutorials/debugging\"}]},{\"text\":\"API\",\"items\":[{\"text\":\"Core Reactant API\",\"link\":\"/api/api\"},{\"text\":\"Sharding\",\"link\":\"/api/sharding\"},{\"text\":\"Serialization\",\"link\":\"/api/serialization\"},{\"text\":\"Ops\",\"link\":\"/api/ops\"},{\"text\":\"Configuration\",\"link\":\"/api/config\"},{\"text\":\"MLIR Dialects\",\"items\":[{\"text\":\"ArithOps\",\"link\":\"/api/dialects/arith\"},{\"text\":\"Affine\",\"link\":\"/api/dialects/affine\"},{\"text\":\"Builtin\",\"link\":\"/api/dialects/builtin\"},{\"text\":\"Chlo\",\"link\":\"/api/dialects/chlo\"},{\"text\":\"Complex\",\"link\":\"/api/dialects/complex\"},{\"text\":\"CUDA Tile\",\"link\":\"/api/dialects/cuda_tile\"},{\"text\":\"Enzyme\",\"link\":\"/api/dialects/enzyme\"},{\"text\":\"EnzymeXLA\",\"link\":\"/api/dialects/enzymexla\"},{\"text\":\"Func\",\"link\":\"/api/dialects/func\"},{\"text\":\"GPU\",\"link\":\"/api/dialects/gpu\"},{\"text\":\"LLVM\",\"link\":\"/api/dialects/llvm\"},{\"text\":\"MPI\",\"link\":\"/api/dialects/mpi\"},{\"text\":\"MemRef\",\"link\":\"/api/dialects/memref\"},{\"text\":\"Mosaic GPU\",\"link\":\"/api/dialects/mosaicgpu\"},{\"text\":\"NVVM\",\"link\":\"/api/dialects/nvvm\"},{\"text\":\"Shape\",\"link\":\"/api/dialects/shape\"},{\"text\":\"Shardy\",\"link\":\"/api/dialects/shardy\"},{\"text\":\"SparseTensor\",\"link\":\"/api/dialects/sparsetensor\"},{\"text\":\"StableHLO\",\"link\":\"/api/dialects/stablehlo\"},{\"text\":\"Tensor\",\"link\":\"/api/dialects/tensor\"},{\"text\":\"Triton\",\"link\":\"/api/dialects/triton\"},{\"text\":\"TritonExt\",\"link\":\"/api/dialects/tritonext\"},{\"text\":\"TPU\",\"link\":\"/api/dialects/tpu\"},{\"text\":\"VHLO\",\"link\":\"/api/dialects/vhlo\"}]},{\"text\":\"Low-Level API\",\"items\":[{\"text\":\"MLIR API\",\"link\":\"/api/mlirc\"},{\"text\":\"XLA\",\"link\":\"/api/xla\"}]},{\"text\":\"Internal API\",\"link\":\"/api/internal\"}]},{\"component\":\"VersionPicker\"}],\"sidebar\":{\"/introduction/\":[{\"text\":\"Getting Started\",\"collapsed\":false,\"items\":[{\"text\":\"Introduction\",\"link\":\"/introduction\"},{\"text\":\"Configuration\",\"link\":\"/introduction/configuration\"},{\"text\":\"FAQs\",\"link\":\"/introduction/FAQs\"}]}],\"/tutorials/\":[{\"text\":\"Tutorials\",\"collapsed\":false,\"items\":[{\"text\":\"Overview\",\"link\":\"/tutorials/\"},{\"text\":\"Partial Evaluation\",\"link\":\"/tutorials/partial-evaluation\"},{\"text\":\"Control Flow\",\"link\":\"/tutorials/control-flow\"},{\"text\":\"Automatic Differentiation\",\"link\":\"/tutorials/automatic-differentiation\"},{\"text\":\"Sharding\",\"link\":\"/tutorials/sharding\"},{\"text\":\"Profiling\",\"link\":\"/tutorials/profiling\"},{\"text\":\"Multi-Host Environments\",\"link\":\"/tutorials/multihost\"},{\"text\":\"Local build\",\"link\":\"/tutorials/local-build\"},{\"text\":\"Persistent Compilation Cache\",\"link\":\"/tutorials/persistent_compile_cache\"},{\"text\":\"Raising\",\"link\":\"/tutorials/raising\"},{\"text\":\"Computational kernels\",\"link\":\"/tutorials/kernels\"},{\"text\":\"Debugging compilation errors\",\"link\":\"/tutorials/debugging\"}]}],\"/api/\":[{\"text\":\"API Reference\",\"collapsed\":false,\"items\":[{\"text\":\"Reactant API\",\"link\":\"/api/api\"},{\"text\":\"Sharding\",\"link\":\"/api/sharding\"},{\"text\":\"Serialization\",\"link\":\"/api/serialization\"},{\"text\":\"Ops\",\"link\":\"/api/ops\"},{\"text\":\"Configuration\",\"link\":\"/api/config\"},{\"text\":\"MLIR Dialects\",\"collapsed\":false,\"items\":[{\"text\":\"ArithOps\",\"link\":\"/api/dialects/arith\"},{\"text\":\"Affine\",\"link\":\"/api/dialects/affine\"},{\"text\":\"Builtin\",\"link\":\"/api/dialects/builtin\"},{\"text\":\"Chlo\",\"link\":\"/api/dialects/chlo\"},{\"text\":\"Complex\",\"link\":\"/api/dialects/complex\"},{\"text\":\"CUDA Tile\",\"link\":\"/api/dialects/cuda_tile\"},{\"text\":\"Enzyme\",\"link\":\"/api/dialects/enzyme\"},{\"text\":\"EnzymeXLA\",\"link\":\"/api/dialects/enzymexla\"},{\"text\":\"Func\",\"link\":\"/api/dialects/func\"},{\"text\":\"GPU\",\"link\":\"/api/dialects/gpu\"},{\"text\":\"LLVM\",\"link\":\"/api/dialects/llvm\"},{\"text\":\"MPI\",\"link\":\"/api/dialects/mpi\"},{\"text\":\"MemRef\",\"link\":\"/api/dialects/memref\"},{\"text\":\"Mosaic GPU\",\"link\":\"/api/dialects/mosaicgpu\"},{\"text\":\"NVVM\",\"link\":\"/api/dialects/nvvm\"},{\"text\":\"Shape\",\"link\":\"/api/dialects/shape\"},{\"text\":\"Shardy\",\"link\":\"/api/dialects/shardy\"},{\"text\":\"SparseTensor\",\"link\":\"/api/dialects/sparsetensor\"},{\"text\":\"StableHLO\",\"link\":\"/api/dialects/stablehlo\"},{\"text\":\"Tensor\",\"link\":\"/api/dialects/tensor\"},{\"text\":\"Triton\",\"link\":\"/api/dialects/triton\"},{\"text\":\"TritonExt\",\"link\":\"/api/dialects/tritonext\"},{\"text\":\"TPU\",\"link\":\"/api/dialects/tpu\"},{\"text\":\"VHLO\",\"link\":\"/api/dialects/vhlo\"}]},{\"text\":\"Low-Level API\",\"collapsed\":false,\"items\":[{\"text\":\"MLIR API\",\"link\":\"/api/mlirc\"},{\"text\":\"XLA\",\"link\":\"/api/xla\"}]},{\"text\":\"Internal API\",\"link\":\"/api/internal\"}]}]},\"editLink\":{\"pattern\":\"https://github.com/EnzymeAD/Reactant.jl/edit/main/docs/src/:path\",\"text\":\"Edit this page on GitHub\"},\"socialLinks\":[{\"icon\":\"slack\",\"link\":\"https://julialang.org/slack/\"}],\"footer\":{\"message\":\"Made with <a href=\\\"https://documenter.juliadocs.org/stable/\\\" target=\\\"_blank\\\"><strong>Documenter.jl</strong></a>, <a href=\\\"https://vitepress.dev\\\" target=\\\"_blank\\\"><strong>VitePress</strong></a> and <a href=\\\"https://luxdl.github.io/DocumenterVitepress.jl/stable\\\" target=\\\"_blank\\\"><strong>DocumenterVitepress.jl</strong></a><br>Released under the MIT License. Powered by the <a href=\\\"https://www.julialang.org\\\">Julia Programming Language</a>.<br>\",\"copyright\":\"© Copyright 2026 Reactant Development Team.\"},\"lastUpdated\":{\"text\":\"Updated at\",\"formatOptions\":{\"dateStyle\":\"full\",\"timeStyle\":\"medium\"}}},\"locales\":{},\"scrollOffset\":134,\"cleanUrls\":true}");</script>
    
  </body>
</html>