import{_ as d,C as c,c as r,o as i,j as t,a,G as s,w as l,al as o}from"./chunks/framework.CrnLiaLw.js";const ue=JSON.parse('{"title":"NVVM Dialect","description":"","frontmatter":{},"headers":[],"relativePath":"api/dialects/nvvm.md","filePath":"api/dialects/nvvm.md","lastUpdated":null}'),p={name:"api/dialects/nvvm.md"},m={class:"jldocstring custom-block"},u={class:"jldocstring custom-block"},T={class:"jldocstring custom-block"},Q={class:"jldocstring custom-block"},h={class:"jldocstring custom-block"},f={class:"jldocstring custom-block"},b={class:"jldocstring custom-block"},g={class:"jldocstring custom-block"},v={class:"jldocstring custom-block"},y={class:"jldocstring custom-block"},R={class:"jldocstring custom-block"},k={class:"jldocstring custom-block"},_={class:"jldocstring custom-block"},x={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},M={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.464ex"},xmlns:"http://www.w3.org/2000/svg",width:"53.79ex",height:"2.059ex",role:"img",focusable:"false",viewBox:"0 -705 23775.1 910","aria-hidden":"true"},I={class:"jldocstring custom-block"},L={class:"jldocstring custom-block"},j={class:"jldocstring custom-block"},D={class:"jldocstring custom-block"},w={class:"jldocstring custom-block"},A={class:"jldocstring custom-block"},V={class:"jldocstring custom-block"},S={class:"jldocstring custom-block"},N={class:"jldocstring custom-block"},C={class:"jldocstring custom-block"},H={class:"jldocstring custom-block"},P={class:"jldocstring custom-block"},F={class:"jldocstring custom-block"},z={class:"jldocstring custom-block"},E={class:"jldocstring custom-block"},O={class:"jldocstring custom-block"},Z={class:"jldocstring custom-block"},X={class:"jldocstring custom-block"},q={class:"jldocstring custom-block"},B={class:"jldocstring custom-block"},$={class:"jldocstring custom-block"},W={class:"jldocstring custom-block"},G={class:"jldocstring custom-block"},J={class:"jldocstring custom-block"},K={class:"jldocstring custom-block"},U={class:"jldocstring custom-block"},Y={class:"jldocstring custom-block"},ee={class:"jldocstring custom-block"},te={class:"jldocstring custom-block"},ae={class:"jldocstring custom-block"},ne={class:"jldocstring custom-block"},se={class:"jldocstring custom-block"},le={class:"jldocstring custom-block"};function oe(re,e,ie,de,ce,pe){const n=c("Badge");return i(),r("div",null,[e[246]||(e[246]=t("h1",{id:"NVVM-Dialect",tabindex:"-1"},[a("NVVM Dialect "),t("a",{class:"header-anchor",href:"#NVVM-Dialect","aria-label":'Permalink to "NVVM Dialect {#NVVM-Dialect}"'},"â€‹")],-1)),e[247]||(e[247]=t("p",null,[a("Refer to the "),t("a",{href:"https://mlir.llvm.org/docs/Dialects/NVVMDialect/",target:"_blank",rel:"noreferrer"},"official documentation"),a(" for more details.")],-1)),t("details",m,[t("summary",null,[e[0]||(e[0]=t("a",{id:"Reactant.MLIR.Dialects.nvvm.barrier_arrive",href:"#Reactant.MLIR.Dialects.nvvm.barrier_arrive"},[t("span",{class:"jlbinding"},"Reactant.MLIR.Dialects.nvvm.barrier_arrive")],-1)),e[1]||(e[1]=a()),s(n,{type:"info",class:"jlObjectType jlFunction",text:"Function"})]),e[3]||(e[3]=t("p",null,[t("code",null,"barrier_arrive")],-1)),e[4]||(e[4]=t("p",null,"Thread that executes this op announces their arrival at the barrier with given id and continue their execution.",-1)),e[5]||(e[5]=t("p",null,[a("The default barrier id is 0 that is similar to "),t("code",null,"nvvm.barrier"),a(" Op. When "),t("code",null,"barrierId"),a(" is not present, the default barrier id is used.")],-1)),e[6]||(e[6]=t("p",null,[t("a",{href:"https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#parallel-synchronization-and-communication-instructions-bar",target:"_blank",rel:"noreferrer"},"For more information, see PTX ISA")],-1)),s(n,{type:"info",class:"source-link",text:"source"},{default:l(()=>e[2]||(e[2]=[t("a",{href:"https://github.com/EnzymeAD/Reactant.jl/blob/9dbf8a81dd5743ab847157c74394e53975a2e470/src/mlir/Dialects/Nvvm.jl#L35-L45",target:"_blank",rel:"noreferrer"},"source",-1)])),_:1,__:[2]})]),t("details",u,[t("summary",null,[e[7]||(e[7]=t("a",{id:"Reactant.MLIR.Dialects.nvvm.breakpoint-Tuple{}",href:"#Reactant.MLIR.Dialects.nvvm.breakpoint-Tuple{}"},[t("span",{class:"jlbinding"},"Reactant.MLIR.Dialects.nvvm.breakpoint")],-1)),e[8]||(e[8]=a()),s(n,{type:"info",class:"jlObjectType jlMethod",text:"Method"})]),e[10]||(e[10]=t("p",null,[t("code",null,"breakpoint")],-1)),e[11]||(e[11]=t("p",null,[a("Breakpoint suspends execution of the program for debugging. "),t("a",{href:"https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#miscellaneous-instructions-brkpt",target:"_blank",rel:"noreferrer"},"For more information, see PTX ISA")],-1)),s(n,{type:"info",class:"source-link",text:"source"},{default:l(()=>e[9]||(e[9]=[t("a",{href:"https://github.com/EnzymeAD/Reactant.jl/blob/9dbf8a81dd5743ab847157c74394e53975a2e470/src/mlir/Dialects/Nvvm.jl#L282-L287",target:"_blank",rel:"noreferrer"},"source",-1)])),_:1,__:[9]})]),t("details",T,[t("summary",null,[e[12]||(e[12]=t("a",{id:"Reactant.MLIR.Dialects.nvvm.cluster_arrive-Tuple{}",href:"#Reactant.MLIR.Dialects.nvvm.cluster_arrive-Tuple{}"},[t("span",{class:"jlbinding"},"Reactant.MLIR.Dialects.nvvm.cluster_arrive")],-1)),e[13]||(e[13]=a()),s(n,{type:"info",class:"jlObjectType jlMethod",text:"Method"})]),e[15]||(e[15]=t("p",null,[t("code",null,"cluster_arrive")],-1)),e[16]||(e[16]=t("p",null,[a("The "),t("code",null,"cluster.arrive"),a(" can be used by the threads within the cluster for synchronization and communication. The "),t("code",null,"cluster.arrive"),a(" instruction marks the warps' arrival at the barrier without causing the executing thread to wait for other participating threads.")],-1)),e[17]||(e[17]=t("p",null,[a("The "),t("code",null,"aligned"),a(" attribute, when provided, generates the .aligned version of the PTX instruction.")],-1)),e[18]||(e[18]=t("p",null,[t("a",{href:"https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#parallel-synchronization-and-communication-instructions-barrier-cluster",target:"_blank",rel:"noreferrer"},"For more information, see PTX ISA")],-1)),s(n,{type:"info",class:"source-link",text:"source"},{default:l(()=>e[14]||(e[14]=[t("a",{href:"https://github.com/EnzymeAD/Reactant.jl/blob/9dbf8a81dd5743ab847157c74394e53975a2e470/src/mlir/Dialects/Nvvm.jl#L376-L386",target:"_blank",rel:"noreferrer"},"source",-1)])),_:1,__:[14]})]),t("details",Q,[t("summary",null,[e[19]||(e[19]=t("a",{id:"Reactant.MLIR.Dialects.nvvm.cluster_arrive_relaxed-Tuple{}",href:"#Reactant.MLIR.Dialects.nvvm.cluster_arrive_relaxed-Tuple{}"},[t("span",{class:"jlbinding"},"Reactant.MLIR.Dialects.nvvm.cluster_arrive_relaxed")],-1)),e[20]||(e[20]=a()),s(n,{type:"info",class:"jlObjectType jlMethod",text:"Method"})]),e[22]||(e[22]=o('<p><code>cluster_arrive_relaxed</code></p><p>The <code>cluster.arrive</code> can be used by the threads within the cluster for synchronization and communication. The <code>cluster.arrive</code> instruction marks the warps&#39; arrival at the barrier without causing the executing thread to wait for other participating threads.</p><p>The <code>aligned</code> attribute, when provided, generates the .aligned version of the PTX instruction. The .relaxed qualifier on <code>cluster.arrive</code> specifies that there are no memory ordering and visibility guarantees provided for the memory accesses performed prior to <code>cluster.arrive</code>.</p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#parallel-synchronization-and-communication-instructions-barrier-cluster" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p>',4)),s(n,{type:"info",class:"source-link",text:"source"},{default:l(()=>e[21]||(e[21]=[t("a",{href:"https://github.com/EnzymeAD/Reactant.jl/blob/9dbf8a81dd5743ab847157c74394e53975a2e470/src/mlir/Dialects/Nvvm.jl#L407-L420",target:"_blank",rel:"noreferrer"},"source",-1)])),_:1,__:[21]})]),t("details",h,[t("summary",null,[e[23]||(e[23]=t("a",{id:"Reactant.MLIR.Dialects.nvvm.cluster_wait-Tuple{}",href:"#Reactant.MLIR.Dialects.nvvm.cluster_wait-Tuple{}"},[t("span",{class:"jlbinding"},"Reactant.MLIR.Dialects.nvvm.cluster_wait")],-1)),e[24]||(e[24]=a()),s(n,{type:"info",class:"jlObjectType jlMethod",text:"Method"})]),e[26]||(e[26]=t("p",null,[t("code",null,"cluster_wait")],-1)),e[27]||(e[27]=t("p",null,[a("The "),t("code",null,"cluster.wait"),a(" causes the executing thread to wait for all non-exited threads of the cluster to perform "),t("code",null,"cluster.arrive"),a(". The "),t("code",null,"aligned"),a(" attribute, when provided, generates the .aligned version of the PTX instruction.")],-1)),e[28]||(e[28]=t("p",null,[t("a",{href:"https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#parallel-synchronization-and-communication-instructions-barrier-cluster",target:"_blank",rel:"noreferrer"},"For more information, see PTX ISA")],-1)),s(n,{type:"info",class:"source-link",text:"source"},{default:l(()=>e[25]||(e[25]=[t("a",{href:"https://github.com/EnzymeAD/Reactant.jl/blob/9dbf8a81dd5743ab847157c74394e53975a2e470/src/mlir/Dialects/Nvvm.jl#L661-L669",target:"_blank",rel:"noreferrer"},"source",-1)])),_:1,__:[25]})]),t("details",f,[t("summary",null,[e[29]||(e[29]=t("a",{id:"Reactant.MLIR.Dialects.nvvm.cp_async_bulk_commit_group-Tuple{}",href:"#Reactant.MLIR.Dialects.nvvm.cp_async_bulk_commit_group-Tuple{}"},[t("span",{class:"jlbinding"},"Reactant.MLIR.Dialects.nvvm.cp_async_bulk_commit_group")],-1)),e[30]||(e[30]=a()),s(n,{type:"info",class:"jlObjectType jlMethod",text:"Method"})]),e[32]||(e[32]=t("p",null,[t("code",null,"cp_async_bulk_commit_group")],-1)),e[33]||(e[33]=t("p",null,"This Op commits all prior initiated but uncommitted cp.async.bulk instructions into a cp.async.bulk-group.",-1)),e[34]||(e[34]=t("p",null,[t("a",{href:"https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cp-async-bulk-commit-group",target:"_blank",rel:"noreferrer"},"For more information, see PTX ISA")],-1)),s(n,{type:"info",class:"source-link",text:"source"},{default:l(()=>e[31]||(e[31]=[t("a",{href:"https://github.com/EnzymeAD/Reactant.jl/blob/9dbf8a81dd5743ab847157c74394e53975a2e470/src/mlir/Dialects/Nvvm.jl#L690-L697",target:"_blank",rel:"noreferrer"},"source",-1)])),_:1,__:[31]})]),t("details",b,[t("summary",null,[e[35]||(e[35]=t("a",{id:"Reactant.MLIR.Dialects.nvvm.cp_async_bulk_global_shared_cta",href:"#Reactant.MLIR.Dialects.nvvm.cp_async_bulk_global_shared_cta"},[t("span",{class:"jlbinding"},"Reactant.MLIR.Dialects.nvvm.cp_async_bulk_global_shared_cta")],-1)),e[36]||(e[36]=a()),s(n,{type:"info",class:"jlObjectType jlFunction",text:"Function"})]),e[38]||(e[38]=t("p",null,[t("code",null,"cp_async_bulk_global_shared_cta")],-1)),e[39]||(e[39]=t("p",null,"Initiates an asynchronous copy operation from Shared CTA memory to global memory.",-1)),e[40]||(e[40]=t("p",null,[a("The "),t("code",null,"l2CacheHint"),a(" operand is optional, and it is used to specify cache eviction policy that may be used during the memory access.")],-1)),e[41]||(e[41]=t("p",null,[t("a",{href:"https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cp-async-bulk",target:"_blank",rel:"noreferrer"},"For more information, see PTX ISA")],-1)),s(n,{type:"info",class:"source-link",text:"source"},{default:l(()=>e[37]||(e[37]=[t("a",{href:"https://github.com/EnzymeAD/Reactant.jl/blob/9dbf8a81dd5743ab847157c74394e53975a2e470/src/mlir/Dialects/Nvvm.jl#L776-L786",target:"_blank",rel:"noreferrer"},"source",-1)])),_:1,__:[37]})]),t("details",g,[t("summary",null,[e[42]||(e[42]=t("a",{id:"Reactant.MLIR.Dialects.nvvm.cp_async_bulk_shared_cluster_global",href:"#Reactant.MLIR.Dialects.nvvm.cp_async_bulk_shared_cluster_global"},[t("span",{class:"jlbinding"},"Reactant.MLIR.Dialects.nvvm.cp_async_bulk_shared_cluster_global")],-1)),e[43]||(e[43]=a()),s(n,{type:"info",class:"jlObjectType jlFunction",text:"Function"})]),e[45]||(e[45]=o('<p><code>cp_async_bulk_shared_cluster_global</code></p><p>Initiates an asynchronous copy operation from global memory to cluster&#39;s shared memory.</p><p>The <code>multicastMask</code> operand is optional. When it is present, the Op copies data from global memory to shared memory of multiple CTAs in the cluster. Operand <code>multicastMask</code> specifies the destination CTAs in the cluster such that each bit position in the 16-bit <code>multicastMask</code> operand corresponds to the <code>nvvm.read.ptx.sreg.ctaid</code> of the destination CTA.</p><p>The <code>l2CacheHint</code> operand is optional, and it is used to specify cache eviction policy that may be used during the memory access.</p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cp-async-bulk" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p>',5)),s(n,{type:"info",class:"source-link",text:"source"},{default:l(()=>e[44]||(e[44]=[t("a",{href:"https://github.com/EnzymeAD/Reactant.jl/blob/9dbf8a81dd5743ab847157c74394e53975a2e470/src/mlir/Dialects/Nvvm.jl#L717-L733",target:"_blank",rel:"noreferrer"},"source",-1)])),_:1,__:[44]})]),t("details",v,[t("summary",null,[e[46]||(e[46]=t("a",{id:"Reactant.MLIR.Dialects.nvvm.cp_async_bulk_shared_cluster_shared_cta-NTuple{4, Reactant.MLIR.IR.Value}",href:"#Reactant.MLIR.Dialects.nvvm.cp_async_bulk_shared_cluster_shared_cta-NTuple{4, Reactant.MLIR.IR.Value}"},[t("span",{class:"jlbinding"},"Reactant.MLIR.Dialects.nvvm.cp_async_bulk_shared_cluster_shared_cta")],-1)),e[47]||(e[47]=a()),s(n,{type:"info",class:"jlObjectType jlMethod",text:"Method"})]),e[49]||(e[49]=t("p",null,[t("code",null,"cp_async_bulk_shared_cluster_shared_cta")],-1)),e[50]||(e[50]=t("p",null,"Initiates an asynchronous copy operation from Shared CTA memory to Shared cluster memory.",-1)),e[51]||(e[51]=t("p",null,[t("a",{href:"https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cp-async-bulk",target:"_blank",rel:"noreferrer"},"For more information, see PTX ISA")],-1)),s(n,{type:"info",class:"source-link",text:"source"},{default:l(()=>e[48]||(e[48]=[t("a",{href:"https://github.com/EnzymeAD/Reactant.jl/blob/9dbf8a81dd5743ab847157c74394e53975a2e470/src/mlir/Dialects/Nvvm.jl#L813-L820",target:"_blank",rel:"noreferrer"},"source",-1)])),_:1,__:[48]})]),t("details",y,[t("summary",null,[e[52]||(e[52]=t("a",{id:"Reactant.MLIR.Dialects.nvvm.cp_async_bulk_tensor_prefetch",href:"#Reactant.MLIR.Dialects.nvvm.cp_async_bulk_tensor_prefetch"},[t("span",{class:"jlbinding"},"Reactant.MLIR.Dialects.nvvm.cp_async_bulk_tensor_prefetch")],-1)),e[53]||(e[53]=a()),s(n,{type:"info",class:"jlObjectType jlFunction",text:"Function"})]),e[55]||(e[55]=o('<p><code>cp_async_bulk_tensor_prefetch</code></p><p>Initiates an asynchronous prefetch operation on the tensor data from global memory to L2 cache.</p><p>The Op has two modes:</p><ol><li>Tiled Mode: It&#39;s the default mode. The source multi-dimensional tensor</li></ol><p>layout is preserved at the destination.</p><ol><li>Im2col Mode: This mode is used when <code>im2colOffsets</code> operands are present.</li></ol><p>the elements in the Bounding Box of the source tensor are rearranged into columns at the destination. In this mode, the tensor has to be at least 3-dimensional.</p><p>The <code>l2CacheHint</code> operand is optional, and it is used to specify cache eviction policy that may be used during the memory access.</p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cp-async-bulk-prefetch-tensor" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p>',9)),s(n,{type:"info",class:"source-link",text:"source"},{default:l(()=>e[54]||(e[54]=[t("a",{href:"https://github.com/EnzymeAD/Reactant.jl/blob/9dbf8a81dd5743ab847157c74394e53975a2e470/src/mlir/Dialects/Nvvm.jl#L919-L938",target:"_blank",rel:"noreferrer"},"source",-1)])),_:1,__:[54]})]),t("details",R,[t("summary",null,[e[56]||(e[56]=t("a",{id:"Reactant.MLIR.Dialects.nvvm.cp_async_bulk_tensor_reduce",href:"#Reactant.MLIR.Dialects.nvvm.cp_async_bulk_tensor_reduce"},[t("span",{class:"jlbinding"},"Reactant.MLIR.Dialects.nvvm.cp_async_bulk_tensor_reduce")],-1)),e[57]||(e[57]=a()),s(n,{type:"info",class:"jlObjectType jlFunction",text:"Function"})]),e[59]||(e[59]=t("p",null,[t("code",null,"cp_async_bulk_tensor_reduce")],-1)),e[60]||(e[60]=t("p",null,"Initiates an asynchronous reduction operation of tensor data in global memory with tensor data in shared memory.",-1)),e[61]||(e[61]=t("p",{"add,":"","min,":"","max,":"","inc,":"","dec,":"","and,":"","or,":"",xor:""},[a("The "),t("code",null,"mode"),a(" attribute indicates whether the copy mode is tile or im2col. The "),t("code",null,"redOp"),a(" attribute specifies the reduction operations applied. The supported reduction operations are:")],-1)),e[62]||(e[62]=t("p",null,[a("The "),t("code",null,"l2CacheHint"),a(" operand is optional, and it is used to specify cache eviction policy that may be used during the memory access.")],-1)),e[63]||(e[63]=t("p",null,[t("a",{href:"https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cp-reduce-async-bulk-tensor",target:"_blank",rel:"noreferrer"},"For more information, see PTX ISA")],-1)),s(n,{type:"info",class:"source-link",text:"source"},{default:l(()=>e[58]||(e[58]=[t("a",{href:"https://github.com/EnzymeAD/Reactant.jl/blob/9dbf8a81dd5743ab847157c74394e53975a2e470/src/mlir/Dialects/Nvvm.jl#L971-L986",target:"_blank",rel:"noreferrer"},"source",-1)])),_:1,__:[58]})]),t("details",k,[t("summary",null,[e[64]||(e[64]=t("a",{id:"Reactant.MLIR.Dialects.nvvm.cp_async_bulk_tensor_shared_cluster_global",href:"#Reactant.MLIR.Dialects.nvvm.cp_async_bulk_tensor_shared_cluster_global"},[t("span",{class:"jlbinding"},"Reactant.MLIR.Dialects.nvvm.cp_async_bulk_tensor_shared_cluster_global")],-1)),e[65]||(e[65]=a()),s(n,{type:"info",class:"jlObjectType jlFunction",text:"Function"})]),e[67]||(e[67]=o('<p><code>cp_async_bulk_tensor_shared_cluster_global</code></p><p>Initiates an asynchronous copy operation on the tensor data from global memory to shared memory.</p><p>The Op operates has two load modes:</p><ol><li>Tiled Mode: It&#39;s the default mode. The source multi-dimensional tensor</li></ol><p>layout is preserved at the destination.</p><ol><li>Im2col Mode: This mode is used when <code>im2colOffsets</code> operands are present.</li></ol><p>the elements in the Bounding Box of the source tensor are rearranged into columns at the destination. In this mode, the tensor has to be at least 3-dimensional.</p><p>The <code>multicastMask</code> operand is optional. When it is present, the Op copies data from global memory to shared memory of multiple CTAs in the cluster. Operand <code>multicastMask</code> specifies the destination CTAs in the cluster such that each bit position in the 16-bit <code>multicastMask</code> operand corresponds to the <code>nvvm.read.ptx.sreg.ctaid</code> of the destination CTA.</p><p>The <code>l2CacheHint</code> operand is optional, and it is used to specify cache eviction policy that may be used during the memory access.</p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cp-async-bulk-tensor" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p>',10)),s(n,{type:"info",class:"source-link",text:"source"},{default:l(()=>e[66]||(e[66]=[t("a",{href:"https://github.com/EnzymeAD/Reactant.jl/blob/9dbf8a81dd5743ab847157c74394e53975a2e470/src/mlir/Dialects/Nvvm.jl#L842-L867",target:"_blank",rel:"noreferrer"},"source",-1)])),_:1,__:[66]})]),t("details",_,[t("summary",null,[e[68]||(e[68]=t("a",{id:"Reactant.MLIR.Dialects.nvvm.cp_async_bulk_wait_group-Tuple{}",href:"#Reactant.MLIR.Dialects.nvvm.cp_async_bulk_wait_group-Tuple{}"},[t("span",{class:"jlbinding"},"Reactant.MLIR.Dialects.nvvm.cp_async_bulk_wait_group")],-1)),e[69]||(e[69]=a()),s(n,{type:"info",class:"jlObjectType jlMethod",text:"Method"})]),e[77]||(e[77]=t("p",null,[t("code",null,"cp_async_bulk_wait_group")],-1)),e[78]||(e[78]=t("p",null,"Op waits for completion of the most recent bulk async-groups.",-1)),t("p",null,[e[72]||(e[72]=a("The ")),e[73]||(e[73]=t("code",null,"$group",-1)),e[74]||(e[74]=a(" operand tells waiting has to be done until for ")),t("mjx-container",x,[(i(),r("svg",M,e[70]||(e[70]=[o('<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(477,0)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(928,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(1413,0)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(1985,0)"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(2488,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(2973,0)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(3424,0)"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(3974,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(4440,0)"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(5156,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(5622,0)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(6073,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(6558,0)"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(7108,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(7469,0)"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(8045,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(8511,0)"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(9389,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(9874,0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(10343,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(10704,0)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(11155,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(11621,0)"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(12054,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(12520,0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(13120,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(13481,0)"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(13910,0)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(14482,0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(14780,0)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(15301,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(15830,0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(16299,0)"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(16789,0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(17389,0)"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(18044.2,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(19044.4,0)"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(19521.4,0)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(19972.4,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(20457.4,0)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(21029.4,0)"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(21532.4,0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(22001.4,0)"><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(22446.1,0)"><path data-c="1D43C" d="M43 1Q26 1 26 10Q26 12 29 24Q34 43 39 45Q42 46 54 46H60Q120 46 136 53Q137 53 138 54Q143 56 149 77T198 273Q210 318 216 344Q286 624 286 626Q284 630 284 631Q274 637 213 637H193Q184 643 189 662Q193 677 195 680T209 683H213Q285 681 359 681Q481 681 487 683H497Q504 676 504 672T501 655T494 639Q491 637 471 637Q440 637 407 634Q393 631 388 623Q381 609 337 432Q326 385 315 341Q245 65 245 59Q245 52 255 50T307 46H339Q345 38 345 37T342 19Q338 6 332 0H316Q279 2 179 2Q143 2 113 2T65 2T43 1Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(22950.1,0)"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(23500.1,0)"><path data-c="2035" d="M12 501Q12 527 31 542T63 558Q73 560 77 560Q114 560 128 528Q133 518 188 293T244 61Q244 56 223 50T195 43Q192 43 190 45T102 263T14 486Q12 496 12 501Z" style="stroke-width:3;"></path></g></g></g>',1)]))),e[71]||(e[71]=t("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[t("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[t("mi",null,"g"),t("mi",null,"r"),t("mi",null,"o"),t("mi",null,"u"),t("mi",null,"p"),t("mi",null,"o"),t("mi",null,"r"),t("mi",null,"f"),t("mi",null,"e"),t("mi",null,"w"),t("mi",null,"e"),t("mi",null,"r"),t("mi",null,"o"),t("mi",null,"f"),t("mi",null,"t"),t("mi",null,"h"),t("mi",null,"e"),t("mi",null,"m"),t("mi",null,"o"),t("mi",null,"s"),t("mi",null,"t"),t("mi",null,"r"),t("mi",null,"e"),t("mi",null,"c"),t("mi",null,"e"),t("mi",null,"n"),t("mi",null,"t"),t("mi",null,"b"),t("mi",null,"u"),t("mi",null,"l"),t("mi",null,"k"),t("mi",null,"a"),t("mi",null,"s"),t("mi",null,"y"),t("mi",null,"n"),t("mi",null,"c"),t("mo",null,"âˆ’"),t("mi",null,"g"),t("mi",null,"r"),t("mi",null,"o"),t("mi",null,"u"),t("mi",null,"p"),t("mi",null,"s"),t("mo",null,"."),t("mi",null,"I"),t("mi",null,"f"),t("mo",{"data-mjx-pseudoscript":"true"},"â€˜")])],-1))]),e[75]||(e[75]=a("group` is 0, the op wait until all the most recent bulk async-groups have completed."))]),e[79]||(e[79]=t("p",null,[a("The "),t("code",null,"$read"),a(" indicates that the waiting has to be done until all the bulk async operations in the specified bulk async-group have completed reading from their source locations.")],-1)),e[80]||(e[80]=t("p",null,[t("a",{href:"https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cp-async-bulk-wait-group",target:"_blank",rel:"noreferrer"},"For more information, see PTX ISA")],-1)),s(n,{type:"info",class:"source-link",text:"source"},{default:l(()=>e[76]||(e[76]=[t("a",{href:"https://github.com/EnzymeAD/Reactant.jl/blob/9dbf8a81dd5743ab847157c74394e53975a2e470/src/mlir/Dialects/Nvvm.jl#L1050-L1064",target:"_blank",rel:"noreferrer"},"source",-1)])),_:1,__:[76]})]),t("details",I,[t("summary",null,[e[81]||(e[81]=t("a",{id:"Reactant.MLIR.Dialects.nvvm.cp_async_mbarrier_arrive-Tuple{Reactant.MLIR.IR.Value}",href:"#Reactant.MLIR.Dialects.nvvm.cp_async_mbarrier_arrive-Tuple{Reactant.MLIR.IR.Value}"},[t("span",{class:"jlbinding"},"Reactant.MLIR.Dialects.nvvm.cp_async_mbarrier_arrive")],-1)),e[82]||(e[82]=a()),s(n,{type:"info",class:"jlObjectType jlMethod",text:"Method"})]),e[84]||(e[84]=t("p",null,[t("code",null,"cp_async_mbarrier_arrive")],-1)),e[85]||(e[85]=t("p",null,[a("The "),t("code",null,"cp.async.mbarrier.arrive"),a(" Op makes the mbarrier object track all prior cp.async operations initiated by the executing thread. The "),t("code",null,"addr"),a(" operand specifies the address of the mbarrier object in generic address space. The "),t("code",null,"noinc"),a(" attr impacts how the mbarrier's state is updated.")],-1)),e[86]||(e[86]=t("p",null,[t("a",{href:"https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#parallel-synchronization-and-communication-instructions-cp-async-mbarrier-arrive",target:"_blank",rel:"noreferrer"},"For more information, see PTX ISA")],-1)),s(n,{type:"info",class:"source-link",text:"source"},{default:l(()=>e[83]||(e[83]=[t("a",{href:"https://github.com/EnzymeAD/Reactant.jl/blob/9dbf8a81dd5743ab847157c74394e53975a2e470/src/mlir/Dialects/Nvvm.jl#L1104-L1114",target:"_blank",rel:"noreferrer"},"source",-1)])),_:1,__:[83]})]),t("details",L,[t("summary",null,[e[87]||(e[87]=t("a",{id:"Reactant.MLIR.Dialects.nvvm.cp_async_mbarrier_arrive_shared-Tuple{Reactant.MLIR.IR.Value}",href:"#Reactant.MLIR.Dialects.nvvm.cp_async_mbarrier_arrive_shared-Tuple{Reactant.MLIR.IR.Value}"},[t("span",{class:"jlbinding"},"Reactant.MLIR.Dialects.nvvm.cp_async_mbarrier_arrive_shared")],-1)),e[88]||(e[88]=a()),s(n,{type:"info",class:"jlObjectType jlMethod",text:"Method"})]),e[90]||(e[90]=t("p",null,[t("code",null,"cp_async_mbarrier_arrive_shared")],-1)),e[91]||(e[91]=t("p",null,[a("The "),t("code",null,"cp.async.mbarrier.arrive.shared"),a(" Op makes the mbarrier object track all prior cp.async operations initiated by the executing thread. The "),t("code",null,"addr"),a(" operand specifies the address of the mbarrier object in shared memory. The "),t("code",null,"noinc"),a(" attr impacts how the mbarrier's state is updated.")],-1)),e[92]||(e[92]=t("p",null,[t("a",{href:"https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#parallel-synchronization-and-communication-instructions-cp-async-mbarrier-arrive",target:"_blank",rel:"noreferrer"},"For more information, see PTX ISA")],-1)),s(n,{type:"info",class:"source-link",text:"source"},{default:l(()=>e[89]||(e[89]=[t("a",{href:"https://github.com/EnzymeAD/Reactant.jl/blob/9dbf8a81dd5743ab847157c74394e53975a2e470/src/mlir/Dialects/Nvvm.jl#L1135-L1145",target:"_blank",rel:"noreferrer"},"source",-1)])),_:1,__:[89]})]),t("details",j,[t("summary",null,[e[93]||(e[93]=t("a",{id:"Reactant.MLIR.Dialects.nvvm.cvt_float_to_tf32-Tuple{Reactant.MLIR.IR.Value}",href:"#Reactant.MLIR.Dialects.nvvm.cvt_float_to_tf32-Tuple{Reactant.MLIR.IR.Value}"},[t("span",{class:"jlbinding"},"Reactant.MLIR.Dialects.nvvm.cvt_float_to_tf32")],-1)),e[94]||(e[94]=a()),s(n,{type:"info",class:"jlObjectType jlMethod",text:"Method"})]),e[96]||(e[96]=o('<p><code>cvt_float_to_tf32</code></p><p>This Op converts the given f32 input to tf32. The result <code>res</code> is represented as an i32 type. The <code>relu</code> attribute, when set, lowers to the &#39;.relu&#39; variant of the cvt instruction. The <code>rnd</code> and <code>sat</code> attributes specify the the rounding and saturation modes respectively.</p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cvt" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p>',3)),s(n,{type:"info",class:"source-link",text:"source"},{default:l(()=>e[95]||(e[95]=[t("a",{href:"https://github.com/EnzymeAD/Reactant.jl/blob/9dbf8a81dd5743ab847157c74394e53975a2e470/src/mlir/Dialects/Nvvm.jl#L1214-L1224",target:"_blank",rel:"noreferrer"},"source",-1)])),_:1,__:[95]})]),t("details",D,[t("summary",null,[e[97]||(e[97]=t("a",{id:"Reactant.MLIR.Dialects.nvvm.cvt_to_f6x2-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}",href:"#Reactant.MLIR.Dialects.nvvm.cvt_to_f6x2-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}"},[t("span",{class:"jlbinding"},"Reactant.MLIR.Dialects.nvvm.cvt_to_f6x2")],-1)),e[98]||(e[98]=a()),s(n,{type:"info",class:"jlObjectType jlMethod",text:"Method"})]),e[100]||(e[100]=o('<p><code>cvt_to_f6x2</code></p><p>This Op converts each of the given float inputs to the specified fp6 type. The result <code>dst</code> is represented either as an i16 type or as a vector of two i8 types. If <code>dst</code> is returned as an i16 type, the converted values are packed such that the value converted from <code>a</code> is stored in the upper 8 bits of <code>dst</code> with 2 MSB bits padded with zeros and the value converted from <code>b</code> is stored in the lower 8 bits of <code>dst</code> with 2 MSB bits padded with zeros. If <code>dst</code> is returned as a vector type, each converted value is stored as an i8 element in the vector. The <code>relu</code> attribute, when set, lowers to the &#39;.relu&#39; variant of the cvt instruction.</p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cvt" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p>',3)),s(n,{type:"info",class:"source-link",text:"source"},{default:l(()=>e[99]||(e[99]=[t("a",{href:"https://github.com/EnzymeAD/Reactant.jl/blob/9dbf8a81dd5743ab847157c74394e53975a2e470/src/mlir/Dialects/Nvvm.jl#L1249-L1265",target:"_blank",rel:"noreferrer"},"source",-1)])),_:1,__:[99]})]),t("details",w,[t("summary",null,[e[101]||(e[101]=t("a",{id:"Reactant.MLIR.Dialects.nvvm.elect_sync-Tuple{}",href:"#Reactant.MLIR.Dialects.nvvm.elect_sync-Tuple{}"},[t("span",{class:"jlbinding"},"Reactant.MLIR.Dialects.nvvm.elect_sync")],-1)),e[102]||(e[102]=a()),s(n,{type:"info",class:"jlObjectType jlMethod",text:"Method"})]),e[104]||(e[104]=o('<p><code>elect_sync</code></p><p>The <code>elect.sync</code> instruction elects one predicated active leader thread from among a set of threads specified in membermask. The membermask is set to <code>0xFFFFFFFF</code> for the current version of this Op. The predicate result is set to <code>True</code> for the leader thread, and <code>False</code> for all other threads.</p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#parallel-synchronization-and-communication-instructions-elect-sync" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p>',3)),s(n,{type:"info",class:"source-link",text:"source"},{default:l(()=>e[103]||(e[103]=[t("a",{href:"https://github.com/EnzymeAD/Reactant.jl/blob/9dbf8a81dd5743ab847157c74394e53975a2e470/src/mlir/Dialects/Nvvm.jl#L1288-L1298",target:"_blank",rel:"noreferrer"},"source",-1)])),_:1,__:[103]})]),t("details",A,[t("summary",null,[e[105]||(e[105]=t("a",{id:"Reactant.MLIR.Dialects.nvvm.exit-Tuple{}",href:"#Reactant.MLIR.Dialects.nvvm.exit-Tuple{}"},[t("span",{class:"jlbinding"},"Reactant.MLIR.Dialects.nvvm.exit")],-1)),e[106]||(e[106]=a()),s(n,{type:"info",class:"jlObjectType jlMethod",text:"Method"})]),e[108]||(e[108]=t("p",null,[t("code",null,"exit")],-1)),e[109]||(e[109]=t("p",null,[a("Ends execution of a thread. "),t("a",{href:"https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#control-flow-instructions-exit",target:"_blank",rel:"noreferrer"},"For more information, see PTX ISA")],-1)),s(n,{type:"info",class:"source-link",text:"source"},{default:l(()=>e[107]||(e[107]=[t("a",{href:"https://github.com/EnzymeAD/Reactant.jl/blob/9dbf8a81dd5743ab847157c74394e53975a2e470/src/mlir/Dialects/Nvvm.jl#L1926-L1931",target:"_blank",rel:"noreferrer"},"source",-1)])),_:1,__:[107]})]),t("details",V,[t("summary",null,[e[110]||(e[110]=t("a",{id:"Reactant.MLIR.Dialects.nvvm.fence_mbarrier_init-Tuple{}",href:"#Reactant.MLIR.Dialects.nvvm.fence_mbarrier_init-Tuple{}"},[t("span",{class:"jlbinding"},"Reactant.MLIR.Dialects.nvvm.fence_mbarrier_init")],-1)),e[111]||(e[111]=a()),s(n,{type:"info",class:"jlObjectType jlMethod",text:"Method"})]),e[113]||(e[113]=t("p",null,[t("code",null,"fence_mbarrier_init")],-1)),e[114]||(e[114]=t("p",null,"Fence operation that applies on the prior nvvm.mbarrier.init",-1)),e[115]||(e[115]=t("p",null,[t("a",{href:"https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#parallel-synchronization-and-communication-instructions-membar",target:"_blank",rel:"noreferrer"},"For more information, see PTX ISA")],-1)),s(n,{type:"info",class:"source-link",text:"source"},{default:l(()=>e[112]||(e[112]=[t("a",{href:"https://github.com/EnzymeAD/Reactant.jl/blob/9dbf8a81dd5743ab847157c74394e53975a2e470/src/mlir/Dialects/Nvvm.jl#L1951-L1957",target:"_blank",rel:"noreferrer"},"source",-1)])),_:1,__:[112]})]),t("details",S,[t("summary",null,[e[116]||(e[116]=t("a",{id:"Reactant.MLIR.Dialects.nvvm.fence_proxy-Tuple{}",href:"#Reactant.MLIR.Dialects.nvvm.fence_proxy-Tuple{}"},[t("span",{class:"jlbinding"},"Reactant.MLIR.Dialects.nvvm.fence_proxy")],-1)),e[117]||(e[117]=a()),s(n,{type:"info",class:"jlObjectType jlMethod",text:"Method"})]),e[119]||(e[119]=t("p",null,[t("code",null,"fence_proxy")],-1)),e[120]||(e[120]=t("p",null,"Fence operation with proxy to establish an ordering between memory accesses that may happen through different proxies.",-1)),e[121]||(e[121]=t("p",null,[t("a",{href:"https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#parallel-synchronization-and-communication-instructions-membar",target:"_blank",rel:"noreferrer"},"For more information, see PTX ISA")],-1)),s(n,{type:"info",class:"source-link",text:"source"},{default:l(()=>e[118]||(e[118]=[t("a",{href:"https://github.com/EnzymeAD/Reactant.jl/blob/9dbf8a81dd5743ab847157c74394e53975a2e470/src/mlir/Dialects/Nvvm.jl#L2016-L2023",target:"_blank",rel:"noreferrer"},"source",-1)])),_:1,__:[118]})]),t("details",N,[t("summary",null,[e[122]||(e[122]=t("a",{id:"Reactant.MLIR.Dialects.nvvm.fence_proxy_acquire-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}",href:"#Reactant.MLIR.Dialects.nvvm.fence_proxy_acquire-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}"},[t("span",{class:"jlbinding"},"Reactant.MLIR.Dialects.nvvm.fence_proxy_acquire")],-1)),e[123]||(e[123]=a()),s(n,{type:"info",class:"jlObjectType jlMethod",text:"Method"})]),e[125]||(e[125]=o('<p><code>fence_proxy_acquire</code></p><p><code>fence.proxy.acquire</code> is a uni-directional fence used to establish ordering between a prior memory access performed via the generic proxy and a subsequent memory access performed via the tensormap proxy</p><p>The address operand <code>addr</code> and the operand <code>size</code> together specify the memory range <code>[addr, addr+size)</code> on which the ordering guarantees on the memory accesses across the proxies is to be provided. The only supported value for the <code>size</code> operand is 128 and must be an immediate. Generic Addressing is used unconditionally, and the address specified by the operand <code>addr</code> must fall within the <code>.global</code> state space. Otherwise, the behavior is undefined</p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#parallel-synchronization-and-communication-instructions-membar" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p>',4)),s(n,{type:"info",class:"source-link",text:"source"},{default:l(()=>e[124]||(e[124]=[t("a",{href:"https://github.com/EnzymeAD/Reactant.jl/blob/9dbf8a81dd5743ab847157c74394e53975a2e470/src/mlir/Dialects/Nvvm.jl#L1977-L1992",target:"_blank",rel:"noreferrer"},"source",-1)])),_:1,__:[124]})]),t("details",C,[t("summary",null,[e[126]||(e[126]=t("a",{id:"Reactant.MLIR.Dialects.nvvm.fence_proxy_release-Tuple{}",href:"#Reactant.MLIR.Dialects.nvvm.fence_proxy_release-Tuple{}"},[t("span",{class:"jlbinding"},"Reactant.MLIR.Dialects.nvvm.fence_proxy_release")],-1)),e[127]||(e[127]=a()),s(n,{type:"info",class:"jlObjectType jlMethod",text:"Method"})]),e[129]||(e[129]=t("p",null,[t("code",null,"fence_proxy_release")],-1)),e[130]||(e[130]=t("p",null,[t("code",null,"fence.proxy.release"),a(" is a uni-directional fence used to establish ordering between a prior memory access performed via the generic proxy and a subsequent memory access performed via the tensormap proxy. "),t("code",null,"fence.proxy.release"),a(" operation can form a release sequence that synchronizes with an acquire sequence that contains the fence.proxy.acquire proxy fence operation")],-1)),e[131]||(e[131]=t("p",null,[t("a",{href:"https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#parallel-synchronization-and-communication-instructions-membar",target:"_blank",rel:"noreferrer"},"For more information, see PTX ISA")],-1)),s(n,{type:"info",class:"source-link",text:"source"},{default:l(()=>e[128]||(e[128]=[t("a",{href:"https://github.com/EnzymeAD/Reactant.jl/blob/9dbf8a81dd5743ab847157c74394e53975a2e470/src/mlir/Dialects/Nvvm.jl#L2044-L2054",target:"_blank",rel:"noreferrer"},"source",-1)])),_:1,__:[128]})]),t("details",H,[t("summary",null,[e[132]||(e[132]=t("a",{id:"Reactant.MLIR.Dialects.nvvm.griddepcontrol_launch_dependents-Tuple{}",href:"#Reactant.MLIR.Dialects.nvvm.griddepcontrol_launch_dependents-Tuple{}"},[t("span",{class:"jlbinding"},"Reactant.MLIR.Dialects.nvvm.griddepcontrol_launch_dependents")],-1)),e[133]||(e[133]=a()),s(n,{type:"info",class:"jlObjectType jlMethod",text:"Method"})]),e[135]||(e[135]=t("p",null,[t("code",null,"griddepcontrol_launch_dependents")],-1)),e[136]||(e[136]=t("p",null,"Signals that specific dependents the runtime system designated to react to this instruction can be scheduled as soon as all other CTAs in the grid issue the same instruction or have completed.",-1)),e[137]||(e[137]=t("p",null,[t("a",{href:"https://docs.nvidia.com/cuda/parallel-thread-execution/#parallel-synchronization-and-communication-instructions-griddepcontrol",target:"_blank",rel:"noreferrer"},"For more information, see PTX ISA")],-1)),s(n,{type:"info",class:"source-link",text:"source"},{default:l(()=>e[134]||(e[134]=[t("a",{href:"https://github.com/EnzymeAD/Reactant.jl/blob/9dbf8a81dd5743ab847157c74394e53975a2e470/src/mlir/Dialects/Nvvm.jl#L2196-L2205",target:"_blank",rel:"noreferrer"},"source",-1)])),_:1,__:[134]})]),t("details",P,[t("summary",null,[e[138]||(e[138]=t("a",{id:"Reactant.MLIR.Dialects.nvvm.griddepcontrol_wait-Tuple{}",href:"#Reactant.MLIR.Dialects.nvvm.griddepcontrol_wait-Tuple{}"},[t("span",{class:"jlbinding"},"Reactant.MLIR.Dialects.nvvm.griddepcontrol_wait")],-1)),e[139]||(e[139]=a()),s(n,{type:"info",class:"jlObjectType jlMethod",text:"Method"})]),e[141]||(e[141]=t("p",null,[t("code",null,"griddepcontrol_wait")],-1)),e[142]||(e[142]=t("p",null,"Causes the executing thread to wait until all prerequisite grids in flight have completed and all the memory operations from the prerequisite grids are performed and made visible to the current grid.",-1)),e[143]||(e[143]=t("p",null,[t("a",{href:"https://docs.nvidia.com/cuda/parallel-thread-execution/#parallel-synchronization-and-communication-instructions-griddepcontrol",target:"_blank",rel:"noreferrer"},"For more information, see PTX ISA")],-1)),s(n,{type:"info",class:"source-link",text:"source"},{default:l(()=>e[140]||(e[140]=[t("a",{href:"https://github.com/EnzymeAD/Reactant.jl/blob/9dbf8a81dd5743ab847157c74394e53975a2e470/src/mlir/Dialects/Nvvm.jl#L2225-L2234",target:"_blank",rel:"noreferrer"},"source",-1)])),_:1,__:[140]})]),t("details",F,[t("summary",null,[e[144]||(e[144]=t("a",{id:"Reactant.MLIR.Dialects.nvvm.match_sync-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}",href:"#Reactant.MLIR.Dialects.nvvm.match_sync-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}"},[t("span",{class:"jlbinding"},"Reactant.MLIR.Dialects.nvvm.match_sync")],-1)),e[145]||(e[145]=a()),s(n,{type:"info",class:"jlObjectType jlMethod",text:"Method"})]),e[147]||(e[147]=o('<p><code>match_sync</code></p><p>The <code>match.sync</code> op performs broadcast and compare of operand <code>val</code> across all non-exited threads in <code>thread_mask</code> and returns a mask depending on the kind and an optional predicate.</p><p>The matching operation kinds are:</p><ul><li><code>any</code>: Returns a mask corresponding to the non-exited threads in the</li></ul><p><code>thread_mask</code> that have the same value of operand <code>val</code>.</p><ul><li><code>all</code>: Returns a mask and a predicate. If all non-exited threads in the</li></ul><p><code>thread_mask</code> have the same value of operand <code>val</code>, the predicate is set to true and the mask corresponds to the non-exited threads in the <code>thread_mask</code>. Otherwise, the predicate is set to false and the mask is 0.</p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#parallel-synchronization-and-communication-instructions-match-sync" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p>',8)),s(n,{type:"info",class:"source-link",text:"source"},{default:l(()=>e[146]||(e[146]=[t("a",{href:"https://github.com/EnzymeAD/Reactant.jl/blob/9dbf8a81dd5743ab847157c74394e53975a2e470/src/mlir/Dialects/Nvvm.jl#L2703-L2719",target:"_blank",rel:"noreferrer"},"source",-1)])),_:1,__:[146]})]),t("details",z,[t("summary",null,[e[148]||(e[148]=t("a",{id:"Reactant.MLIR.Dialects.nvvm.mma_sync-Tuple{Vector{Reactant.MLIR.IR.Value}, Vector{Reactant.MLIR.IR.Value}, Vector{Reactant.MLIR.IR.Value}}",href:"#Reactant.MLIR.Dialects.nvvm.mma_sync-Tuple{Vector{Reactant.MLIR.IR.Value}, Vector{Reactant.MLIR.IR.Value}, Vector{Reactant.MLIR.IR.Value}}"},[t("span",{class:"jlbinding"},"Reactant.MLIR.Dialects.nvvm.mma_sync")],-1)),e[149]||(e[149]=a()),s(n,{type:"info",class:"jlObjectType jlMethod",text:"Method"})]),e[151]||(e[151]=o(`<p><code>mma_sync</code></p><p>The <code>nvvm.mma.sync</code> operation collectively performs the operation <code>D = matmul(A, B) + C</code> using all threads in a warp.</p><p>All the threads in the warp must execute the same <code>mma.sync</code> operation.</p><p>For each possible multiplicand PTX data type, there are one or more possible instruction shapes given as &quot;mMnNkK&quot;. The below table describes the posssibilities as well as the types required for the operands. Note that the data type for C (the accumulator) and D (the result) can vary independently when there are multiple possibilities in the &quot;C/D Type&quot; column.</p><p>When an optional attribute cannot be immediately inferred from the types of the operands and the result during parsing or validation, an error will be raised.</p><p><code>b1Op</code> is only relevant when the binary (b1) type is given to <code>multiplicandDataType</code>. It specifies how the multiply-and-acumulate is performed and is either <code>xor_popc</code> or <code>and_poc</code>. The default is <code>xor_popc</code>.</p><p><code>intOverflowBehavior</code> is only relevant when the <code>multiplicandType</code> attribute is one of <code>u8, s8, u4, s4</code>, this attribute describes how overflow is handled in the accumulator. When the attribute is <code>satfinite</code>, the accumulator values are clamped in the int32 range on overflow. This is the default behavior. Alternatively, accumulator behavior <code>wrapped</code> can also be specified, in which case overflow wraps from one end of the range to the other.</p><p><code>layoutA</code> and <code>layoutB</code> are required and should generally be set to <code>#nvvm.mma_layout&lt;row&gt;</code> and <code>#nvvm.mma_layout&lt;col&gt;</code> respectively, but other combinations are possible for certain layouts according to the table below.</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>| A/B Type | Shape     | ALayout | BLayout | A Type   | B Type   | C/D Type          |</span></span>
<span class="line"><span>|----------|-----------|---------|---------|----------|----------|-------------------|</span></span>
<span class="line"><span>| f64      | .m8n8k4   | row     | col     | 1x f64   | 1x f64   | 2x f64            |</span></span>
<span class="line"><span>| f16      | .m8n8k4   | row/col | row/col | 2x f16x2 | 2x f16x2 | 4x f16x2 or 8xf32 |</span></span>
<span class="line"><span>|          | .m16n8k8  | row     | col     | 2x f16x2 | 1x f16x2 | 2x f16x2 or 4 f32 |</span></span>
<span class="line"><span>|          | .m16n8k16 | row     | col     | 4x f16x2 | 2x f16x2 | 2x f16x2 or 4 f32 |</span></span>
<span class="line"><span>| bf16     | .m16n8k8  | row     | col     | 2x i32   | 1x i32   | 4x f32            |</span></span>
<span class="line"><span>|          | .m16n8k16 | row     | col     | 4x i32   | 2x i32   | 4x f32            |</span></span>
<span class="line"><span>| tf32     | .m16n8k4  | row     | col     | 2x i32   | 1x i32   | 4x f32            |</span></span>
<span class="line"><span>|          | .m16n8k8  | row     | col     | 4x i32   | 2x i32   | 2x f16x2 or 4 f32 |</span></span>
<span class="line"><span>| u8/s8    | .m8n8k16  | row     | col     | 1x i32   | 1x i32   | 2x i32            |</span></span>
<span class="line"><span>|          | .m16n8k16 | row     | col     | 2x i32   | 1x i32   | 4x i32            |</span></span>
<span class="line"><span>|          | .m16n8k32 | row     | col     | 4x i32   | 2x i32   | 4x i32            |</span></span>
<span class="line"><span>| u4/s4    | .m8n8k32  | row     | col     | 1x i32   | 1x i32   | 2x i32            |</span></span>
<span class="line"><span>|          | m16n8k32  | row     | col     | 2x i32   | 1x i32   | 4x i32            |</span></span>
<span class="line"><span>|          | m16n8k64  | row     | col     | 4x i32   | 2x i32   | 4x i32            |</span></span>
<span class="line"><span>| b1       | m8n8k128  | row     | col     | 1x i32   | 1x i32   | 2x i32            |</span></span>
<span class="line"><span>|          | m16n8k128 | row     | col     | 2x i32   | 1x i32   | 4x i32            |</span></span></code></pre></div><p><strong>Example</strong></p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span></span></span>
<span class="line"><span>%128 = nvvm.mma.sync A[%120, %121, %122, %123]</span></span>
<span class="line"><span>                     B[%124, %125]</span></span>
<span class="line"><span>                     C[%126, %127]</span></span>
<span class="line"><span>                     {layoutA = #nvvm.mma_layout&lt;row&gt;,</span></span>
<span class="line"><span>                      layoutB = #nvvm.mma_layout&lt;col&gt;,</span></span>
<span class="line"><span>                      shape = {k = 16 : i32, m = 16 : i32, n = 8 : i32}}</span></span>
<span class="line"><span>    : (vector&lt;2xf16&gt;, vector&lt;2xf16&gt;, vector&lt;2xf16&gt;)</span></span>
<span class="line"><span>       -&gt; !llvm.struct&lt;(vector&lt;2xf16&gt;, vector&lt;2xf16&gt;)&gt;</span></span></code></pre></div>`,11)),s(n,{type:"info",class:"source-link",text:"source"},{default:l(()=>e[150]||(e[150]=[t("a",{href:"https://github.com/EnzymeAD/Reactant.jl/blob/9dbf8a81dd5743ab847157c74394e53975a2e470/src/mlir/Dialects/Nvvm.jl#L2739-L2806",target:"_blank",rel:"noreferrer"},"source",-1)])),_:1,__:[150]})]),t("details",E,[t("summary",null,[e[152]||(e[152]=t("a",{id:"Reactant.MLIR.Dialects.nvvm.redux_sync-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}",href:"#Reactant.MLIR.Dialects.nvvm.redux_sync-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}"},[t("span",{class:"jlbinding"},"Reactant.MLIR.Dialects.nvvm.redux_sync")],-1)),e[153]||(e[153]=a()),s(n,{type:"info",class:"jlObjectType jlMethod",text:"Method"})]),e[155]||(e[155]=o('<p><code>redux_sync</code></p><p><code>redux.sync</code> performs a reduction operation <code>kind</code> of the 32 bit source register across all non-exited threads in the membermask.</p><p>The <code>abs</code> and <code>nan</code> attributes can be used in the case of f32 input type, where the <code>abs</code> attribute causes the absolute value of the input to be used in the reduction operation, and the <code>nan</code> attribute causes the reduction operation to return NaN if any of the inputs to participating threads are NaN.</p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#parallel-synchronization-and-communication-instructions-redux-sync" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p>',4)),s(n,{type:"info",class:"source-link",text:"source"},{default:l(()=>e[154]||(e[154]=[t("a",{href:"https://github.com/EnzymeAD/Reactant.jl/blob/9dbf8a81dd5743ab847157c74394e53975a2e470/src/mlir/Dialects/Nvvm.jl#L2895-L2908",target:"_blank",rel:"noreferrer"},"source",-1)])),_:1,__:[154]})]),t("details",O,[t("summary",null,[e[156]||(e[156]=t("a",{id:"Reactant.MLIR.Dialects.nvvm.shfl_sync-NTuple{4, Reactant.MLIR.IR.Value}",href:"#Reactant.MLIR.Dialects.nvvm.shfl_sync-NTuple{4, Reactant.MLIR.IR.Value}"},[t("span",{class:"jlbinding"},"Reactant.MLIR.Dialects.nvvm.shfl_sync")],-1)),e[157]||(e[157]=a()),s(n,{type:"info",class:"jlObjectType jlMethod",text:"Method"})]),e[159]||(e[159]=o('<p><code>shfl_sync</code></p><p>The <code>shfl.sync</code> Op implements data shuffle within threads of a warp. The <code>thread_mask</code> denotes the threads participating in the Op where the bit position corresponds to a particular threadâ€™s laneid. The <code>offset</code> specifies a source lane or source lane offset (depending on <code>kind</code>). The <code>val</code> is the input value to be copied from the source. The <code>mask_and_clamp</code> contains two packed values specifying a mask for logically splitting warps into sub-segments and an upper bound for clamping the source lane index.</p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#data-movement-and-conversion-instructions-shfl-sync" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p>',3)),s(n,{type:"info",class:"source-link",text:"source"},{default:l(()=>e[158]||(e[158]=[t("a",{href:"https://github.com/EnzymeAD/Reactant.jl/blob/9dbf8a81dd5743ab847157c74394e53975a2e470/src/mlir/Dialects/Nvvm.jl#L2959-L2972",target:"_blank",rel:"noreferrer"},"source",-1)])),_:1,__:[158]})]),t("details",Z,[t("summary",null,[e[160]||(e[160]=t("a",{id:"Reactant.MLIR.Dialects.nvvm.st_bulk-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}",href:"#Reactant.MLIR.Dialects.nvvm.st_bulk-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}"},[t("span",{class:"jlbinding"},"Reactant.MLIR.Dialects.nvvm.st_bulk")],-1)),e[161]||(e[161]=a()),s(n,{type:"info",class:"jlObjectType jlMethod",text:"Method"})]),e[163]||(e[163]=t("p",null,[t("code",null,"st_bulk")],-1)),e[164]||(e[164]=t("p",null,[a("Initializes a region of shared memory at the address given by "),t("code",null,"addr"),a(". The "),t("code",null,"size"),a(" operand specifies the number of bytes to initialize and must be a multiple of 8. The "),t("code",null,"initVal"),a(" operand specifies the value to initialize the memory to. The only supported value is 0.")],-1)),e[165]||(e[165]=t("p",null,[t("a",{href:"https://docs.nvidia.com/cuda/parallel-thread-execution/#data-movement-and-conversion-instructions-st-bulk",target:"_blank",rel:"noreferrer"},"For more information, see PTX ISA")],-1)),s(n,{type:"info",class:"source-link",text:"source"},{default:l(()=>e[162]||(e[162]=[t("a",{href:"https://github.com/EnzymeAD/Reactant.jl/blob/9dbf8a81dd5743ab847157c74394e53975a2e470/src/mlir/Dialects/Nvvm.jl#L307-L317",target:"_blank",rel:"noreferrer"},"source",-1)])),_:1,__:[162]})]),t("details",X,[t("summary",null,[e[166]||(e[166]=t("a",{id:"Reactant.MLIR.Dialects.nvvm.stmatrix-Tuple{Reactant.MLIR.IR.Value, Vector{Reactant.MLIR.IR.Value}}",href:"#Reactant.MLIR.Dialects.nvvm.stmatrix-Tuple{Reactant.MLIR.IR.Value, Vector{Reactant.MLIR.IR.Value}}"},[t("span",{class:"jlbinding"},"Reactant.MLIR.Dialects.nvvm.stmatrix")],-1)),e[167]||(e[167]=a()),s(n,{type:"info",class:"jlObjectType jlMethod",text:"Method"})]),e[169]||(e[169]=t("p",null,[t("code",null,"stmatrix")],-1)),e[170]||(e[170]=t("p",null,"Collectively store one or more matrices across all threads in a warp to the location indicated by the address operand ptr in shared memory.",-1)),e[171]||(e[171]=t("p",null,[t("a",{href:"https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-store-instruction-stmatrix",target:"_blank",rel:"noreferrer"},"For more information, see PTX ISA")],-1)),s(n,{type:"info",class:"source-link",text:"source"},{default:l(()=>e[168]||(e[168]=[t("a",{href:"https://github.com/EnzymeAD/Reactant.jl/blob/9dbf8a81dd5743ab847157c74394e53975a2e470/src/mlir/Dialects/Nvvm.jl#L3045-L3052",target:"_blank",rel:"noreferrer"},"source",-1)])),_:1,__:[168]})]),t("details",q,[t("summary",null,[e[172]||(e[172]=t("a",{id:"Reactant.MLIR.Dialects.nvvm.tcgen05_alloc-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}",href:"#Reactant.MLIR.Dialects.nvvm.tcgen05_alloc-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}"},[t("span",{class:"jlbinding"},"Reactant.MLIR.Dialects.nvvm.tcgen05_alloc")],-1)),e[173]||(e[173]=a()),s(n,{type:"info",class:"jlObjectType jlMethod",text:"Method"})]),e[175]||(e[175]=t("p",null,[t("code",null,"tcgen05_alloc")],-1)),e[176]||(e[176]=t("p",null,[a("The "),t("code",null,"tcgen05.alloc"),a(" Op allocates tensor core memory for the amount specified by "),t("code",null,"nCols"),a(" and writes the destination address to the "),t("code",null,"addr"),a(" argument. The "),t("code",null,"nCols"),a(" operand specifies the number of columns to be allocated and it must be a power-of-two. "),t("a",{href:"https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-memory-alloc-manage-instructions",target:"_blank",rel:"noreferrer"},"For more information, see PTX ISA")],-1)),s(n,{type:"info",class:"source-link",text:"source"},{default:l(()=>e[174]||(e[174]=[t("a",{href:"https://github.com/EnzymeAD/Reactant.jl/blob/9dbf8a81dd5743ab847157c74394e53975a2e470/src/mlir/Dialects/Nvvm.jl#L3091-L3099",target:"_blank",rel:"noreferrer"},"source",-1)])),_:1,__:[174]})]),t("details",B,[t("summary",null,[e[177]||(e[177]=t("a",{id:"Reactant.MLIR.Dialects.nvvm.tcgen05_commit",href:"#Reactant.MLIR.Dialects.nvvm.tcgen05_commit"},[t("span",{class:"jlbinding"},"Reactant.MLIR.Dialects.nvvm.tcgen05_commit")],-1)),e[178]||(e[178]=a()),s(n,{type:"info",class:"jlObjectType jlFunction",text:"Function"})]),e[180]||(e[180]=o('<p><code>tcgen05_commit</code></p><p>The <code>tcgen05.commit</code> makes the mbarrier object, specified by the operand <code>addr</code>, track the completion of all the prior async-tcgen05 operations initiated by the executing thread. The multicast variants allow signaling on the mbarrier objects of multiple CTAs within the cluster. Operand <code>multicastMask</code>, when present, specifies the destination CTAs in the cluster such that each bit position in the 16-bit <code>multicastMask</code> operand corresponds to the <code>nvvm.read.ptx.sreg.ctaid</code> of the destination CTA. <a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen-async-sync-operations-commit" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p>',2)),s(n,{type:"info",class:"source-link",text:"source"},{default:l(()=>e[179]||(e[179]=[t("a",{href:"https://github.com/EnzymeAD/Reactant.jl/blob/9dbf8a81dd5743ab847157c74394e53975a2e470/src/mlir/Dialects/Nvvm.jl#L3120-L3132",target:"_blank",rel:"noreferrer"},"source",-1)])),_:1,__:[179]})]),t("details",$,[t("summary",null,[e[181]||(e[181]=t("a",{id:"Reactant.MLIR.Dialects.nvvm.tcgen05_cp-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}",href:"#Reactant.MLIR.Dialects.nvvm.tcgen05_cp-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}"},[t("span",{class:"jlbinding"},"Reactant.MLIR.Dialects.nvvm.tcgen05_cp")],-1)),e[182]||(e[182]=a()),s(n,{type:"info",class:"jlObjectType jlMethod",text:"Method"})]),e[184]||(e[184]=o(`<p><code>tcgen05_cp</code></p><p>Instruction tcgen05.cp initiates an asynchronous copy operation from shared memory to the location specified by the address operand <code>taddr</code> in the Tensor Memory. The 64-bit register operand <code>smem_desc</code> specifies the matrix descriptor representing the source matrix in the shared memory that needs to be copied.</p><p><strong>Example</strong></p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>  nvvm.tcgen05.cp %taddr, %smem_desc {</span></span>
<span class="line"><span>    group = #nvvm.tcgen05_group&lt;cta_2&gt;,</span></span>
<span class="line"><span>    shape = #nvvm.tcgen05_cp_shape&lt;shape_64x128b&gt;,</span></span>
<span class="line"><span>    multicast = #nvvm.tcgen05_cp_multicast&lt;warpx2_01_23&gt;,</span></span>
<span class="line"><span>    srcFormat = #nvvm.tcgen05_cp_src_fmt&lt;b6x16_p32&gt;</span></span>
<span class="line"><span>  }</span></span></code></pre></div><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#tensorcore-5th-generation-instructions-tcgen05-cp" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p>`,5)),s(n,{type:"info",class:"source-link",text:"source"},{default:l(()=>e[183]||(e[183]=[t("a",{href:"https://github.com/EnzymeAD/Reactant.jl/blob/9dbf8a81dd5743ab847157c74394e53975a2e470/src/mlir/Dialects/Nvvm.jl#L3159-L3178",target:"_blank",rel:"noreferrer"},"source",-1)])),_:1,__:[183]})]),t("details",W,[t("summary",null,[e[185]||(e[185]=t("a",{id:"Reactant.MLIR.Dialects.nvvm.tcgen05_dealloc-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}",href:"#Reactant.MLIR.Dialects.nvvm.tcgen05_dealloc-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}"},[t("span",{class:"jlbinding"},"Reactant.MLIR.Dialects.nvvm.tcgen05_dealloc")],-1)),e[186]||(e[186]=a()),s(n,{type:"info",class:"jlObjectType jlMethod",text:"Method"})]),e[188]||(e[188]=t("p",null,[t("code",null,"tcgen05_dealloc")],-1)),e[189]||(e[189]=t("p",null,[a("The "),t("code",null,"tcgen05.dealloc"),a(" Op de-allocates the tensor core memory specified by "),t("code",null,"tmemAddr"),a(", which must be from a previous tensor memory allocation. The "),t("code",null,"nCols"),a(" operand specifies the number of columns to be de-allocated, and it must be a power-of-two. "),t("a",{href:"https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-memory-alloc-manage-instructions",target:"_blank",rel:"noreferrer"},"For more information, see PTX ISA")],-1)),s(n,{type:"info",class:"source-link",text:"source"},{default:l(()=>e[187]||(e[187]=[t("a",{href:"https://github.com/EnzymeAD/Reactant.jl/blob/9dbf8a81dd5743ab847157c74394e53975a2e470/src/mlir/Dialects/Nvvm.jl#L3209-L3217",target:"_blank",rel:"noreferrer"},"source",-1)])),_:1,__:[187]})]),t("details",G,[t("summary",null,[e[190]||(e[190]=t("a",{id:"Reactant.MLIR.Dialects.nvvm.tcgen05_fence-Tuple{}",href:"#Reactant.MLIR.Dialects.nvvm.tcgen05_fence-Tuple{}"},[t("span",{class:"jlbinding"},"Reactant.MLIR.Dialects.nvvm.tcgen05_fence")],-1)),e[191]||(e[191]=a()),s(n,{type:"info",class:"jlObjectType jlMethod",text:"Method"})]),e[193]||(e[193]=t("p",null,[t("code",null,"tcgen05_fence")],-1)),e[194]||(e[194]=t("p",null,[a("The "),t("code",null,"tcgen05.fence<before>"),a(" orders all prior async tcgen05 operations with respect to the subsequent tcgen05 and execution ordering operations. The "),t("code",null,"tcgen05.fence<after>"),a(" orders all subsequent async tcgen05 operations with respect to the prior tcgen05 and execution ordering operations.")],-1)),e[195]||(e[195]=t("p",null,[t("a",{href:"https://docs.nvidia.com/cuda/parallel-thread-execution/#tensorcore-5th-generation-instructions-tcgen05-fence",target:"_blank",rel:"noreferrer"},"For more information, see PTX ISA")],-1)),s(n,{type:"info",class:"source-link",text:"source"},{default:l(()=>e[192]||(e[192]=[t("a",{href:"https://github.com/EnzymeAD/Reactant.jl/blob/9dbf8a81dd5743ab847157c74394e53975a2e470/src/mlir/Dialects/Nvvm.jl#L3238-L3247",target:"_blank",rel:"noreferrer"},"source",-1)])),_:1,__:[192]})]),t("details",J,[t("summary",null,[e[196]||(e[196]=t("a",{id:"Reactant.MLIR.Dialects.nvvm.tcgen05_ld",href:"#Reactant.MLIR.Dialects.nvvm.tcgen05_ld"},[t("span",{class:"jlbinding"},"Reactant.MLIR.Dialects.nvvm.tcgen05_ld")],-1)),e[197]||(e[197]=a()),s(n,{type:"info",class:"jlObjectType jlFunction",text:"Function"})]),e[199]||(e[199]=o(`<p><code>tcgen05_ld</code></p><p>Instruction <code>tcgen05.ld</code> asynchronously loads data from the Tensor Memory at the location specified by the 32-bit address operand <code>tmemAddr</code> into the destination register <code>res</code>, collectively across all threads of the warps.</p><p>The <code>shape</code> and the <code>num</code> attribute together determines the total dimension of the data which is loaded from the Tensor Memory. The <code>shape</code> attribute indicates the base dimension of data to be accessed as described in the Data Movement Shape. The <code>num</code> attribute indicates the repeat factor on the base dimension resulting in the total dimension of the data that is accessed.</p><p>The shape <code>16x32bx2</code> performs two accesses into Tensor Memory of the shape <code>16x32b</code>. The base address of the first access is specified by <code>tmemAddr</code> and the base address of the second access is specified by <code>tmemAddr + offset</code>, where <code>offset</code> is an immediate argument.</p><p>The unit attribute <code>pack</code> can be used to pack two 16-bit elements from adjacent columns into a single 32-bit element during the load.</p><p>The following table describes the size of the vector for various combinations of <code>num</code> and <code>shape</code> attributes |=====================================================================| | num/shape | 16x32bx2/16x64b/32x32b | 16x128b | 16x256b | |=====================================================================| | x1 | 1 | 2 | 4 | | x2 | 2 | 4 | 8 | | x4 | 4 | 8 | 16 | | x8 | 8 | 16 | 32 | | x16 | 16 | 32 | 64 | | x32 | 32 | 64 | 128 | | x64 | 64 | 128 | NA | | x128 | 128 | NA | NA | |=====================================================================|</p><p><strong>Example</strong></p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>  nvvm.tcgen05.ld %tmemAddr, %offset pack {</span></span>
<span class="line"><span>    shape = #nvvm.tcgen05_ldst_shape&lt;shape_16x32bx2&gt;,</span></span>
<span class="line"><span>  } : &lt;2xi32&gt;</span></span></code></pre></div><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-instructions-tcgen05-st" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p>`,9)),s(n,{type:"info",class:"source-link",text:"source"},{default:l(()=>e[198]||(e[198]=[t("a",{href:"https://github.com/EnzymeAD/Reactant.jl/blob/9dbf8a81dd5743ab847157c74394e53975a2e470/src/mlir/Dialects/Nvvm.jl#L3267-L3312",target:"_blank",rel:"noreferrer"},"source",-1)])),_:1,__:[198]})]),t("details",K,[t("summary",null,[e[200]||(e[200]=t("a",{id:"Reactant.MLIR.Dialects.nvvm.tcgen05_relinquish_alloc_permit-Tuple{}",href:"#Reactant.MLIR.Dialects.nvvm.tcgen05_relinquish_alloc_permit-Tuple{}"},[t("span",{class:"jlbinding"},"Reactant.MLIR.Dialects.nvvm.tcgen05_relinquish_alloc_permit")],-1)),e[201]||(e[201]=a()),s(n,{type:"info",class:"jlObjectType jlMethod",text:"Method"})]),e[203]||(e[203]=t("p",null,[t("code",null,"tcgen05_relinquish_alloc_permit")],-1)),e[204]||(e[204]=t("p",null,[a("The "),t("code",null,"tcgen05.relinquish_alloc_permit"),a(" Op specifies that the CTA of the executing thread is relinquishing the right to allocate Tensor Memory. So, it is illegal for a CTA to perform "),t("code",null,"tcgen05.alloc"),a(" after any of its constituent threads execute "),t("code",null,"tcgen05.relinquish_alloc_permit"),a(". "),t("a",{href:"https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-memory-alloc-manage-instructions",target:"_blank",rel:"noreferrer"},"For more information, see PTX ISA")],-1)),s(n,{type:"info",class:"source-link",text:"source"},{default:l(()=>e[202]||(e[202]=[t("a",{href:"https://github.com/EnzymeAD/Reactant.jl/blob/9dbf8a81dd5743ab847157c74394e53975a2e470/src/mlir/Dialects/Nvvm.jl#L3341-L3349",target:"_blank",rel:"noreferrer"},"source",-1)])),_:1,__:[202]})]),t("details",U,[t("summary",null,[e[205]||(e[205]=t("a",{id:"Reactant.MLIR.Dialects.nvvm.tcgen05_shift-Tuple{Reactant.MLIR.IR.Value}",href:"#Reactant.MLIR.Dialects.nvvm.tcgen05_shift-Tuple{Reactant.MLIR.IR.Value}"},[t("span",{class:"jlbinding"},"Reactant.MLIR.Dialects.nvvm.tcgen05_shift")],-1)),e[206]||(e[206]=a()),s(n,{type:"info",class:"jlObjectType jlMethod",text:"Method"})]),e[208]||(e[208]=t("p",null,[t("code",null,"tcgen05_shift")],-1)),e[209]||(e[209]=t("p",null,[a("The "),t("code",null,"tcgen05.shift"),a(" is an asynchronous instruction which initiates the shifting of 32-byte elements downwards across all the rows, except the last, by one row. The operand "),t("code",null,"taddr"),a(" specifies the base address of the matrix in Tensor Memory whose rows must be down shifted.")],-1)),e[210]||(e[210]=t("p",null,[t("a",{href:"https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-instructions-tcgen05-shift",target:"_blank",rel:"noreferrer"},"For more information, see PTX ISA")],-1)),s(n,{type:"info",class:"source-link",text:"source"},{default:l(()=>e[207]||(e[207]=[t("a",{href:"https://github.com/EnzymeAD/Reactant.jl/blob/9dbf8a81dd5743ab847157c74394e53975a2e470/src/mlir/Dialects/Nvvm.jl#L3370-L3379",target:"_blank",rel:"noreferrer"},"source",-1)])),_:1,__:[207]})]),t("details",Y,[t("summary",null,[e[211]||(e[211]=t("a",{id:"Reactant.MLIR.Dialects.nvvm.tcgen05_st",href:"#Reactant.MLIR.Dialects.nvvm.tcgen05_st"},[t("span",{class:"jlbinding"},"Reactant.MLIR.Dialects.nvvm.tcgen05_st")],-1)),e[212]||(e[212]=a()),s(n,{type:"info",class:"jlObjectType jlFunction",text:"Function"})]),e[214]||(e[214]=o(`<p><code>tcgen05_st</code></p><p>Instruction <code>tcgen05.st</code> asynchronously stores data from the source register <code>r</code> into the Tensor Memory at the location specified by the 32-bit address operand <code>tmemAddr</code>, collectively across all threads of the warps.</p><p>The <code>shape</code> and the <code>num</code> attribute together determines the total dimension of the data which is stored to the Tensor Memory. The <code>shape</code> indicates the base dimension of data to be accessed. The <code>num</code> attribute indicates the repeat factor on the base dimension resulting in the total dimension of the data that is accessed.</p><p>The shape <code>16x32bx2</code> performs two accesses into Tensor Memory of the shape <code>16x32b</code>. The base address of the first access is specified by <code>tmemAddr</code> and the base address of the second access is specified by <code>tmemAddr + offset</code>, where <code>offset</code> is an immediate argument.</p><p>The unit attribute <code>unpack</code> can be used to unpack a 32-bit element in the register into two 16-bit elements and store them in adjacent columns.</p><p>The following table describes the size of the vector for various combinations of <code>num</code> and <code>shape</code> attributes |=====================================================================| | num/shape | 16x32bx2/16x64b/32x32b | 16x128b | 16x256b | |=====================================================================| | x1 | 1 | 2 | 4 | | x2 | 2 | 4 | 8 | | x4 | 4 | 8 | 16 | | x8 | 8 | 16 | 32 | | x16 | 16 | 32 | 64 | | x32 | 32 | 64 | 128 | | x64 | 64 | 128 | NA | | x128 | 128 | NA | NA | |=====================================================================|</p><p><strong>Example</strong></p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>  nvvm.tcgen05.st %tmemAddr, %val, %offset unpack {</span></span>
<span class="line"><span>    shape = #nvvm.tcgen05_ldst_shape&lt;shape_16x32bx2&gt;,</span></span>
<span class="line"><span>  } : &lt;2xi32&gt;</span></span></code></pre></div><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-instructions-tcgen05-st" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p>`,9)),s(n,{type:"info",class:"source-link",text:"source"},{default:l(()=>e[213]||(e[213]=[t("a",{href:"https://github.com/EnzymeAD/Reactant.jl/blob/9dbf8a81dd5743ab847157c74394e53975a2e470/src/mlir/Dialects/Nvvm.jl#L3400-L3444",target:"_blank",rel:"noreferrer"},"source",-1)])),_:1,__:[213]})]),t("details",ee,[t("summary",null,[e[215]||(e[215]=t("a",{id:"Reactant.MLIR.Dialects.nvvm.tcgen05_wait-Tuple{}",href:"#Reactant.MLIR.Dialects.nvvm.tcgen05_wait-Tuple{}"},[t("span",{class:"jlbinding"},"Reactant.MLIR.Dialects.nvvm.tcgen05_wait")],-1)),e[216]||(e[216]=a()),s(n,{type:"info",class:"jlObjectType jlMethod",text:"Method"})]),e[218]||(e[218]=t("p",null,[t("code",null,"tcgen05_wait")],-1)),e[219]||(e[219]=t("p",null,[a("The "),t("code",null,"tcgen05.wait<load>"),a(" causes the executing thread to block until all prior "),t("code",null,"tcgen05.ld"),a(" operations issued by the executing thread have completed. Similarly, the "),t("code",null,"tcgen05.wait<store>"),a(" causes the executing thread to block until all prior "),t("code",null,"tcgen05.st"),a(" operations issued by the executing thread have completed. "),t("a",{href:"https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-instructions-tcgen05-wait",target:"_blank",rel:"noreferrer"},"For more information, see PTX ISA")],-1)),s(n,{type:"info",class:"source-link",text:"source"},{default:l(()=>e[217]||(e[217]=[t("a",{href:"https://github.com/EnzymeAD/Reactant.jl/blob/9dbf8a81dd5743ab847157c74394e53975a2e470/src/mlir/Dialects/Nvvm.jl#L3473-L3482",target:"_blank",rel:"noreferrer"},"source",-1)])),_:1,__:[217]})]),t("details",te,[t("summary",null,[e[220]||(e[220]=t("a",{id:"Reactant.MLIR.Dialects.nvvm.vote_sync-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}",href:"#Reactant.MLIR.Dialects.nvvm.vote_sync-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}"},[t("span",{class:"jlbinding"},"Reactant.MLIR.Dialects.nvvm.vote_sync")],-1)),e[221]||(e[221]=a()),s(n,{type:"info",class:"jlObjectType jlMethod",text:"Method"})]),e[223]||(e[223]=o('<p><code>vote_sync</code></p><p>The <code>vote.sync</code> op will cause executing thread to wait until all non-exited threads corresponding to membermask have executed <code>vote.sync</code> with the same qualifiers and same membermask value before resuming execution.</p><p>The vote operation kinds are:</p><ul><li><p><code>any</code>: True if source predicate is True for some thread in membermask.</p></li><li><p><code>all</code>: True if source predicate is True for all non-exited threads in membermask.</p></li><li><p><code>uni</code>: True if source predicate has the same value in all non-exited threads in membermask.</p></li><li><p><code>ballot</code>: In the ballot form, the destination result is a 32 bit integer. In this form, the predicate from each thread in membermask are copied into the corresponding bit position of the result, where the bit position corresponds to the threadâ€™s lane id.</p></li></ul><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#parallel-synchronization-and-communication-instructions-vote-sync" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p>',5)),s(n,{type:"info",class:"source-link",text:"source"},{default:l(()=>e[222]||(e[222]=[t("a",{href:"https://github.com/EnzymeAD/Reactant.jl/blob/9dbf8a81dd5743ab847157c74394e53975a2e470/src/mlir/Dialects/Nvvm.jl#L3562-L3581",target:"_blank",rel:"noreferrer"},"source",-1)])),_:1,__:[222]})]),t("details",ae,[t("summary",null,[e[224]||(e[224]=t("a",{id:"Reactant.MLIR.Dialects.nvvm.wgmma_commit_group_sync_aligned-Tuple{}",href:"#Reactant.MLIR.Dialects.nvvm.wgmma_commit_group_sync_aligned-Tuple{}"},[t("span",{class:"jlbinding"},"Reactant.MLIR.Dialects.nvvm.wgmma_commit_group_sync_aligned")],-1)),e[225]||(e[225]=a()),s(n,{type:"info",class:"jlObjectType jlMethod",text:"Method"})]),e[227]||(e[227]=t("p",null,[t("code",null,"wgmma_commit_group_sync_aligned")],-1)),e[228]||(e[228]=t("p",null,"Commits all prior uncommitted warpgroup level matrix multiplication operations.",-1)),e[229]||(e[229]=t("p",null,[t("a",{href:"https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#asynchronous-warpgroup-level-matrix-instructions-wgmma-commit-group",target:"_blank",rel:"noreferrer"},"For more information, see PTX ISA")],-1)),s(n,{type:"info",class:"source-link",text:"source"},{default:l(()=>e[226]||(e[226]=[t("a",{href:"https://github.com/EnzymeAD/Reactant.jl/blob/9dbf8a81dd5743ab847157c74394e53975a2e470/src/mlir/Dialects/Nvvm.jl#L3798-L3804",target:"_blank",rel:"noreferrer"},"source",-1)])),_:1,__:[226]})]),t("details",ne,[t("summary",null,[e[230]||(e[230]=t("a",{id:"Reactant.MLIR.Dialects.nvvm.wgmma_fence_aligned-Tuple{}",href:"#Reactant.MLIR.Dialects.nvvm.wgmma_fence_aligned-Tuple{}"},[t("span",{class:"jlbinding"},"Reactant.MLIR.Dialects.nvvm.wgmma_fence_aligned")],-1)),e[231]||(e[231]=a()),s(n,{type:"info",class:"jlObjectType jlMethod",text:"Method"})]),e[233]||(e[233]=t("p",null,[t("code",null,"wgmma_fence_aligned")],-1)),e[234]||(e[234]=t("p",null,"Enforce an ordering of register accesses between warpgroup level matrix multiplication and other operations.",-1)),e[235]||(e[235]=t("p",null,[t("a",{href:"https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#asynchronous-warpgroup-level-matrix-instructions-wgmma-fence",target:"_blank",rel:"noreferrer"},"For more information, see PTX ISA")],-1)),s(n,{type:"info",class:"source-link",text:"source"},{default:l(()=>e[232]||(e[232]=[t("a",{href:"https://github.com/EnzymeAD/Reactant.jl/blob/9dbf8a81dd5743ab847157c74394e53975a2e470/src/mlir/Dialects/Nvvm.jl#L3771-L3778",target:"_blank",rel:"noreferrer"},"source",-1)])),_:1,__:[232]})]),t("details",se,[t("summary",null,[e[236]||(e[236]=t("a",{id:"Reactant.MLIR.Dialects.nvvm.wgmma_mma_async-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}",href:"#Reactant.MLIR.Dialects.nvvm.wgmma_mma_async-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}"},[t("span",{class:"jlbinding"},"Reactant.MLIR.Dialects.nvvm.wgmma_mma_async")],-1)),e[237]||(e[237]=a()),s(n,{type:"info",class:"jlObjectType jlMethod",text:"Method"})]),e[239]||(e[239]=o(`<p><code>wgmma_mma_async</code></p><p>The warpgroup (128 threads) level matrix multiply and accumulate operation has either of the following forms, where matrix D is called accumulator: D = A * B + D D = A * B, where the input from accumulator D is disabled.</p><p>Supported shapes:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>|--------------|--------------|------------|--------------|---------------|</span></span>
<span class="line"><span>|              |              |            |              |f16+=e4m3*e4m3 |</span></span>
<span class="line"><span>|              |              |            |              |f16+=e5m2*e5m2 |</span></span>
<span class="line"><span>|f32+=tf32*tf32|f16+=f16 *f16 | s32+=s8*s8 |s32 += b1 * b1|f16+=e5m2*e4m3 |</span></span>
<span class="line"><span>|              |f32+=f16 *f16 | s32+=u8*u8 |              |f16+=e4m3*e5m2 |</span></span>
<span class="line"><span>|              |f32+=bf16*bf16| s32+=u8*u8 |              |f16+=e4m3*e5m2 |</span></span>
<span class="line"><span>|              |f32+=bf16*bf16| s32+=s8*u8 |              |f32+=e4m3*e4m3 |</span></span>
<span class="line"><span>|              |              | s32+=u8*s8 |              |f32+=e5m2*e5m2 |</span></span>
<span class="line"><span>|              |              |            |              |f32+=e4m3*e5m2 |</span></span>
<span class="line"><span>|              |              |            |              |f32+=e4m3*e5m2 |</span></span>
<span class="line"><span>|--------------|--------------|------------|--------------|---------------|</span></span>
<span class="line"><span>|   .m64n8k8   |  .m64n8k16   | .m64n8k32  | .m64n8k256   | .m64n8k32     |</span></span>
<span class="line"><span>|   .m64n16k8  |  .m64n16k16  | .m64n16k32 | .m64n16k256  | .m64n16k32    |</span></span>
<span class="line"><span>|   .m64n24k8  |  .m64n24k16  | .m64n24k32 | .m64n24k256  | .m64n24k32    |</span></span>
<span class="line"><span>|   .m64n32k8  |  .m64n32k16  | .m64n32k32 | .m64n32k256  | .m64n32k32    |</span></span>
<span class="line"><span>|   .m64n40k8  |  .m64n40k16  | .m64n48k32 | .m64n48k256  | .m64n40k32    |</span></span>
<span class="line"><span>|   .m64n48k8  |  .m64n48k16  | .m64n64k32 | .m64n64k256  | .m64n48k32    |</span></span>
<span class="line"><span>|   .m64n56k8  |  .m64n56k16  | .m64n80k32 | .m64n80k256  | .m64n56k32    |</span></span>
<span class="line"><span>|   .m64n64k8  |  .m64n64k16  | .m64n96k32 | .m64n96k256  | .m64n64k32    |</span></span>
<span class="line"><span>|   .m64n72k8  |  .m64n72k16  | .m64n112k32| .m64n112k256 | .m64n72k32    |</span></span>
<span class="line"><span>|   .m64n80k8  |  .m64n80k16  | .m64n128k32| .m64n128k256 | .m64n80k32    |</span></span>
<span class="line"><span>|   .m64n88k8  |  .m64n88k16  | .m64n144k32| .m64n144k256 | .m64n88k32    |</span></span>
<span class="line"><span>|   .m64n96k8  |  .m64n96k16  | .m64n160k32| .m64n160k256 | .m64n96k32    |</span></span>
<span class="line"><span>|   .m64n104k8 |  .m64n104k16 | .m64n176k32| .m64n176k256 | .m64n104k32   |</span></span>
<span class="line"><span>|   .m64n112k8 |  .m64n112k16 | .m64n192k32| .m64n192k256 | .m64n112k32   |</span></span>
<span class="line"><span>|   .m64n120k8 |  .m64n120k16 | .m64n208k32| .m64n208k256 | .m64n120k32   |</span></span>
<span class="line"><span>|   .m64n128k8 |  .m64n128k16 | .m64n224k32| .m64n224k256 | .m64n128k32   |</span></span>
<span class="line"><span>|   .m64n136k8 |  .m64n136k16 | .m64n240k32| .m64n240k256 | .m64n136k32   |</span></span>
<span class="line"><span>|   .m64n144k8 |  .m64n144k16 | .m64n256k32| .m64n256k256 | .m64n144k32   |</span></span>
<span class="line"><span>|   .m64n152k8 |  .m64n152k16 |            |              | .m64n152k32   |</span></span>
<span class="line"><span>|   .m64n160k8 |  .m64n160k16 |            |              | .m64n160k32   |</span></span>
<span class="line"><span>|   .m64n168k8 |  .m64n168k16 |            |              | .m64n168k32   |</span></span>
<span class="line"><span>|   .m64n176k8 |  .m64n176k16 |            |              | .m64n176k32   |</span></span>
<span class="line"><span>|   .m64n184k8 |  .m64n184k16 |            |              | .m64n184k32   |</span></span>
<span class="line"><span>|   .m64n192k8 |  .m64n192k16 |            |              | .m64n192k32   |</span></span>
<span class="line"><span>|   .m64n200k8 |  .m64n200k16 |            |              | .m64n200k32   |</span></span>
<span class="line"><span>|   .m64n208k8 |  .m64n208k16 |            |              | .m64n208k32   |</span></span>
<span class="line"><span>|   .m64n216k8 |  .m64n216k16 |            |              | .m64n216k32   |</span></span>
<span class="line"><span>|   .m64n224k8 |  .m64n224k16 |            |              | .m64n224k32   |</span></span>
<span class="line"><span>|   .m64n232k8 |  .m64n232k16 |            |              | .m64n232k32   |</span></span>
<span class="line"><span>|   .m64n240k8 |  .m64n240k16 |            |              | .m64n240k32   |</span></span>
<span class="line"><span>|   .m64n248k8 |  .m64n248k16 |            |              | .m64n248k32   |</span></span>
<span class="line"><span>|   .m64n256k8 |  .m64n256k16 |            |              | .m64n256k32   |</span></span>
<span class="line"><span>|--------------|--------------|------------|--------------|---------------|</span></span></code></pre></div><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#asynchronous-warpgroup-level-matrix-instructions" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p>`,5)),s(n,{type:"info",class:"source-link",text:"source"},{default:l(()=>e[238]||(e[238]=[t("a",{href:"https://github.com/EnzymeAD/Reactant.jl/blob/9dbf8a81dd5743ab847157c74394e53975a2e470/src/mlir/Dialects/Nvvm.jl#L3824-L3882",target:"_blank",rel:"noreferrer"},"source",-1)])),_:1,__:[238]})]),t("details",le,[t("summary",null,[e[240]||(e[240]=t("a",{id:"Reactant.MLIR.Dialects.nvvm.wgmma_wait_group_sync_aligned-Tuple{}",href:"#Reactant.MLIR.Dialects.nvvm.wgmma_wait_group_sync_aligned-Tuple{}"},[t("span",{class:"jlbinding"},"Reactant.MLIR.Dialects.nvvm.wgmma_wait_group_sync_aligned")],-1)),e[241]||(e[241]=a()),s(n,{type:"info",class:"jlObjectType jlMethod",text:"Method"})]),e[243]||(e[243]=t("p",null,[t("code",null,"wgmma_wait_group_sync_aligned")],-1)),e[244]||(e[244]=t("p",null,"Signal the completion of a preceding warpgroup operation.",-1)),e[245]||(e[245]=t("p",null,[t("a",{href:"https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#asynchronous-warpgroup-level-matrix-instructions-wgmma-wait-group",target:"_blank",rel:"noreferrer"},"For more information, see PTX ISA")],-1)),s(n,{type:"info",class:"source-link",text:"source"},{default:l(()=>e[242]||(e[242]=[t("a",{href:"https://github.com/EnzymeAD/Reactant.jl/blob/9dbf8a81dd5743ab847157c74394e53975a2e470/src/mlir/Dialects/Nvvm.jl#L3929-L3935",target:"_blank",rel:"noreferrer"},"source",-1)])),_:1,__:[242]})])])}const Te=d(p,[["render",oe]]);export{ue as __pageData,Te as default};
