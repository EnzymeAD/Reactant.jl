<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>NVVM Dialect | Reactant.jl</title>
    <meta name="description" content="Documentation for Reactant.jl">
    <meta name="generator" content="VitePress v1.6.4">
    <link rel="preload stylesheet" href="/Reactant.jl/previews/PR2506/assets/style.-klmQ5bE.css" as="style">
    <link rel="preload stylesheet" href="/Reactant.jl/previews/PR2506/vp-icons.css" as="style">
    
    <script type="module" src="/Reactant.jl/previews/PR2506/assets/app.Nr-eBkBn.js"></script>
    <link rel="preload" href="/Reactant.jl/previews/PR2506/assets/inter-roman-latin.Di8DUHzh.woff2" as="font" type="font/woff2" crossorigin="">
    <link rel="modulepreload" href="/Reactant.jl/previews/PR2506/assets/chunks/theme.DDVXlgm0.js">
    <link rel="modulepreload" href="/Reactant.jl/previews/PR2506/assets/chunks/framework.BaBFehJr.js">
    <link rel="modulepreload" href="/Reactant.jl/previews/PR2506/assets/api_dialects_nvvm.md.CQQqgKi5.lean.js">
    <link rel="icon" href="/Reactant.jl/previews/PR2506/favicon.ico">
    <script src="/versions.js"></script>
    <script src="/Reactant.jl/previews/PR2506/siteinfo.js"></script>
    <script id="check-dark-mode">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"auto",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
    <script id="check-mac-os">document.documentElement.classList.toggle("mac",/Mac|iPhone|iPod|iPad/i.test(navigator.platform));</script>
  </head>
  <body>
    <div id="app"><div class="Layout" data-v-a9a9e638><!--[--><!--]--><!--[--><span tabindex="-1" data-v-492508fc></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-492508fc>Skip to content</a><!--]--><!----><header class="VPNav" data-v-a9a9e638 data-v-f1e365da><div class="VPNavBar" data-v-f1e365da data-v-822684d1><div class="wrapper" data-v-822684d1><div class="container" data-v-822684d1><div class="title" data-v-822684d1><div class="VPNavBarTitle has-sidebar" data-v-822684d1 data-v-0f4f798b><a class="title" href="/Reactant.jl/previews/PR2506/" data-v-0f4f798b><!--[--><!--]--><!--[--><!--[--><!--[--><img class="VPImage dark logo" src="/Reactant.jl/previews/PR2506/logo.svg" alt data-v-35a7d0b8><!--]--><!--[--><img class="VPImage light logo" src="/Reactant.jl/previews/PR2506/logo.svg" alt data-v-35a7d0b8><!--]--><!--]--><!--]--><span data-v-0f4f798b>Reactant.jl</span><!--[--><!--]--></a></div></div><div class="content" data-v-822684d1><div class="content-body" data-v-822684d1><!--[--><!--]--><div class="VPNavBarSearch search" data-v-822684d1><!--[--><!----><div id="local-search"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><span class="vp-icon DocSearch-Search-Icon"></span><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"><kbd class="DocSearch-Button-Key"></kbd><kbd class="DocSearch-Button-Key">K</kbd></span></button></div><!--]--></div><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-822684d1 data-v-e6d46098><span id="main-nav-aria-label" class="visually-hidden" data-v-e6d46098> Main Navigation </span><!--[--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/Reactant.jl/previews/PR2506/" tabindex="0" data-v-e6d46098 data-v-956ec74c><!--[--><span data-v-956ec74c>Home</span><!--]--></a><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-e6d46098 data-v-04f5c5e9><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-04f5c5e9><span class="text" data-v-04f5c5e9><!----><span data-v-04f5c5e9>Getting Started</span><span class="vpi-chevron-down text-icon" data-v-04f5c5e9></span></span></button><div class="menu" data-v-04f5c5e9><div class="VPMenu" data-v-04f5c5e9 data-v-7dd3104a><div class="items" data-v-7dd3104a><!--[--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2506/introduction" data-v-acbfed09><!--[--><span data-v-acbfed09>Introduction</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2506/introduction/configuration" data-v-acbfed09><!--[--><span data-v-acbfed09>Configuration</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2506/introduction/FAQs" data-v-acbfed09><!--[--><span data-v-acbfed09>FAQs</span><!--]--></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><a class="VPLink link vp-external-link-icon VPNavBarMenuLink" href="https://enzymead.github.io/Reactant.jl/benchmarks/" target="_blank" rel="noreferrer" tabindex="0" data-v-e6d46098 data-v-956ec74c><!--[--><span data-v-956ec74c>Benchmarks</span><!--]--></a><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-e6d46098 data-v-04f5c5e9><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-04f5c5e9><span class="text" data-v-04f5c5e9><!----><span data-v-04f5c5e9>Tutorials</span><span class="vpi-chevron-down text-icon" data-v-04f5c5e9></span></span></button><div class="menu" data-v-04f5c5e9><div class="VPMenu" data-v-04f5c5e9 data-v-7dd3104a><div class="items" data-v-7dd3104a><!--[--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2506/tutorials/" data-v-acbfed09><!--[--><span data-v-acbfed09>Overview</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2506/tutorials/partial-evaluation" data-v-acbfed09><!--[--><span data-v-acbfed09>Partial Evaluation</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2506/tutorials/control-flow" data-v-acbfed09><!--[--><span data-v-acbfed09>Control Flow</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2506/tutorials/automatic-differentiation" data-v-acbfed09><!--[--><span data-v-acbfed09>Automatic Differentiation</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2506/tutorials/sharding" data-v-acbfed09><!--[--><span data-v-acbfed09>Sharding</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2506/tutorials/profiling" data-v-acbfed09><!--[--><span data-v-acbfed09>Profiling</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2506/tutorials/multihost" data-v-acbfed09><!--[--><span data-v-acbfed09>Multi-Host Environments</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2506/tutorials/local-build" data-v-acbfed09><!--[--><span data-v-acbfed09>Local build</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2506/tutorials/persistent_compile_cache" data-v-acbfed09><!--[--><span data-v-acbfed09>Persistent Compilation Cache</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2506/tutorials/raising" data-v-acbfed09><!--[--><span data-v-acbfed09>Raising</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2506/tutorials/kernels" data-v-acbfed09><!--[--><span data-v-acbfed09>Computational kernels</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2506/tutorials/debugging" data-v-acbfed09><!--[--><span data-v-acbfed09>Debugging compilation errors</span><!--]--></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup active" data-v-e6d46098 data-v-04f5c5e9><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-04f5c5e9><span class="text" data-v-04f5c5e9><!----><span data-v-04f5c5e9>API</span><span class="vpi-chevron-down text-icon" data-v-04f5c5e9></span></span></button><div class="menu" data-v-04f5c5e9><div class="VPMenu" data-v-04f5c5e9 data-v-7dd3104a><div class="items" data-v-7dd3104a><!--[--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2506/api/api" data-v-acbfed09><!--[--><span data-v-acbfed09>Core Reactant API</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2506/api/sharding" data-v-acbfed09><!--[--><span data-v-acbfed09>Sharding</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2506/api/serialization" data-v-acbfed09><!--[--><span data-v-acbfed09>Serialization</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2506/api/ops" data-v-acbfed09><!--[--><span data-v-acbfed09>Ops</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2506/api/config" data-v-acbfed09><!--[--><span data-v-acbfed09>Configuration</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuGroup" data-v-7dd3104a data-v-48c802d0><p class="title" data-v-48c802d0>MLIR Dialects</p><!--[--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2506/api/dialects/arith" data-v-acbfed09><!--[--><span data-v-acbfed09>ArithOps</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2506/api/dialects/affine" data-v-acbfed09><!--[--><span data-v-acbfed09>Affine</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2506/api/dialects/builtin" data-v-acbfed09><!--[--><span data-v-acbfed09>Builtin</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2506/api/dialects/chlo" data-v-acbfed09><!--[--><span data-v-acbfed09>Chlo</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2506/api/dialects/complex" data-v-acbfed09><!--[--><span data-v-acbfed09>Complex</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2506/api/dialects/cuda_tile" data-v-acbfed09><!--[--><span data-v-acbfed09>CUDA Tile</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2506/api/dialects/enzyme" data-v-acbfed09><!--[--><span data-v-acbfed09>Enzyme</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2506/api/dialects/enzymexla" data-v-acbfed09><!--[--><span data-v-acbfed09>EnzymeXLA</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2506/api/dialects/func" data-v-acbfed09><!--[--><span data-v-acbfed09>Func</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2506/api/dialects/gpu" data-v-acbfed09><!--[--><span data-v-acbfed09>GPU</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2506/api/dialects/llvm" data-v-acbfed09><!--[--><span data-v-acbfed09>LLVM</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2506/api/dialects/mpi" data-v-acbfed09><!--[--><span data-v-acbfed09>MPI</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2506/api/dialects/memref" data-v-acbfed09><!--[--><span data-v-acbfed09>MemRef</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2506/api/dialects/mosaicgpu" data-v-acbfed09><!--[--><span data-v-acbfed09>Mosaic GPU</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link active" href="/Reactant.jl/previews/PR2506/api/dialects/nvvm" data-v-acbfed09><!--[--><span data-v-acbfed09>NVVM</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2506/api/dialects/shape" data-v-acbfed09><!--[--><span data-v-acbfed09>Shape</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2506/api/dialects/shardy" data-v-acbfed09><!--[--><span data-v-acbfed09>Shardy</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2506/api/dialects/sparsetensor" data-v-acbfed09><!--[--><span data-v-acbfed09>SparseTensor</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2506/api/dialects/stablehlo" data-v-acbfed09><!--[--><span data-v-acbfed09>StableHLO</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2506/api/dialects/tensor" data-v-acbfed09><!--[--><span data-v-acbfed09>Tensor</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2506/api/dialects/triton" data-v-acbfed09><!--[--><span data-v-acbfed09>Triton</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2506/api/dialects/tritonext" data-v-acbfed09><!--[--><span data-v-acbfed09>TritonExt</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2506/api/dialects/tpu" data-v-acbfed09><!--[--><span data-v-acbfed09>TPU</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2506/api/dialects/vhlo" data-v-acbfed09><!--[--><span data-v-acbfed09>VHLO</span><!--]--></a></div><!--]--><!--]--></div><!--]--><!--[--><div class="VPMenuGroup" data-v-7dd3104a data-v-48c802d0><p class="title" data-v-48c802d0>Low-Level API</p><!--[--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2506/api/mlirc" data-v-acbfed09><!--[--><span data-v-acbfed09>MLIR API</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2506/api/xla" data-v-acbfed09><!--[--><span data-v-acbfed09>XLA</span><!--]--></a></div><!--]--><!--]--></div><!--]--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/Reactant.jl/previews/PR2506/api/internal" data-v-acbfed09><!--[--><span data-v-acbfed09>Internal API</span><!--]--></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><!----><!--]--><!--]--></nav><!----><div class="VPNavBarAppearance appearance" data-v-822684d1 data-v-af096f4a><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-af096f4a data-v-e40a8bb6 data-v-4a1c76db><span class="check" data-v-4a1c76db><span class="icon" data-v-4a1c76db><!--[--><span class="vpi-sun sun" data-v-e40a8bb6></span><span class="vpi-moon moon" data-v-e40a8bb6></span><!--]--></span></span></button></div><div class="VPSocialLinks VPNavBarSocialLinks social-links" data-v-822684d1 data-v-164c457f data-v-ee7a9424><!--[--><a class="VPSocialLink no-icon" href="https://julialang.org/slack/" aria-label="slack" target="_blank" rel="noopener" data-v-ee7a9424 data-v-d26d30cb><span class="vpi-social-slack"></span></a><!--]--></div><div class="VPFlyout VPNavBarExtra extra" data-v-822684d1 data-v-925effce data-v-04f5c5e9><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-04f5c5e9><span class="vpi-more-horizontal icon" data-v-04f5c5e9></span></button><div class="menu" data-v-04f5c5e9><div class="VPMenu" data-v-04f5c5e9 data-v-7dd3104a><!----><!--[--><!--[--><!----><div class="group" data-v-925effce><div class="item appearance" data-v-925effce><p class="label" data-v-925effce>Appearance</p><div class="appearance-action" data-v-925effce><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-925effce data-v-e40a8bb6 data-v-4a1c76db><span class="check" data-v-4a1c76db><span class="icon" data-v-4a1c76db><!--[--><span class="vpi-sun sun" data-v-e40a8bb6></span><span class="vpi-moon moon" data-v-e40a8bb6></span><!--]--></span></span></button></div></div></div><div class="group" data-v-925effce><div class="item social-links" data-v-925effce><div class="VPSocialLinks social-links-list" data-v-925effce data-v-ee7a9424><!--[--><a class="VPSocialLink no-icon" href="https://julialang.org/slack/" aria-label="slack" target="_blank" rel="noopener" data-v-ee7a9424 data-v-d26d30cb><span class="vpi-social-slack"></span></a><!--]--></div></div></div><!--]--><!--]--></div></div></div><!--[--><!--[--><!--[--><!--[--><a target="_blank" data-decoration="★" title="301 GitHub stars" href="https://github.com/EnzymeAD/Reactant.jl" data-v-b4d08338><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="20" height="20" fill="currentColor" style="vertical-align:middle;margin-right:0.25rem;margin-left:0.5rem;" data-v-b4d08338><path d="M12 .297C5.375.297 0 5.673 0 12.3c0 5.292 3.438 9.8 8.207 11.387.6.11.793-.26.793-.577 0-.285-.01-1.04-.015-2.04-3.338.727-4.042-1.61-4.042-1.61-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.807 1.305 3.493.997.107-.774.42-1.305.762-1.605-2.665-.3-5.467-1.333-5.467-5.931 0-1.31.47-2.382 1.236-3.222-.123-.303-.535-1.52.117-3.166 0 0 1.01-.323 3.31 1.23.96-.267 1.98-.4 3-.405 1.02.005 2.04.138 3 .405 2.3-1.553 3.31-1.23 3.31-1.23.653 1.646.24 2.863.117 3.166.765.84 1.236 1.912 1.236 3.222 0 4.61-2.807 5.625-5.477 5.921.43.372.823 1.102.823 2.222 0 1.606-.015 2.902-.015 3.293 0 .32.192.693.8.577C20.565 22.1 24 17.588 24 12.297 24 5.673 18.627.297 12 .297z" data-v-b4d08338></path></svg><span data-v-b4d08338>0.3k</span></a><a class="mobile" target="_blank" title="301 GitHub stars" href="https://github.com/EnzymeAD/Reactant.jl" data-v-b4d08338><svg xmlns="http://www.w3.org/2000/svg" width="21" height="21" viewBox="0 0 21 21" fill="none" data-v-b4d08338><path d="M19.625 5.60534C18.7083 4.03477 17.4649 2.79135 15.8945 1.87479C14.3238 0.958185 12.6091 0.5 10.7492 0.5C8.88947 0.5 7.17422 0.958325 5.60388 1.87479C4.0333 2.7913 2.78997 4.03477 1.87332 5.60534C0.956814 7.17587 0.498535 8.89089 0.498535 10.7504C0.498535 12.984 1.15021 14.9926 2.4539 16.7766C3.75744 18.5607 5.44142 19.7952 7.50571 20.4803C7.746 20.5249 7.92388 20.4936 8.03954 20.387C8.15524 20.2804 8.21302 20.1467 8.21302 19.9868C8.21302 19.9601 8.21073 19.7199 8.20629 19.266C8.20171 18.8122 8.19956 18.4162 8.19956 18.0783L7.89256 18.1315C7.69682 18.1673 7.44989 18.1825 7.15178 18.1782C6.8538 18.174 6.54446 18.1428 6.22419 18.0847C5.90377 18.0272 5.60575 17.8937 5.32988 17.6846C5.05416 17.4755 4.85842 17.2018 4.74272 16.8639L4.60925 16.5568C4.52029 16.3523 4.38023 16.1251 4.18888 15.8761C3.99754 15.6269 3.80405 15.458 3.60831 15.369L3.51486 15.3021C3.45259 15.2577 3.39481 15.204 3.34138 15.1418C3.28799 15.0796 3.24802 15.0173 3.22132 14.955C3.19458 14.8926 3.21674 14.8414 3.28804 14.8012C3.35933 14.761 3.48817 14.7416 3.67512 14.7416L3.94196 14.7814C4.11993 14.8171 4.34007 14.9236 4.60266 15.1017C4.86511 15.2796 5.08085 15.5109 5.24994 15.7956C5.4547 16.1605 5.7014 16.4385 5.99072 16.6299C6.27982 16.8212 6.5713 16.9167 6.86488 16.9167C7.15846 16.9167 7.41203 16.8945 7.62567 16.8502C7.83908 16.8057 8.0393 16.7388 8.22625 16.6499C8.30633 16.0535 8.52437 15.5953 8.88017 15.275C8.37304 15.2217 7.9171 15.1414 7.51212 15.0347C7.10736 14.9278 6.6891 14.7544 6.25761 14.5139C5.82589 14.2738 5.46774 13.9756 5.18309 13.6198C4.89839 13.2639 4.66474 12.7966 4.48247 12.2183C4.3001 11.6399 4.20889 10.9726 4.20889 10.2163C4.20889 9.13941 4.56044 8.22304 5.26341 7.46665C4.93411 6.65705 4.96519 5.74947 5.35676 4.744C5.61482 4.66382 5.9975 4.72399 6.50463 4.92412C7.01186 5.12434 7.38323 5.29587 7.61912 5.43808C7.85502 5.58024 8.04402 5.70071 8.18642 5.79842C9.01411 5.56715 9.86825 5.45149 10.7491 5.45149C11.6299 5.45149 12.4843 5.56715 13.312 5.79842L13.8192 5.47823C14.166 5.26459 14.5756 5.06881 15.0469 4.89083C15.5185 4.71295 15.8791 4.66396 16.1284 4.74414C16.5286 5.74966 16.5643 6.65719 16.2349 7.46679C16.9378 8.22318 17.2895 9.13978 17.2895 10.2164C17.2895 10.9727 17.198 11.6421 17.0159 12.225C16.8336 12.808 16.5979 13.2749 16.3088 13.6265C16.0194 13.9781 15.659 14.274 15.2275 14.5141C14.7959 14.7544 14.3775 14.9278 13.9728 15.0347C13.5678 15.1415 13.1119 15.2219 12.6047 15.2752C13.0673 15.6755 13.2986 16.3073 13.2986 17.1704V19.9864C13.2986 20.1464 13.3542 20.2799 13.4656 20.3867C13.5768 20.4932 13.7524 20.5246 13.9927 20.4799C16.0573 19.7949 17.7413 18.5603 19.0448 16.7762C20.3481 14.9922 21 12.9837 21 10.75C20.9996 8.89075 20.541 7.17587 19.625 5.60534Z" fill="currentColor" data-v-b4d08338></path></svg></a><!--]--><div class="VPFlyout VPNolebaseEnhancedReadabilitiesMenu VPNolebaseEnhancedReadabilitiesMenuFlyout" aria-label="Enhanced Readability" role="menuitem" data-v-04f5c5e9><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-04f5c5e9><span class="text" data-v-04f5c5e9><span class="i-icon-park-outline:book-open option-icon" data-v-04f5c5e9></span><!----><span class="vpi-chevron-down text-icon" data-v-04f5c5e9></span></span></button><div class="menu" data-v-04f5c5e9><div class="VPMenu" data-v-04f5c5e9 data-v-7dd3104a><!----><!--[--><!--]--></div></div></div><!--]--><!--]--><!--]--><button type="button" class="VPNavBarHamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-822684d1 data-v-5dea55bf><span class="container" data-v-5dea55bf><span class="top" data-v-5dea55bf></span><span class="middle" data-v-5dea55bf></span><span class="bottom" data-v-5dea55bf></span></span></button></div></div></div></div><div class="divider" data-v-822684d1><div class="divider-line" data-v-822684d1></div></div></div><!----></header><div class="VPLocalNav has-sidebar empty" data-v-a9a9e638 data-v-070ab83d><div class="container" data-v-070ab83d><button class="menu" aria-expanded="false" aria-controls="VPSidebarNav" data-v-070ab83d><span class="vpi-align-left menu-icon" data-v-070ab83d></span><span class="menu-text" data-v-070ab83d>Menu</span></button><div class="VPLocalNavOutlineDropdown" style="--vp-vh:0px;" data-v-070ab83d data-v-168ddf5d><button data-v-168ddf5d>Return to top</button><!----></div></div></div><aside class="VPSidebar" data-v-a9a9e638 data-v-18756405><div class="curtain" data-v-18756405></div><nav class="nav" id="VPSidebarNav" aria-labelledby="sidebar-aria-label" tabindex="-1" data-v-18756405><span class="visually-hidden" id="sidebar-aria-label" data-v-18756405> Sidebar Navigation </span><!--[--><!--]--><!--[--><div class="no-transition group" data-v-9e426adc><section class="VPSidebarItem level-0 collapsible has-active" data-v-9e426adc data-v-a4b0d9bf><div class="item" role="button" tabindex="0" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><h2 class="text" data-v-a4b0d9bf>API Reference</h2><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-a4b0d9bf><span class="vpi-chevron-right caret-icon" data-v-a4b0d9bf></span></div></div><div class="items" data-v-a4b0d9bf><!--[--><div class="VPSidebarItem level-1 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR2506/api/api" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Reactant API</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR2506/api/sharding" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Sharding</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR2506/api/serialization" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Serialization</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR2506/api/ops" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Ops</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR2506/api/config" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Configuration</p><!--]--></a><!----></div><!----></div><section class="VPSidebarItem level-1 collapsible has-active" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" role="button" tabindex="0" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><h3 class="text" data-v-a4b0d9bf>MLIR Dialects</h3><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-a4b0d9bf><span class="vpi-chevron-right caret-icon" data-v-a4b0d9bf></span></div></div><div class="items" data-v-a4b0d9bf><!--[--><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR2506/api/dialects/arith" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>ArithOps</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR2506/api/dialects/affine" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Affine</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR2506/api/dialects/builtin" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Builtin</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR2506/api/dialects/chlo" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Chlo</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR2506/api/dialects/complex" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Complex</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR2506/api/dialects/cuda_tile" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>CUDA Tile</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR2506/api/dialects/enzyme" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Enzyme</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR2506/api/dialects/enzymexla" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>EnzymeXLA</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR2506/api/dialects/func" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Func</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR2506/api/dialects/gpu" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>GPU</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR2506/api/dialects/llvm" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>LLVM</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR2506/api/dialects/mpi" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>MPI</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR2506/api/dialects/memref" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>MemRef</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR2506/api/dialects/mosaicgpu" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Mosaic GPU</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR2506/api/dialects/nvvm" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>NVVM</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR2506/api/dialects/shape" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Shape</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR2506/api/dialects/shardy" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Shardy</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR2506/api/dialects/sparsetensor" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>SparseTensor</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR2506/api/dialects/stablehlo" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>StableHLO</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR2506/api/dialects/tensor" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Tensor</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR2506/api/dialects/triton" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Triton</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR2506/api/dialects/tritonext" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>TritonExt</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR2506/api/dialects/tpu" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>TPU</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR2506/api/dialects/vhlo" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>VHLO</p><!--]--></a><!----></div><!----></div><!--]--></div></section><section class="VPSidebarItem level-1 collapsible" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" role="button" tabindex="0" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><h3 class="text" data-v-a4b0d9bf>Low-Level API</h3><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-a4b0d9bf><span class="vpi-chevron-right caret-icon" data-v-a4b0d9bf></span></div></div><div class="items" data-v-a4b0d9bf><!--[--><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR2506/api/mlirc" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>MLIR API</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR2506/api/xla" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>XLA</p><!--]--></a><!----></div><!----></div><!--]--></div></section><div class="VPSidebarItem level-1 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Reactant.jl/previews/PR2506/api/internal" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Internal API</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><!--]--><!--[--><!--]--></nav></aside><div class="VPContent has-sidebar" id="VPContent" data-v-a9a9e638 data-v-91765379><div class="VPDoc has-sidebar has-aside" data-v-91765379 data-v-83890dd9><!--[--><!--]--><div class="container" data-v-83890dd9><div class="aside" data-v-83890dd9><div class="aside-curtain" data-v-83890dd9></div><div class="aside-container" data-v-83890dd9><div class="aside-content" data-v-83890dd9><div class="VPDocAside" data-v-83890dd9 data-v-6d7b3c46><!--[--><!--]--><!--[--><!--]--><nav aria-labelledby="doc-outline-aria-label" class="VPDocAsideOutline" data-v-6d7b3c46 data-v-b38bf2ff><div class="content" data-v-b38bf2ff><div class="outline-marker" data-v-b38bf2ff></div><div aria-level="2" class="outline-title" id="doc-outline-aria-label" role="heading" data-v-b38bf2ff>On this page</div><ul class="VPDocOutlineItem root" data-v-b38bf2ff data-v-3f927ebe><!--[--><!--]--></ul></div></nav><!--[--><!--]--><div class="spacer" data-v-6d7b3c46></div><!--[--><!--[--><!--[--><!--[--><!--[--><br><h2> Trusted by </h2><a class="enjoyer" href="https://lux.csail.mit.edu/" target="_blank"><img width="32" height="32" src="https://raw.githubusercontent.com/LuxDL/Lux.jl/refs/heads/main/assets/lux-logo.svg"><span><p class="extra-info">Scientific Computing</p><p class="heading">Lux.jl</p><p class="extra-info">Machine Learning</p></span></a><!--]--><!--]--><!--]--><!--]--><!--]--><!----><!--[--><!--]--><!--[--><!--]--></div></div></div></div><div class="content" data-v-83890dd9><div class="content-container" data-v-83890dd9><!--[--><!--]--><main class="main" data-v-83890dd9><div style="position:relative;" class="vp-doc _Reactant_jl_previews_PR2506_api_dialects_nvvm" data-v-83890dd9><div><h1 id="NVVM-Dialect" tabindex="-1">NVVM Dialect <a class="header-anchor" href="#NVVM-Dialect" aria-label="Permalink to &quot;NVVM Dialect {#NVVM-Dialect}&quot;">​</a></h1><p>Refer to the <a href="https://mlir.llvm.org/docs/Dialects/NVVMDialect/" target="_blank" rel="noreferrer">official documentation</a> for more details.</p><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.bar_warp_sync-Tuple{Reactant.MLIR.IR.Value}" href="#Reactant.MLIR.Dialects.nvvm.bar_warp_sync-Tuple{Reactant.MLIR.IR.Value}"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.bar_warp_sync</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>bar_warp_sync</code></p><p>The <code>nvvm.bar.warp.sync</code> operation performs barrier synchronization for threads within a warp.</p><p>This operation causes the executing thread to wait until all threads corresponding to the <code>mask</code> operand have executed a <code>bar.warp.sync</code> with the same mask value before resuming execution.</p><p>The <code>mask</code> operand specifies the threads participating in the barrier, where each bit position corresponds to the thread&#39;s lane ID within the warp. Only threads with their corresponding bit set in the mask participate in the barrier synchronization.</p><p><strong>Important constraints</strong>:</p><ul><li><p>The behavior is undefined if the executing thread is not included in the mask (i.e., the bit corresponding to the thread&#39;s lane ID is not set)</p></li><li><p>For compute capability sm_6x or below, all threads in the mask must execute the same <code>bar.warp.sync</code> instruction in convergence</p></li></ul><p>This operation also guarantees memory ordering among participating threads. Threads within the warp that wish to communicate via memory can store to memory, execute <code>bar.warp.sync</code>, and then safely read values stored by other threads in the warp.</p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#parallel-synchronization-and-communication-instructions-bar-warp-sync" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L4898-L4924" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.barrier" href="#Reactant.MLIR.Dialects.nvvm.barrier"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.barrier</span></a> <span class="VPBadge info jlObjectType jlFunction"><!--[-->Function<!--]--></span></summary><p><code>barrier</code></p><p>The <code>nvvm.barrier</code> operation performs barrier synchronization and communication within a CTA (Cooperative Thread Array). It causes executing threads to wait for all non-exited threads participating in the barrier to arrive.</p><p>The operation takes two optional operands:</p><ul><li><p><code>barrierId</code>: Specifies a logical barrier resource with value 0 through 15. Each CTA instance has sixteen barriers numbered 0..15. Defaults to 0 if not specified.</p></li><li><p><code>numberOfThreads</code>: Specifies the number of threads participating in the barrier. When specified, the value must be a multiple of the warp size. If not specified, all threads in the CTA participate in the barrier.</p></li><li><p><code>reductionOp</code>: specifies the reduction operation (<code>popc</code>, <code>and</code>, <code>or</code>).</p></li><li><p><code>reductionPredicate</code>: specifies the predicate to be used with the <code>reductionOp</code>.</p></li></ul><p>The barrier operation guarantees that when the barrier completes, prior memory accesses requested by participating threads are performed relative to all threads participating in the barrier. It also ensures that no new memory access is requested by participating threads before the barrier completes.</p><p>When a barrier completes, the waiting threads are restarted without delay, and the barrier is reinitialized so that it can be immediately reused.</p><p>This operation generates an aligned barrier, indicating that all threads in the CTA will execute the same barrier instruction. Behavior is undefined if all threads in the CTA do not reach this instruction.</p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#parallel-synchronization-and-communication-instructions-bar" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L96-L127" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.barrier0-Tuple{}" href="#Reactant.MLIR.Dialects.nvvm.barrier0-Tuple{}"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.barrier0</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>barrier0</code></p><p>The <code>nvvm.barrier0</code> operation is a convenience operation that performs barrier synchronization and communication within a CTA (Cooperative Thread Array) using barrier ID 0. It is functionally equivalent to <code>nvvm.barrier</code> or <code>nvvm.barrier id=0</code>.</p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#parallel-synchronization-and-communication-instructions-bar" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L35-L43" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.barrier_arrive" href="#Reactant.MLIR.Dialects.nvvm.barrier_arrive"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.barrier_arrive</span></a> <span class="VPBadge info jlObjectType jlFunction"><!--[-->Function<!--]--></span></summary><p><code>barrier_arrive</code></p><p>Thread that executes this op announces their arrival at the barrier with given id and continue their execution.</p><p>The default barrier id is 0 that is similar to <code>nvvm.barrier</code> Op. When <code>barrierId</code> is not present, the default barrier id is used.</p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#parallel-synchronization-and-communication-instructions-bar" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L63-L73" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.breakpoint-Tuple{}" href="#Reactant.MLIR.Dialects.nvvm.breakpoint-Tuple{}"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.breakpoint</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>breakpoint</code></p><p>Breakpoint suspends execution of the program for debugging. <a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#miscellaneous-instructions-brkpt" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L347-L352" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.cluster_arrive-Tuple{}" href="#Reactant.MLIR.Dialects.nvvm.cluster_arrive-Tuple{}"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.cluster_arrive</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>cluster_arrive</code></p><p>The <code>cluster.arrive</code> can be used by the threads within the cluster for synchronization and communication. The <code>cluster.arrive</code> instruction marks the warps&#39; arrival at the barrier without causing the executing thread to wait for other participating threads.</p><p>The <code>aligned</code> attribute, when provided, generates the .aligned version of the PTX instruction.</p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#parallel-synchronization-and-communication-instructions-barrier-cluster" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L441-L451" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.cluster_arrive_relaxed-Tuple{}" href="#Reactant.MLIR.Dialects.nvvm.cluster_arrive_relaxed-Tuple{}"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.cluster_arrive_relaxed</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>cluster_arrive_relaxed</code></p><p>The <code>cluster.arrive</code> can be used by the threads within the cluster for synchronization and communication. The <code>cluster.arrive</code> instruction marks the warps&#39; arrival at the barrier without causing the executing thread to wait for other participating threads.</p><p>The <code>aligned</code> attribute, when provided, generates the .aligned version of the PTX instruction. The .relaxed qualifier on <code>cluster.arrive</code> specifies that there are no memory ordering and visibility guarantees provided for the memory accesses performed prior to <code>cluster.arrive</code>.</p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#parallel-synchronization-and-communication-instructions-barrier-cluster" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L472-L485" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.cluster_wait-Tuple{}" href="#Reactant.MLIR.Dialects.nvvm.cluster_wait-Tuple{}"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.cluster_wait</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>cluster_wait</code></p><p>The <code>cluster.wait</code> causes the executing thread to wait for all non-exited threads of the cluster to perform <code>cluster.arrive</code>. The <code>aligned</code> attribute, when provided, generates the .aligned version of the PTX instruction.</p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#parallel-synchronization-and-communication-instructions-barrier-cluster" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L806-L814" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.clusterlaunchcontrol_query_cancel-Tuple{Reactant.MLIR.IR.Value}" href="#Reactant.MLIR.Dialects.nvvm.clusterlaunchcontrol_query_cancel-Tuple{Reactant.MLIR.IR.Value}"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.clusterlaunchcontrol_query_cancel</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>clusterlaunchcontrol_query_cancel</code></p><p><code>clusterlaunchcontrol.query.cancel</code> queries the response of a <code>clusterlaunchcontrol.try.cancel</code> operation specified by operand <code>try_cancel_response</code>.</p><p>Operand <code>query_type</code> specifies the type of query to perform and can be one of the following:</p><ul><li><code>is_canceled</code> : Returns true if the try cancel request succeeded,</li></ul><p>and false otherwise.</p><ul><li><code>get_first_cta_id_{x/y/z}</code> : Returns the x, y, or z coordinate of the</li></ul><p>first CTA in the canceled cluster. Behaviour is defined only if the try cancel request succeeded.</p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#parallel-synchronization-and-communication-instructions-clusterlaunchcontrol-query-cancel" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L726-L742" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.clusterlaunchcontrol_try_cancel-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}" href="#Reactant.MLIR.Dialects.nvvm.clusterlaunchcontrol_try_cancel-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.clusterlaunchcontrol_try_cancel</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>clusterlaunchcontrol_try_cancel</code></p><p><code>clusterlaunchcontrol.try.cancel</code> requests atomically canceling the launch of a cluster that has not started running yet. It asynchronously writes an opaque response to shared memory indicating whether the operation succeeded or failed.</p><p>Operand <code>smemAddress</code> specifies the naturally aligned address of the 16-byte wide shared memory location where the request&#39;s response is written.</p><p>Operand <code>mbarrier</code> specifies the mbarrier object used to track the completion of the asynchronous operation.</p><p>If <code>multicast</code> is specified, the response is asynchronously written to the corresponding local shared memory location (specifed by <code>addr</code>) of each CTA in the requesting cluster.</p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#parallel-synchronization-and-communication-instructions-clusterlaunchcontrol-try-cancel" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L764-L783" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.convert_bf16x2_to_f8x2-Tuple{Reactant.MLIR.IR.Value}" href="#Reactant.MLIR.Dialects.nvvm.convert_bf16x2_to_f8x2-Tuple{Reactant.MLIR.IR.Value}"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.convert_bf16x2_to_f8x2</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>convert_bf16x2_to_f8x2</code></p><p>This Op converts the given bf16 inputs in a bf16x2 vector to the specified f8 type. The result <code>dst</code> is represented as an i16 type or as a vector of two i8 types. If <code>dst</code> is returned as an i16 type, the converted values from <code>a</code> are packed such that the value converted from the first element of <code>a</code> is stored in the upper 8 bits of <code>dst</code> and the value converted from the second element of <code>a</code> is stored in the lower 8 bits of <code>dst</code>. If <code>dst</code> is returned as a vector type, each converted value is stored as an i8 element in the vector. The <code>rnd</code> and <code>sat</code> attributes specify the rounding and saturation modes respectively.</p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cvt" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L835-L852" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.convert_f16x2_to_f8x2-Tuple{Reactant.MLIR.IR.Value}" href="#Reactant.MLIR.Dialects.nvvm.convert_f16x2_to_f8x2-Tuple{Reactant.MLIR.IR.Value}"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.convert_f16x2_to_f8x2</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>convert_f16x2_to_f8x2</code></p><p>This Op converts the given f16 inputs in an f16x2 vector to the specified f8 type. The result <code>dst</code> is represented as an i16 type or as a vector of two i8 types. If <code>dst</code> is returned as an i16 type, the converted values from <code>a</code> are packed such that the value converted from the first element of <code>a</code> is stored in the upper 8 bits of <code>dst</code> and the value converted from the second element of <code>a</code> is stored in the lower 8 bits of <code>dst</code>. If <code>dst</code> is returned as a vector type, each converted value is stored as an i8 element in the vector. The <code>relu</code> attribute, when set, lowers to the &#39;.relu&#39; variant of the cvt instruction.</p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cvt" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L1004-L1021" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.convert_f32x2_to_bf16x2" href="#Reactant.MLIR.Dialects.nvvm.convert_f32x2_to_bf16x2"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.convert_f32x2_to_bf16x2</span></a> <span class="VPBadge info jlObjectType jlFunction"><!--[-->Function<!--]--></span></summary><p><code>convert_f32x2_to_bf16x2</code></p><p>Converts two F32 values to packed bf16x2 format with the specified rounding mode. The <code>src_hi</code> and <code>src_lo</code> parameters correspond to operands <code>a</code> and <code>b</code> in the PTX ISA, respectively.</p><p>The <code>random_bits</code> parameter is required for stochastic rounding and provides the <a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#cvt-rs-rbits-layout-bf16" target="_blank" rel="noreferrer">random bits</a> to be used for the conversion.</p><p>The <code>relu</code> attribute clamps negative results to 0.</p><p>The <code>sat</code> attribute determines saturation behavior.</p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cvt" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L1044-L1059" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.convert_f32x2_to_f16x2" href="#Reactant.MLIR.Dialects.nvvm.convert_f32x2_to_f16x2"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.convert_f32x2_to_f16x2</span></a> <span class="VPBadge info jlObjectType jlFunction"><!--[-->Function<!--]--></span></summary><p><code>convert_f32x2_to_f16x2</code></p><p>Converts two F32 values to packed f16x2 format with the specified rounding mode. The <code>src_hi</code> and <code>src_lo</code> parameters correspond to operands <code>a</code> and <code>b</code> in the PTX ISA, respectively.</p><p>The <code>random_bits</code> parameter is required for stochastic rounding and provides the <a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#cvt-rs-rbits-layout-f16" target="_blank" rel="noreferrer">random bits</a> to be used for the conversion.</p><p>The <code>relu</code> attribute clamps negative results to 0.</p><p>The <code>sat</code> attribute determines saturation behavior.</p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cvt" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L1214-L1229" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.convert_f32x2_to_f4x2-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}" href="#Reactant.MLIR.Dialects.nvvm.convert_f32x2_to_f4x2-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.convert_f32x2_to_f4x2</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>convert_f32x2_to_f4x2</code></p><p>This Op converts each of the given float inputs to the specified fp4 type. The result <code>dst</code> is returned as an i8 type where the converted values are packed such that the value converted from <code>a</code> is stored in the upper 4 bits of <code>dst</code> and the value converted from <code>b</code> is stored in the lower 4 bits of <code>dst</code>. The <code>relu</code> attribute, when set, lowers to the &#39;.relu&#39; variant of the cvt instruction.</p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cvt" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L1092-L1104" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.convert_f32x2_to_f6x2-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}" href="#Reactant.MLIR.Dialects.nvvm.convert_f32x2_to_f6x2-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.convert_f32x2_to_f6x2</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>convert_f32x2_to_f6x2</code></p><p>This Op converts each of the given float inputs to the specified fp6 type. The result <code>dst</code> is represented either as an i16 type or as a vector of two i8 types. If <code>dst</code> is returned as an i16 type, the converted values are packed such that the value converted from <code>a</code> is stored in the upper 8 bits of <code>dst</code> with 2 MSB bits padded with zeros and the value converted from <code>b</code> is stored in the lower 8 bits of <code>dst</code> with 2 MSB bits padded with zeros. If <code>dst</code> is returned as a vector type, each converted value is stored as an i8 element in the vector. The <code>relu</code> attribute, when set, lowers to the &#39;.relu&#39; variant of the cvt instruction.</p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cvt" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L1127-L1143" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.convert_f32x2_to_f8x2-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}" href="#Reactant.MLIR.Dialects.nvvm.convert_f32x2_to_f8x2-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.convert_f32x2_to_f8x2</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>convert_f32x2_to_f8x2</code></p><p>This Op converts each of the given float inputs to the specified fp8 type. The result <code>dst</code> is represented as an i16 type or as a vector of two i8 types. If <code>dst</code> is returned as an i16 type, the converted values are packed such that the value converted from <code>a</code> is stored in the upper 8 bits of <code>dst</code> and the value converted from <code>b</code> is stored in the lower 8 bits of <code>dst</code>. If <code>dst</code> is returned as a vector type, each converted value is stored as an i8 element in the vector. The <code>rnd</code> and <code>sat</code> attributes specify the rounding and saturation modes respectively. The <code>relu</code> attribute, when set, lowers to the &#39;.relu&#39; variant of the cvt instruction.</p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cvt" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L1166-L1182" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.convert_f32x4_to_f4x4-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}" href="#Reactant.MLIR.Dialects.nvvm.convert_f32x4_to_f4x4-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.convert_f32x4_to_f4x4</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>convert_f32x4_to_f4x4</code></p><p>Converts a vector&lt;4xf32&gt; to packed f4x4 format using stochastic rounding (.rs) mode with SATFINITE saturation. Randomness is provided by the <code>rbits</code> parameter. The <code>dstTy</code> attribute specifies the target floating-point format. The <code>relu</code> attribute clamps negative results to 0.</p><p>Note: These operations always use RS rounding mode and SATFINITE saturation mode.</p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cvt" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L1262-L1273" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.convert_f32x4_to_f6x4-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}" href="#Reactant.MLIR.Dialects.nvvm.convert_f32x4_to_f6x4-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.convert_f32x4_to_f6x4</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>convert_f32x4_to_f6x4</code></p><p>Converts a vector&lt;4xf32&gt; to packed f6x4 format using stochastic rounding (.rs) mode with SATFINITE saturation. Randomness is provided by the <code>rbits</code> parameter. The <code>dstTy</code> attribute specifies the target floating-point format. The <code>relu</code> attribute clamps negative results to 0.</p><p>Note: These operations always use RS rounding mode and SATFINITE saturation mode.</p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cvt" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L1296-L1307" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.convert_f32x4_to_f8x4-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}" href="#Reactant.MLIR.Dialects.nvvm.convert_f32x4_to_f8x4-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.convert_f32x4_to_f8x4</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>convert_f32x4_to_f8x4</code></p><p>Converts a vector&lt;4xf32&gt; to packed f8x4 format using stochastic rounding (.rs) mode with SATFINITE saturation. Randomness is provided by the <code>rbits</code> parameter. The <code>dstTy</code> attribute specifies the target floating-point format. The <code>relu</code> attribute clamps negative results to 0.</p><p>Note: These operations always use RS rounding mode and SATFINITE saturation mode.</p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cvt" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L1330-L1341" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.convert_f4x2_to_f16x2-Tuple{Reactant.MLIR.IR.Value}" href="#Reactant.MLIR.Dialects.nvvm.convert_f4x2_to_f16x2-Tuple{Reactant.MLIR.IR.Value}"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.convert_f4x2_to_f16x2</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>convert_f4x2_to_f16x2</code></p><p>This Op converts the given f4 inputs in a packed i8 to f16.</p><p>The result <code>dst</code> is represented as a vector of f16 elements. The <code>relu</code> attribute, when set, lowers to the &#39;.relu&#39; variant of the cvt instruction.&quot;</p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cvt" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L876-L886" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.convert_f6x2_to_f16x2-Tuple{Reactant.MLIR.IR.Value}" href="#Reactant.MLIR.Dialects.nvvm.convert_f6x2_to_f16x2-Tuple{Reactant.MLIR.IR.Value}"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.convert_f6x2_to_f16x2</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>convert_f6x2_to_f16x2</code></p><p>This Op converts the given f6 inputs in a i8x2 vector to f16.</p><p>The result <code>dst</code> is represented as a vector of f16 elements. The <code>relu</code> attribute, when set, lowers to the &#39;.relu&#39; variant of the cvt instruction.&quot;</p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cvt" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L909-L919" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.convert_f8x2_to_bf16x2-Tuple{Reactant.MLIR.IR.Value}" href="#Reactant.MLIR.Dialects.nvvm.convert_f8x2_to_bf16x2-Tuple{Reactant.MLIR.IR.Value}"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.convert_f8x2_to_bf16x2</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>convert_f8x2_to_bf16x2</code></p><p>This Op converts the given f8 inputs in a i8x2 vector to bf16.</p><p>The result <code>dst</code> is represented as a vector of bf16 elements.</p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cvt" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L942-L951" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.convert_f8x2_to_f16x2-Tuple{Reactant.MLIR.IR.Value}" href="#Reactant.MLIR.Dialects.nvvm.convert_f8x2_to_f16x2-Tuple{Reactant.MLIR.IR.Value}"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.convert_f8x2_to_f16x2</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>convert_f8x2_to_f16x2</code></p><p>This Op converts the given f8 inputs in a i8x2 vector to f16.</p><p>The result <code>dst</code> is represented as a vector of f16 elements. The <code>relu</code> attribute, when set, lowers to the &#39;.relu&#39; variant of the cvt instruction.&quot;</p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cvt" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L971-L981" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.convert_float_to_tf32-Tuple{Reactant.MLIR.IR.Value}" href="#Reactant.MLIR.Dialects.nvvm.convert_float_to_tf32-Tuple{Reactant.MLIR.IR.Value}"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.convert_float_to_tf32</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>convert_float_to_tf32</code></p><p>This Op converts the given f32 input to tf32. The result <code>res</code> is represented as an i32 type. The <code>relu</code> attribute, when set, lowers to the &#39;.relu&#39; variant of the cvt instruction. The <code>rnd</code> and <code>sat</code> attributes specify the the rounding and saturation modes respectively.</p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cvt" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L1364-L1374" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.cp_async_bulk_commit_group-Tuple{}" href="#Reactant.MLIR.Dialects.nvvm.cp_async_bulk_commit_group-Tuple{}"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.cp_async_bulk_commit_group</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>cp_async_bulk_commit_group</code></p><p>This Op commits all prior initiated but uncommitted cp.async.bulk instructions into a cp.async.bulk-group.</p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cp-async-bulk-commit-group" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L1399-L1406" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.cp_async_bulk_global_shared_cta" href="#Reactant.MLIR.Dialects.nvvm.cp_async_bulk_global_shared_cta"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.cp_async_bulk_global_shared_cta</span></a> <span class="VPBadge info jlObjectType jlFunction"><!--[-->Function<!--]--></span></summary><p><code>cp_async_bulk_global_shared_cta</code></p><p>Initiates an asynchronous copy operation from Shared CTA memory to global memory. The 32-bit operand <code>size</code> specifies the amount of memory to be copied, in terms of number of bytes. <code>size</code> must be a multiple of 16. The <code>l2CacheHint</code> operand is optional, and it is used to specify cache eviction policy that may be used during the memory access. The <code>byteMask</code> operand is optional. The i-th bit in the 16-bit wide <code>byteMask</code> specifies whether the i-th byte of each 16-byte wide chunk of source data is copied to the destination. If the bit is set, the byte is copied.</p><p><strong>Example</strong></p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>  nvvm.cp.async.bulk.global.shared.cta %dst, %src, %size</span></span>
<span class="line"><span>      : !llvm.ptr&lt;1&gt;, !llvm.ptr&lt;3&gt;</span></span>
<span class="line"><span></span></span>
<span class="line"><span>  // with l2_cache_hint</span></span>
<span class="line"><span>  nvvm.cp.async.bulk.global.shared.cta %dst, %src, %size l2_cache_hint = %ch</span></span>
<span class="line"><span>      : !llvm.ptr&lt;1&gt;, !llvm.ptr&lt;3&gt;</span></span>
<span class="line"><span></span></span>
<span class="line"><span>  // with byte_mask</span></span>
<span class="line"><span>  nvvm.cp.async.bulk.global.shared.cta %dst, %src, %size byte_mask = %mask</span></span>
<span class="line"><span>      : !llvm.ptr&lt;1&gt;, !llvm.ptr&lt;3&gt;</span></span>
<span class="line"><span></span></span>
<span class="line"><span>  // with both l2_cache_hint and byte_mask</span></span>
<span class="line"><span>  nvvm.cp.async.bulk.global.shared.cta %dst, %src, %size l2_cache_hint = %ch byte_mask = %mask</span></span>
<span class="line"><span>      : !llvm.ptr&lt;1&gt;, !llvm.ptr&lt;3&gt;</span></span></code></pre></div><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cp-async-bulk" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L1523-L1555" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.cp_async_bulk_prefetch" href="#Reactant.MLIR.Dialects.nvvm.cp_async_bulk_prefetch"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.cp_async_bulk_prefetch</span></a> <span class="VPBadge info jlObjectType jlFunction"><!--[-->Function<!--]--></span></summary><p><code>cp_async_bulk_prefetch</code></p><p>Initiates an asynchronous prefetch of data from the location specified by <code>srcMem</code> to the L2 cache.</p><p>The <code>l2CacheHint</code> operand is optional, and it is used to specify cache eviction policy that may be used during the memory access.</p><p><strong>Example</strong></p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>  nvvm.cp.async.bulk.prefetch %src, %size : !llvm.ptr&lt;1&gt;</span></span>
<span class="line"><span></span></span>
<span class="line"><span>  // with l2_cache_hint</span></span>
<span class="line"><span>  nvvm.cp.async.bulk.prefetch %src, %size l2_cache_hint = %ch : !llvm.ptr&lt;1&gt;</span></span></code></pre></div><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cp-async-bulk-prefetch" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L1479-L1497" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.cp_async_bulk_shared_cluster_global" href="#Reactant.MLIR.Dialects.nvvm.cp_async_bulk_shared_cluster_global"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.cp_async_bulk_shared_cluster_global</span></a> <span class="VPBadge info jlObjectType jlFunction"><!--[-->Function<!--]--></span></summary><p><code>cp_async_bulk_shared_cluster_global</code></p><p>Initiates an asynchronous copy operation from global memory to shared memory or shared_cluster memory.</p><p>The <code>multicastMask</code> operand is optional and can be used only when the destination is shared::cluster memory. When it is present, this Op copies data from global memory to shared memory of multiple CTAs in the cluster. Operand <code>multicastMask</code> specifies the destination CTAs in the cluster such that each bit position in the 16-bit <code>multicastMask</code> operand corresponds to the <code>nvvm.read.ptx.sreg.ctaid</code> of the destination CTA.</p><p>The <code>l2CacheHint</code> operand is optional, and it is used to specify cache eviction policy that may be used during the memory access.</p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cp-async-bulk" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L1426-L1443" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.cp_async_bulk_shared_cluster_shared_cta-NTuple{4, Reactant.MLIR.IR.Value}" href="#Reactant.MLIR.Dialects.nvvm.cp_async_bulk_shared_cluster_shared_cta-NTuple{4, Reactant.MLIR.IR.Value}"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.cp_async_bulk_shared_cluster_shared_cta</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>cp_async_bulk_shared_cluster_shared_cta</code></p><p>Initiates an asynchronous copy operation from Shared CTA memory to Shared cluster memory.</p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cp-async-bulk" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L1590-L1597" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.cp_async_bulk_tensor_global_shared_cta" href="#Reactant.MLIR.Dialects.nvvm.cp_async_bulk_tensor_global_shared_cta"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.cp_async_bulk_tensor_global_shared_cta</span></a> <span class="VPBadge info jlObjectType jlFunction"><!--[-->Function<!--]--></span></summary><p><code>cp_async_bulk_tensor_global_shared_cta</code></p><p>Initiates an asynchronous copy of the tensor data from shared::cta memory to global memory. This Op supports all the store modes specified in <code>TMAStoreMode</code>.</p><p>The <code>l2CacheHint</code> operand is optional, and it is used to specify cache eviction policy that may be used during the memory access.</p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#data-movement-and-conversion-instructions-cp-async-bulk-tensor" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L1787-L1798" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.cp_async_bulk_tensor_prefetch" href="#Reactant.MLIR.Dialects.nvvm.cp_async_bulk_tensor_prefetch"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.cp_async_bulk_tensor_prefetch</span></a> <span class="VPBadge info jlObjectType jlFunction"><!--[-->Function<!--]--></span></summary><p><code>cp_async_bulk_tensor_prefetch</code></p><p>Initiates an asynchronous prefetch operation on the tensor data from global memory to L2 cache. This Op supports all the load modes specified in <code>TMALoadMode</code>.</p><p>The <code>l2CacheHint</code> operand is optional, and it is used to specify cache eviction policy that may be used during the memory access.</p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cp-async-bulk-prefetch-tensor" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L1692-L1703" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.cp_async_bulk_tensor_reduce" href="#Reactant.MLIR.Dialects.nvvm.cp_async_bulk_tensor_reduce"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.cp_async_bulk_tensor_reduce</span></a> <span class="VPBadge info jlObjectType jlFunction"><!--[-->Function<!--]--></span></summary><p><code>cp_async_bulk_tensor_reduce</code></p><p>Initiates an asynchronous reduction operation of tensor data in global memory with tensor data in shared memory.</p><p add,="" min,="" max,="" inc,="" dec,="" and,="" or,="" xor="">The <code>mode</code> attribute indicates whether the copy mode is tile or im2col. The <code>redOp</code> attribute specifies the reduction operations applied. The supported reduction operations are:</p><p>The <code>l2CacheHint</code> operand is optional, and it is used to specify cache eviction policy that may be used during the memory access.</p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cp-reduce-async-bulk-tensor" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L1738-L1753" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.cp_async_bulk_tensor_shared_cluster_global" href="#Reactant.MLIR.Dialects.nvvm.cp_async_bulk_tensor_shared_cluster_global"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.cp_async_bulk_tensor_shared_cluster_global</span></a> <span class="VPBadge info jlObjectType jlFunction"><!--[-->Function<!--]--></span></summary><p><code>cp_async_bulk_tensor_shared_cluster_global</code></p><p>Initiates an asynchronous copy operation on the tensor data from global memory to shared::cluster (or) shared::cta memory. This Op supports all the load modes specified in <code>TMALoadMode</code>.</p><p>The <code>multicastMask</code> operand is optional. When it is present, the Op copies data from global memory to shared memory of multiple CTAs in the cluster. Operand <code>multicastMask</code> specifies the destination CTAs in the cluster such that each bit position in the 16-bit <code>multicastMask</code> operand corresponds to the <code>nvvm.read.ptx.sreg.ctaid</code> of the destination CTA.</p><p>The <code>l2CacheHint</code> operand is optional, and it is used to specify cache eviction policy that may be used during the memory access.</p><p>When the <code>isCTAOnly</code> attribute is set to true, the destination is shared::cta only. Hence, <code>multicastMask</code> and <code>CTAGroup</code> are not applicable when <code>isCTAOnly</code> is true.</p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cp-async-bulk-tensor" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L1619-L1640" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.cp_async_bulk_wait_group-Tuple{}" href="#Reactant.MLIR.Dialects.nvvm.cp_async_bulk_wait_group-Tuple{}"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.cp_async_bulk_wait_group</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>cp_async_bulk_wait_group</code></p><p>Op waits for completion of the most recent bulk async-groups.</p><p>The <code>$group</code> operand tells waiting has to be done until for <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.464ex;" xmlns="http://www.w3.org/2000/svg" width="53.79ex" height="2.059ex" role="img" focusable="false" viewBox="0 -705 23775.1 910" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(477,0)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(928,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(1413,0)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(1985,0)"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(2488,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(2973,0)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(3424,0)"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(3974,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(4440,0)"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(5156,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(5622,0)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(6073,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(6558,0)"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(7108,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(7469,0)"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(8045,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(8511,0)"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(9389,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(9874,0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(10343,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(10704,0)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(11155,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(11621,0)"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(12054,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(12520,0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(13120,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(13481,0)"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(13910,0)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(14482,0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(14780,0)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(15301,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(15830,0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(16299,0)"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(16789,0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(17389,0)"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(18044.2,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(19044.4,0)"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(19521.4,0)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(19972.4,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(20457.4,0)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(21029.4,0)"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(21532.4,0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(22001.4,0)"><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(22446.1,0)"><path data-c="1D43C" d="M43 1Q26 1 26 10Q26 12 29 24Q34 43 39 45Q42 46 54 46H60Q120 46 136 53Q137 53 138 54Q143 56 149 77T198 273Q210 318 216 344Q286 624 286 626Q284 630 284 631Q274 637 213 637H193Q184 643 189 662Q193 677 195 680T209 683H213Q285 681 359 681Q481 681 487 683H497Q504 676 504 672T501 655T494 639Q491 637 471 637Q440 637 407 634Q393 631 388 623Q381 609 337 432Q326 385 315 341Q245 65 245 59Q245 52 255 50T307 46H339Q345 38 345 37T342 19Q338 6 332 0H316Q279 2 179 2Q143 2 113 2T65 2T43 1Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(22950.1,0)"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(23500.1,0)"><path data-c="2035" d="M12 501Q12 527 31 542T63 558Q73 560 77 560Q114 560 128 528Q133 518 188 293T244 61Q244 56 223 50T195 43Q192 43 190 45T102 263T14 486Q12 496 12 501Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>g</mi><mi>r</mi><mi>o</mi><mi>u</mi><mi>p</mi><mi>o</mi><mi>r</mi><mi>f</mi><mi>e</mi><mi>w</mi><mi>e</mi><mi>r</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>h</mi><mi>e</mi><mi>m</mi><mi>o</mi><mi>s</mi><mi>t</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>b</mi><mi>u</mi><mi>l</mi><mi>k</mi><mi>a</mi><mi>s</mi><mi>y</mi><mi>n</mi><mi>c</mi><mo>−</mo><mi>g</mi><mi>r</mi><mi>o</mi><mi>u</mi><mi>p</mi><mi>s</mi><mo>.</mo><mi>I</mi><mi>f</mi><mo data-mjx-pseudoscript="true">‘</mo></math></mjx-assistive-mml></mjx-container>group` is 0, the op wait until all the most recent bulk async-groups have completed.</p><p>The <code>$read</code> indicates that the waiting has to be done until all the bulk async operations in the specified bulk async-group have completed reading from their source locations.</p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cp-async-bulk-wait-group" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L1839-L1853" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.cp_async_mbarrier_arrive-Tuple{Reactant.MLIR.IR.Value}" href="#Reactant.MLIR.Dialects.nvvm.cp_async_mbarrier_arrive-Tuple{Reactant.MLIR.IR.Value}"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.cp_async_mbarrier_arrive</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>cp_async_mbarrier_arrive</code></p><p>The <code>cp.async.mbarrier.arrive</code> Op makes the <em>mbarrier object</em> track all prior cp.async operations initiated by the executing thread. The <code>addr</code> operand specifies the address of the <em>mbarrier object</em> in generic or shared::cta address space. When it is generic, the underlying memory should fall within the shared::cta space; otherwise the behavior is undefined. The <code>noinc</code> attr impacts how the mbarrier&#39;s state is updated.</p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#parallel-synchronization-and-communication-instructions-cp-async-mbarrier-arrive" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L1893-L1905" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.dot_accumulate_2way-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}" href="#Reactant.MLIR.Dialects.nvvm.dot_accumulate_2way-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.dot_accumulate_2way</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>dot_accumulate_2way</code></p><p>Performs a two-way 16-bit to 8-bit dot-product which is accumulated in a 32-bit result. Operand <code>a</code> is a vector of two 16-bit elements and operand <code>b</code> a vector of four 8-bit elements between which the dot product is computed.</p><p>The <code>a_type</code> and <code>b_type</code> attributes specify the type of the elements in <code>a</code> and <code>b</code> respectively. If <code>a_type</code> or <code>b_type</code> is <code>s</code>, then the elements in the corresponding vector are sign-extended to 32-bit before the dot product is computed. If <code>a_type</code> or <code>b_type</code> is <code>u</code>, then the elements in the corresponding vector are zero-extended to 32-bit instead.</p><p>The <code>b_hi</code> boolean attribute specifies which two bytes of <code>b</code> are used for the dot product. If <code>b_hi</code> is true, then the dot product is computed between <code>a</code> and elements at indices 2 and 3 of <code>b</code>. If <code>b_hi</code> is false, then the dot product is computed between <code>a</code> and elements at indices 0 and 1 of <code>b</code>.</p><p>Operand <code>c</code> is a 32-bit integer to which the result is accumulated. It is treated as holding a signed integer if any of <code>a_type</code> or <code>b_type</code> is signed.</p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#integer-arithmetic-instructions-dp2a" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L1974-L2000" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.dot_accumulate_4way-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}" href="#Reactant.MLIR.Dialects.nvvm.dot_accumulate_4way-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.dot_accumulate_4way</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>dot_accumulate_4way</code></p><p>Performs a four-way byte dot-product which is accumulated in a 32-bit result. Operand <code>a</code> and <code>b</code> are vectors of 4 bytes between which the dot product is computed.</p><p>The <code>a_type</code> and <code>b_type</code> attributes specify the type of the elements in <code>a</code> and <code>b</code> respectively. If <code>a_type</code> or <code>b_type</code> is <code>signed</code>, then the elements in the corresponding vector are sign-extended to 32-bit before the dot product is computed. If <code>a_type</code> or <code>b_type</code> is <code>unsigned</code>, then the elements in the corresponding vector are zero-extended to 32-bit instead.</p><p>Operand <code>c</code> is a 32-bit integer to which the result is accumulated. It is treated as holding a signed integer if any of <code>a_type</code> or <code>b_type</code> is <code>s8</code>.</p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#integer-arithmetic-instructions-dp4a" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L2026-L2045" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.elect_sync" href="#Reactant.MLIR.Dialects.nvvm.elect_sync"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.elect_sync</span></a> <span class="VPBadge info jlObjectType jlFunction"><!--[-->Function<!--]--></span></summary><p><code>elect_sync</code></p><p>The <code>elect.sync</code> instruction elects one predicated active leader thread from among a set of threads specified in the <code>membermask</code>. When the <code>membermask</code> is not provided explicitly, a default value of <code>0xFFFFFFFF</code> is used. The predicate result is set to <code>True</code> for the leader thread, and <code>False</code> for all other threads.</p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#parallel-synchronization-and-communication-instructions-elect-sync" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L2088-L2098" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.exit-Tuple{}" href="#Reactant.MLIR.Dialects.nvvm.exit-Tuple{}"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.exit</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>exit</code></p><p>Ends execution of a thread. <a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#control-flow-instructions-exit" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L2729-L2734" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.fence_mbarrier_init-Tuple{}" href="#Reactant.MLIR.Dialects.nvvm.fence_mbarrier_init-Tuple{}"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.fence_mbarrier_init</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>fence_mbarrier_init</code></p><p>Fence operation that applies on the prior nvvm.mbarrier.init</p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#parallel-synchronization-and-communication-instructions-membar" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L2754-L2760" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.fence_proxy-Tuple{}" href="#Reactant.MLIR.Dialects.nvvm.fence_proxy-Tuple{}"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.fence_proxy</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>fence_proxy</code></p><p>Fence operation with proxy to establish an ordering between memory accesses that may happen through different proxies.</p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#parallel-synchronization-and-communication-instructions-membar" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L2819-L2826" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.fence_proxy_acquire-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}" href="#Reactant.MLIR.Dialects.nvvm.fence_proxy_acquire-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.fence_proxy_acquire</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>fence_proxy_acquire</code></p><p><code>fence.proxy.acquire</code> is a uni-directional fence used to establish ordering between a prior memory access performed via the generic proxy and a subsequent memory access performed via the tensormap proxy</p><p>The address operand <code>addr</code> and the operand <code>size</code> together specify the memory range <code>[addr, addr+size)</code> on which the ordering guarantees on the memory accesses across the proxies is to be provided. The only supported value for the <code>size</code> operand is 128 and must be an immediate. Generic Addressing is used unconditionally, and the address specified by the operand <code>addr</code> must fall within the <code>.global</code> state space. Otherwise, the behavior is undefined</p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#parallel-synchronization-and-communication-instructions-membar" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L2780-L2795" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.fence_proxy_release-Tuple{}" href="#Reactant.MLIR.Dialects.nvvm.fence_proxy_release-Tuple{}"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.fence_proxy_release</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>fence_proxy_release</code></p><p><code>fence.proxy.release</code> is a uni-directional fence used to establish ordering between a prior memory access performed via the generic proxy and a subsequent memory access performed via the tensormap proxy. <code>fence.proxy.release</code> operation can form a release sequence that synchronizes with an acquire sequence that contains the fence.proxy.acquire proxy fence operation</p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#parallel-synchronization-and-communication-instructions-membar" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L2847-L2857" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.fence_proxy_sync_restrict-Tuple{}" href="#Reactant.MLIR.Dialects.nvvm.fence_proxy_sync_restrict-Tuple{}"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.fence_proxy_sync_restrict</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>fence_proxy_sync_restrict</code></p><p>The <code>nvvm.fence.proxy.sync_restrict</code> Op used to establish ordering between a prior memory access performed between proxies. Currently, the ordering is only supported between async and generic proxies. <code>sync_restrict</code> restricts <code>acquire</code> memory semantics to <code>shared_cluster</code> and <code>release</code> memory semantics to <code>shared_cta</code> with cluster scope. <a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#parallel-synchronization-and-communication-instructions-membar" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L2881-L2890" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.fence_sync_restrict-Tuple{}" href="#Reactant.MLIR.Dialects.nvvm.fence_sync_restrict-Tuple{}"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.fence_sync_restrict</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>fence_sync_restrict</code></p><p>The <code>nvvm.fence.sync_restrict</code> Op restricts the class of memory operations for which the fence instruction provides the memory ordering guarantees. <code>sync_restrict</code> restricts <code>acquire</code> memory semantics to <code>shared_cluster</code> and <code>release</code> memory semantics to <code>shared_cta</code> with cluster scope. <a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#parallel-synchronization-and-communication-instructions-membar" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L2933-L2941" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.griddepcontrol-Tuple{}" href="#Reactant.MLIR.Dialects.nvvm.griddepcontrol-Tuple{}"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.griddepcontrol</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>griddepcontrol</code></p><p>If the kind attribute is set to <code>wait</code>, it causes the executing thread to wait until all prerequisite grids in flight have completed and all the memory operations from the prerequisite grids are performed and made visible to the current grid.</p><p>When the kind is launch_dependents, it signals that specific dependents the runtime system designated to react to this instruction can be scheduled as soon as all other CTAs in the grid issue the same instruction or have completed.</p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#parallel-synchronization-and-communication-instructions-griddepcontrol" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L3079-L3093" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.inline_ptx" href="#Reactant.MLIR.Dialects.nvvm.inline_ptx"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.inline_ptx</span></a> <span class="VPBadge info jlObjectType jlFunction"><!--[-->Function<!--]--></span></summary><p><code>inline_ptx</code> This op allows using PTX directly within the NVVM dialect, while greatly simplifying llvm.inline_asm generation. It automatically handles register size selection and sets the correct read/write access for each operand. The operation leverages the <code>BasicPtxBuilderInterface</code> to abstract away low-level details of PTX assembly formatting.</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>The `predicate` attribute is used to specify a predicate for the </span></span>
<span class="line"><span>PTX instruction.</span></span>
<span class="line"><span></span></span>
<span class="line"><span>Example 1: Read-only Parameters</span></span>
<span class="line"><span>```mlir</span></span>
<span class="line"><span>nvvm.inline_ptx &quot;mbarrier.init.b64 [$0], $1;&quot; (%barrier_gen, %count) : !llvm.ptr, i32</span></span>
<span class="line"><span></span></span>
<span class="line"><span>// Lowers to:</span></span>
<span class="line"><span>llvm.inline_asm has_side_effects asm_dialect = att </span></span>
<span class="line"><span>  &quot;mbarrier.init.b64 [$0], $1;&quot;, &quot;l,r&quot; %arg0, %arg2 : (!llvm.ptr, i32) -&gt; ()</span></span>
<span class="line"><span>```</span></span>
<span class="line"><span></span></span>
<span class="line"><span>Example 2: Read-only and Write-only Parameters</span></span>
<span class="line"><span>```mlir</span></span>
<span class="line"><span>%0 = nvvm.inline_ptx &quot;ex2.approx.ftz.f32 $0, $1;&quot; (%input) : f32 -&gt; f32</span></span>
<span class="line"><span></span></span>
<span class="line"><span>// Lowers to:</span></span>
<span class="line"><span>%0 = llvm.inline_asm has_side_effects asm_dialect = att </span></span>
<span class="line"><span>  &quot;ex2.approx.ftz.f32 $0, $1;&quot;, &quot;=f,f&quot; %arg0 : (f32) -&gt; f32</span></span>
<span class="line"><span>```</span></span>
<span class="line"><span></span></span>
<span class="line"><span>Example 3: Predicate Usage</span></span>
<span class="line"><span>```mlir</span></span>
<span class="line"><span>nvvm.inline_ptx &quot;mbarrier.init.b64 [$0], $1;&quot; (%barrier_gen, %count), </span></span>
<span class="line"><span>  predicate = %pred : !llvm.ptr, i32, i1</span></span>
<span class="line"><span></span></span>
<span class="line"><span>// Lowers to:</span></span>
<span class="line"><span>llvm.inline_asm has_side_effects asm_dialect = att </span></span>
<span class="line"><span>  &quot;@$2 mbarrier.init.b64 [$0], $1;&quot;, &quot;l,r,b&quot; %arg0, %arg2, %arg3 </span></span>
<span class="line"><span>  : (!llvm.ptr, i32, i1) -&gt; ()</span></span>
<span class="line"><span>```</span></span></code></pre></div><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L3113-L3153" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.match_sync-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}" href="#Reactant.MLIR.Dialects.nvvm.match_sync-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.match_sync</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>match_sync</code></p><p>The <code>match.sync</code> op performs broadcast and compare of operand <code>val</code> across all non-exited threads in <code>thread_mask</code> and returns a mask depending on the kind and an optional predicate.</p><p>The matching operation kinds are:</p><ul><li><code>any</code>: Returns a mask corresponding to the non-exited threads in the</li></ul><p><code>thread_mask</code> that have the same value of operand <code>val</code>.</p><ul><li><code>all</code>: Returns a mask and a predicate. If all non-exited threads in the</li></ul><p><code>thread_mask</code> have the same value of operand <code>val</code>, the predicate is set to true and the mask corresponds to the non-exited threads in the <code>thread_mask</code>. Otherwise, the predicate is set to false and the mask is 0.</p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#parallel-synchronization-and-communication-instructions-match-sync" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L3982-L3998" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.mbarrier_arrive" href="#Reactant.MLIR.Dialects.nvvm.mbarrier_arrive"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.mbarrier_arrive</span></a> <span class="VPBadge info jlObjectType jlFunction"><!--[-->Function<!--]--></span></summary><p><code>mbarrier_arrive</code></p><p>The <code>nvvm.mbarrier.arrive</code> operation performs an arrive-on operation on the <em>mbarrier object</em> at the specified address. Uses the default <code>.release.cta</code> semantics. This release pattern establishes memory ordering for operations occurring in program order before this arrive instruction by making operations from the current thread visible to subsequent operations in other threads within the CTA. When other threads perform corresponding acquire operations (like &#39;mbarrier.test.wait&#39;), they synchronize with this release pattern.</p><p>This operation causes the executing thread to signal its arrival at the barrier.</p><ul><li><code>res</code>: When the <code>space</code> is not shared_cluster, this operation returns an opaque 64-bit value capturing the phase of the <em>mbarrier object</em> prior to the arrive-on operation. The contents of this return value are implementation-specific. An <em>mbarrier object</em> located in the shared_cluster space cannot return a value.</li></ul><p>The operation takes the following operands:</p><ul><li><p><code>addr</code>: A pointer to the memory location of the <em>mbarrier object</em>. The <code>addr</code> must be a pointer to generic or shared_cta or shared_cluster memory. When it is generic, the underlying address must be within the shared_cta memory space; otherwise the behavior is undefined.</p></li><li><p><code>count</code>: This specifies the amount by which the pending arrival count is decremented. If the <code>count</code> argument is not specified, the pending arrival count is decremented by 1.</p></li><li><p><code>scope</code>: This specifies the set of threads that directly observe the memory synchronizing effect of the <code>mbarrier.arrive</code> operation.</p></li><li><p><code>space</code>: This indicates the memory space where the mbarrier object resides.</p></li><li><p><code>relaxed</code>: When set to true, the <code>arrive</code> operation has relaxed memory semantics and does not provide any ordering or visibility guarantees.</p></li></ul><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#parallel-synchronization-and-communication-instructions-mbarrier-arrive" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L3550-L3584" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.mbarrier_arrive_drop" href="#Reactant.MLIR.Dialects.nvvm.mbarrier_arrive_drop"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.mbarrier_arrive_drop</span></a> <span class="VPBadge info jlObjectType jlFunction"><!--[-->Function<!--]--></span></summary><p><code>mbarrier_arrive_drop</code></p><p>The <code>nvvm.mbarrier.arrive_drop</code> operation decrements the expected arrival count of the <em>mbarrier object</em> by <code>count</code> and then performs an arrive-on operation. When <code>count</code> is not specified, it defaults to 1. The decrement of the expected arrival count applies to all the subsequent phases of the <em>mbarrier object</em>. The remaining semantics are identical to those of the <code>nvvm.mbarrier.arrive</code> operation.</p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#parallel-synchronization-and-communication-instructions-mbarrier-arrive-drop" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L3397-L3408" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.mbarrier_arrive_drop_expect_tx-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}" href="#Reactant.MLIR.Dialects.nvvm.mbarrier_arrive_drop_expect_tx-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.mbarrier_arrive_drop_expect_tx</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>mbarrier_arrive_drop_expect_tx</code></p><p>The <code>nvvm.mbarrier.arrive_drop.expect_tx</code> operation is similar to the <code>nvvm.mbarrier.arrive.expect_tx</code> operation except that it performs an <code>arrive_drop</code> operation instead of only an <code>arrive</code> operation.</p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#parallel-synchronization-and-communication-instructions-mbarrier-arrive-drop" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L3328-L3336" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.mbarrier_arrive_drop_nocomplete-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}" href="#Reactant.MLIR.Dialects.nvvm.mbarrier_arrive_drop_nocomplete-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.mbarrier_arrive_drop_nocomplete</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>mbarrier_arrive_drop_nocomplete</code></p><p>The <code>nvvm.mbarrier.arrive_drop.nocomplete</code> operation decrements the expected arrival count of the <em>mbarrier object</em> by the amount <code>count</code> and then performs an arrive-on operation on the <em>mbarrier object</em> with the guarantee that it will not cause the barrier to complete its current phase.</p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#parallel-synchronization-and-communication-instructions-mbarrier-arrive-drop" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L3366-L3375" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.mbarrier_arrive_expect_tx" href="#Reactant.MLIR.Dialects.nvvm.mbarrier_arrive_expect_tx"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.mbarrier_arrive_expect_tx</span></a> <span class="VPBadge info jlObjectType jlFunction"><!--[-->Function<!--]--></span></summary><p><code>mbarrier_arrive_expect_tx</code></p><p>The <code>nvvm.mbarrier.arrive.expect_tx</code> operation performs an expect-tx operation followed by an arrive-on operation on the <em>mbarrier object</em>. Uses the default <code>.release.cta</code> semantics. This release pattern establishes memory ordering for operations occurring in program order before this arrive instruction by making operations from the current thread visible to subsequent operations in other threads within the CTA. When other threads perform corresponding acquire operations (like &#39;mbarrier.test.wait&#39;), they synchronize with this release pattern.</p><p>This operation first performs an expect-tx operation with the specified transaction count, then performs an arrive-on operation with an implicit count of 1. The expect-tx operation increases the expect-count of the <em>mbarrier object</em> by the specified value (i.e. <code>txcount</code>), setting the current phase to expect and track the completion of additional asynchronous transactions.</p><p>The operation takes the following operands:</p><ul><li><p><code>addr</code>: A pointer to the memory location of the <em>mbarrier object</em>. Uses generic addressing, but the address must still be in the shared memory space.</p></li><li><p><code>txcount</code>: An unsigned integer specifying the expected transaction count for the expect-tx operation. This represents the number of asynchronous transactions expected to complete before the barrier phase completes.</p></li><li><p><code>scope</code>: This specifies the set of threads that directly observe the memory synchronizing effect of the <code>mbarrier.test.wait</code> operation.</p></li><li><p><code>relaxed</code>: When set to true, the <code>arrive</code> operation has relaxed memory semantics and does not provide any ordering or visibility guarantees.</p></li><li><p><code>predicate</code>: Optional predicate for conditional execution used only when lowering to inline-ptx.</p></li></ul><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#parallel-synchronization-and-communication-instructions-mbarrier-arrive-drop" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L3439-L3470" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.mbarrier_arrive_nocomplete-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}" href="#Reactant.MLIR.Dialects.nvvm.mbarrier_arrive_nocomplete-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.mbarrier_arrive_nocomplete</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>mbarrier_arrive_nocomplete</code></p><p>The <code>nvvm.mbarrier.arrive.nocomplete</code> operation performs an arrive-on operation on the <em>mbarrier object</em> with the guarantee that it will not cause the barrier to complete its current phase. Uses the default <code>.release.cta</code> semantics. This release pattern establishes memory ordering for operations occurring in program order before this arrive instruction by making operations from the current thread visible to subsequent operations in other threads within the CTA. When other threads perform corresponding acquire operations (like &#39;mbarrier.test.wait&#39;), they synchronize with this release pattern.</p><p>This operation causes the executing thread to signal its arrival at the barrier with a specified count, but ensures that the barrier phase will not complete as a result of this operation. The operation returns an opaque value that captures the phase of the <em>mbarrier object</em> prior to the arrive-on operation.</p><p>The operation takes the following operands:</p><ul><li><p><code>addr</code>: A pointer to the memory location of the <em>mbarrier object</em>. The <code>addr</code> must be a pointer to generic or shared::cta memory. When it is generic, the underlying address must be within the shared::cta memory space; otherwise the behavior is undefined.</p></li><li><p><code>count</code>: Integer specifying the count argument to the arrive-on operation. Must be in the valid range as specified in the <em>mbarrier object</em> contents.</p></li></ul><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#parallel-synchronization-and-communication-instructions-mbarrier-arrive" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L3502-L3528" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.mbarrier_complete_tx-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}" href="#Reactant.MLIR.Dialects.nvvm.mbarrier_complete_tx-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.mbarrier_complete_tx</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>mbarrier_complete_tx</code></p><p>The <code>nvvm.mbarrier.complete_tx</code> operation decrements the transaction count of the <em>mbarrier object</em> at <code>addr</code> by <code>txcount</code>. It also signals the completion of asynchronous transactions that were tracked by the current phase. The <code>scope</code> specifies the set of threads that can directly observe the memory synchronizing effect of the <code>mbarrier.complete_tx</code> operation. <code>CTA</code> and <code>CLUSTER</code> are the only allowed values for <code>scope</code>.</p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#parallel-synchronization-and-communication-instructions-mbarrier-complete-tx" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L3615-L3626" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.mbarrier_expect_tx-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}" href="#Reactant.MLIR.Dialects.nvvm.mbarrier_expect_tx-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.mbarrier_expect_tx</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>mbarrier_expect_tx</code></p><p>The <code>nvvm.mbarrier.expect_tx</code> operation increases the transaction count of the mbarrier located at <code>addr</code> by <code>txcount</code> amount. The <code>scope</code> specifies the set of threads that can directly observe the memory synchronizing effect of the <code>mbarrier.expect_tx</code> operation. <code>CTA</code> and <code>CLUSTER</code> are the only allowed values for <code>scope</code>.</p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#parallel-synchronization-and-communication-instructions-mbarrier-expect-tx" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L3649-L3659" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.mbarrier_init" href="#Reactant.MLIR.Dialects.nvvm.mbarrier_init"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.mbarrier_init</span></a> <span class="VPBadge info jlObjectType jlFunction"><!--[-->Function<!--]--></span></summary><p><code>mbarrier_init</code></p><p>The <code>nvvm.mbarrier.init</code> operation initializes an <em>mbarrier object</em> at the specified memory location.</p><p>This operation initializes the <em>mbarrier object</em> with the following state:</p><ul><li><p>Current phase: 0</p></li><li><p>Expected arrival count: <code>count</code></p></li><li><p>Pending arrival count: <code>count</code></p></li><li><p>Transaction count (tx-count): 0</p></li></ul><p>The operation takes the following operands:</p><ul><li><p><code>addr</code>: A pointer to the memory location of the <em>mbarrier object</em>. The <code>addr</code> must be a pointer to generic or shared::cta memory. When it is generic, the underlying address must be within the shared::cta memory space; otherwise the behavior is undefined.</p></li><li><p><code>count</code>: Integer specifying the number of threads that will participate in barrier synchronization. Must be in the range [1, 2²⁰ - 1].</p></li><li><p><code>predicate</code>: Optional predicate for conditional execution.</p></li></ul><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#parallel-synchronization-and-communication-instructions-mbarrier-init" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L3680-L3702" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.mbarrier_inval-Tuple{Reactant.MLIR.IR.Value}" href="#Reactant.MLIR.Dialects.nvvm.mbarrier_inval-Tuple{Reactant.MLIR.IR.Value}"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.mbarrier_inval</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>mbarrier_inval</code></p><p>The <code>nvvm.mbarrier.inval</code> operation invalidates an <em>mbarrier object</em> at the specified memory location.</p><p>This operation marks the <em>mbarrier object</em> as invalid, making it safe to repurpose the memory location for other uses or to reinitialize it as a new <em>mbarrier object</em>. It is undefined behavior if the <em>mbarrier object</em> is already invalid.</p><p>The operation takes the following operand:</p><ul><li><code>addr</code>: A pointer to the memory location of the <em>mbarrier object</em>. The <code>addr</code> must be a pointer to generic or shared::cta memory. When it is generic, the underlying address must be within the shared::cta memory space; otherwise the behavior is undefined.</li></ul><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#parallel-synchronization-and-communication-instructions-mbarrier-inval" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L3725-L3742" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.mbarrier_test_wait-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}" href="#Reactant.MLIR.Dialects.nvvm.mbarrier_test_wait-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.mbarrier_test_wait</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>mbarrier_test_wait</code></p><p>The <code>nvvm.mbarrier.test.wait</code> operation performs a non-blocking test for the completion of a specific phase of an <em>mbarrier object</em>. It uses the default <code>.acquire.cta</code> semantics. This acquire pattern establishes memory ordering for operations occurring in program order after this wait instruction by making operations from other threads in the CTA visible to subsequent operations in the current thread. When this wait completes, it synchronizes with the corresponding release pattern from the <code>mbarrier.arrive</code> operation, establishing memory ordering within the CTA.</p><p>This operation tests whether the mbarrier phase specified by the state operand has completed. It is a non-blocking instruction that immediately returns the completion status without suspending the executing thread.</p><p>The operation takes the following operands:</p><ul><li><p><code>addr</code>: A pointer to the memory location of the <em>mbarrier object</em>. Uses generic addressing, but the address must still be in the shared memory space.</p></li><li><p><code>stateOrPhase</code>: This argument represents a <code>state</code> when it is a 64-bit value and represents a <code>phase</code> when it is a 32-bit value. The <code>state</code> is an opaque value returned by a previous <code>mbarrier.arrive</code> operation on the same <em>mbarrier object</em> during the current or immediately preceding phase. The <code>phase</code> is an integer specifying the phase parity (0 or 1). Even phases have parity 0, odd phases have parity 1.</p></li><li><p><code>scope</code>: This specifies the set of threads that directly observe the memory synchronizing effect of the <code>mbarrier.test.wait</code> operation.</p></li><li><p><code>relaxed</code>: When set to true, the <code>arrive</code> operation has relaxed memory semantics and does not provide any ordering or visibility guarantees.</p></li></ul><p>The operation returns a boolean value indicating whether the specified phase has completed:</p><ul><li><p><code>true</code>: The immediately preceding phase has completed</p></li><li><p><code>false</code>: The phase is still incomplete (current phase)</p></li></ul><p><strong>Memory ordering guarantees</strong>: When this wait returns true, the following ordering guarantees hold:</p><ol><li><p>All memory accesses (except async operations) requested prior to <code>mbarrier.arrive</code> having release semantics by participating CTA threads are visible to the executing thread.</p></li><li><p>All <code>cp.async</code> operations requested prior to <code>cp.async.mbarrier.arrive</code> by participating CTA threads are visible to the executing thread.</p></li><li><p>All <code>cp.async.bulk</code> operations using the same <em>mbarrier object</em> requested prior to <code>mbarrier.arrive</code> having release semantics by participating CTA threads are visible to the executing thread.</p></li><li><p>Memory accesses requested after this wait are not visible to memory accesses performed prior to <code>mbarrier.arrive</code> by other participating threads.</p></li><li><p>No ordering guarantee exists for memory accesses by the same thread between <code>mbarrier.arrive</code> and this wait.</p></li></ol><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#parallel-synchronization-and-communication-instructions-mbarrier-test-wait-try-wait" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L3762-L3815" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.mbarrier_try_wait" href="#Reactant.MLIR.Dialects.nvvm.mbarrier_try_wait"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.mbarrier_try_wait</span></a> <span class="VPBadge info jlObjectType jlFunction"><!--[-->Function<!--]--></span></summary><p><code>mbarrier_try_wait</code></p><p>The <code>nvvm.mbarrier.try_wait</code> operation checks whether the specified <em>mbarrier object</em> at <code>addr</code> has completed the given phase. Note that unlike the <code>nvvm.mbarrier.test.wait</code> operation, the try_wait operation is a potentially-blocking one. If the phase is not yet complete, the calling thread may be suspended. A suspended thread resumes execution once the phase completes or when a system-defined timeout occurs. Optionally, the <code>ticks</code> operand can be used to provide a custom timeout (in nanoseconds), overriding the system-defined one. The semantics of this operation and its operands are otherwise similar to those of the <code>nvvm.mbarrier.test.wait</code> Op.</p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#parallel-synchronization-and-communication-instructions-mbarrier-test-wait-try-wait" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L3844-L3859" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.mbarrier_try_wait_parity-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}" href="#Reactant.MLIR.Dialects.nvvm.mbarrier_try_wait_parity-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.mbarrier_try_wait_parity</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>mbarrier_try_wait_parity</code></p><p>The <code>nvvm.mbarrier.try_wait.parity</code> operation performs a potentially-blocking test for the completion of a specific phase of an <em>mbarrier object</em> using phase parity. It uses the default <code>.acquire.cta</code> semantics. This acquire pattern establishes memory ordering for operations occurring in program order after this wait instruction by making operations from other threads in the CTA visible to subsequent operations in the current thread. When this wait completes, it synchronizes with the corresponding release pattern from the <code>mbarrier.arrive</code> operation, establishing memory ordering within the CTA.</p><p>This operation waits for the completion of the mbarrier phase indicated by the phase parity. While it uses the underlying PTX <code>mbarrier.try_wait.parity</code> instruction, this MLIR operation generates a loop that enforces the test to complete before continuing execution, ensuring the barrier phase is actually completed rather than potentially timing out.</p><p>The operation takes the following operands:</p><ul><li><p><code>addr</code>: A pointer to the memory location of the <em>mbarrier object</em>. Uses generic addressing, but the address must still be in the shared memory space.</p></li><li><p><code>phase</code>: An integer specifying the phase parity (0 or 1). Even phases have parity 0, odd phases have parity 1.</p></li><li><p><code>ticks</code>: An unsigned integer specifying the suspend time hint in nanoseconds. This may be used instead of the system-dependent time limit.</p></li></ul><p><strong>Memory ordering guarantees</strong>: When this wait returns true, the following ordering guarantees hold:</p><ol><li><p>All memory accesses (except async operations) requested prior to <code>mbarrier.arrive</code> having release semantics by participating CTA threads are visible to the executing thread.</p></li><li><p>All <code>cp.async</code> operations requested prior to <code>cp.async.mbarrier.arrive</code> by participating CTA threads are visible to the executing thread.</p></li><li><p>All <code>cp.async.bulk</code> operations using the same <em>mbarrier object</em> requested prior to <code>mbarrier.arrive</code> having release semantics by participating CTA threads are visible to the executing thread.</p></li><li><p>Memory accesses requested after this wait are not visible to memory accesses performed prior to <code>mbarrier.arrive</code> by other participating threads.</p></li><li><p>No ordering guarantee exists for memory accesses by the same thread between <code>mbarrier.arrive</code> and this wait.</p></li></ol><p><strong>Implementation behavior</strong>: This operation generates a PTX loop that repeatedly calls the underlying <code>mbarrier.try_wait.parity</code> instruction until the barrier phase completes. Unlike the raw PTX instruction which may return without completion after a timeout, this MLIR operation guarantees completion by continuing to loop until the specified phase is reached.</p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#parallel-synchronization-and-communication-instructions-mbarrier-test-wait-try-wait" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L3890-L3941" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.memory_barrier-Tuple{}" href="#Reactant.MLIR.Dialects.nvvm.memory_barrier-Tuple{}"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.memory_barrier</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>memory_barrier</code></p><p><code>membar</code> operation guarantees that prior memory accesses requested by this thread are performed at the specified <code>scope</code>, before later memory operations requested by this thread following the membar instruction.</p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#parallel-synchronization-and-communication-instructions-membar" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L4018-L4026" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.mma_block_scale-Tuple{Vector{Reactant.MLIR.IR.Value}, Vector{Reactant.MLIR.IR.Value}, Vector{Reactant.MLIR.IR.Value}, Vararg{Reactant.MLIR.IR.Value, 6}}" href="#Reactant.MLIR.Dialects.nvvm.mma_block_scale-Tuple{Vector{Reactant.MLIR.IR.Value}, Vector{Reactant.MLIR.IR.Value}, Vector{Reactant.MLIR.IR.Value}, Vararg{Reactant.MLIR.IR.Value, 6}}"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.mma_block_scale</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>mma_block_scale</code></p><p>The <code>nvvm.mma.block_scale</code> operation collectively performs the operation <code>D = matmul(A * SF_A, B * SF_B) + C</code> using all threads in a warp.</p><p>A, B, C and D are dense matrices and SF_A and SF_B are scaling factors. Dimensions of SF_A and SF_B are based on scale vector sizes (x1, x2, x4), and the data type must be either ue8m0 or ue4m3.</p><p>All the threads in the warp must execute the same <code>mma.block_scale</code> operation.</p><p>This operation follows the same design pattern as <code>nvvm.mma.sync</code>, with additional scaling operands for both A and B matrices.</p><p><strong>Example</strong></p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>%d = nvvm.mma.block_scale A[%a0, %a1] B[%b0, %b1] C[%c0, %c1]</span></span>
<span class="line"><span>                          scaleA[%scaleAData, %byteIdA, %threadIdA]</span></span>
<span class="line"><span>                          scaleB[%scaleBData, %byteIdB, %threadIdB]</span></span>
<span class="line"><span>                          {shape = #nvvm.shape&lt;m = 16, n = 8, k = 64&gt;,</span></span>
<span class="line"><span>                           multiplicandAPtxType = #nvvm.mma_type&lt;e2m1&gt;,</span></span>
<span class="line"><span>                           multiplicandBPtxType = #nvvm.mma_type&lt;e2m1&gt;,</span></span>
<span class="line"><span>                           scaleVecSize = #nvvm.scale_vec_size&lt;x2&gt;,</span></span>
<span class="line"><span>                           blockScaleFormat = #nvvm.block_scale_format&lt;ue8m0&gt;,</span></span>
<span class="line"><span>                           kind = #nvvm.block_scale_kind&lt;mxf4nvf4&gt;}</span></span>
<span class="line"><span>    : (vector&lt;4xf16&gt;, vector&lt;2xf16&gt;, vector&lt;2xf32&gt;) -&gt; !llvm.struct&lt;(f32, f32)&gt;</span></span></code></pre></div><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L4046-L4074" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.mma_sp_block_scale-Tuple{Vector{Reactant.MLIR.IR.Value}, Vector{Reactant.MLIR.IR.Value}, Vector{Reactant.MLIR.IR.Value}, Vararg{Reactant.MLIR.IR.Value, 8}}" href="#Reactant.MLIR.Dialects.nvvm.mma_sp_block_scale-Tuple{Vector{Reactant.MLIR.IR.Value}, Vector{Reactant.MLIR.IR.Value}, Vector{Reactant.MLIR.IR.Value}, Vararg{Reactant.MLIR.IR.Value, 8}}"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.mma_sp_block_scale</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>mma_sp_block_scale</code></p><p>The <code>nvvm.mma.sp.block_scale</code> operation collectively performs the operation <code>D = matmul(A_sparse * SF_A, B * SF_B) + C</code> using all threads in a warp.</p><p>A is a sparse matrix, and B, C and D are dense matrices. SF_A and SF_B are scaling factors. Dimensions of SF_A and SF_B are based on scale vector sizes (x1, x2, x4), and the data type must be either ue8m0 or ue4m3.</p><p>This operation is similar to <code>nvvm.mma.block_scale</code> but with structured sparsity in the A operand. The sparsity follows the 2:4 structured sparse pattern where 2 out of every 4 elements are non-zero.</p><p>All the threads in the warp must execute the same <code>mma.sp.block_scale</code> operation.</p><p>The <code>sparseMetadata</code> operand provides the sparsity indices that indicate which elements in the A operand are non-zero. The <code>sparsitySelector</code> controls how the indices are distributed among threads in the warp and should typically be 0 or 1.</p><p>This operation follows the same design pattern as <code>nvvm.mma.sp.sync</code>, with additional scaling operands for both A and B matrices. Note that sparse block scale operations always use ordered metadata (sm_90+).</p><p><strong>Example</strong></p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>%d = nvvm.mma.sp.block_scale A[%a0, %a1] B[%b0, %b1] C[%c0, %c1]</span></span>
<span class="line"><span>                             sparseMetadata[%meta] selector[%sel]</span></span>
<span class="line"><span>                             scaleA[%scaleAData, %byteIdA, %threadIdA]</span></span>
<span class="line"><span>                             scaleB[%scaleBData, %byteIdB, %threadIdB]</span></span>
<span class="line"><span>                             {shape = #nvvm.shape&lt;m = 16, n = 8, k = 128&gt;,</span></span>
<span class="line"><span>                              multiplicandAPtxType = #nvvm.mma_type&lt;e2m1&gt;,</span></span>
<span class="line"><span>                              multiplicandBPtxType = #nvvm.mma_type&lt;e2m1&gt;,</span></span>
<span class="line"><span>                              scaleVecSize = #nvvm.scale_vec_size&lt;x2&gt;,</span></span>
<span class="line"><span>                              blockScaleFormat = #nvvm.block_scale_format&lt;ue8m0&gt;,</span></span>
<span class="line"><span>                              kind = #nvvm.block_scale_kind&lt;mxf4&gt;}</span></span>
<span class="line"><span>    : (vector&lt;2xf16&gt;, vector&lt;2xf16&gt;, vector&lt;2xf32&gt;) -&gt; !llvm.struct&lt;(f32, f32)&gt;</span></span></code></pre></div><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L4252-L4292" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.mma_sp_sync-Tuple{Vector{Reactant.MLIR.IR.Value}, Vector{Reactant.MLIR.IR.Value}, Vector{Reactant.MLIR.IR.Value}, Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}" href="#Reactant.MLIR.Dialects.nvvm.mma_sp_sync-Tuple{Vector{Reactant.MLIR.IR.Value}, Vector{Reactant.MLIR.IR.Value}, Vector{Reactant.MLIR.IR.Value}, Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.mma_sp_sync</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>mma_sp_sync</code></p><p>The <code>nvvm.mma.sp.sync</code> operation collectively performs the sparse operation <code>D = matmul(A_sparse, B) + C</code> using all threads in a warp.</p><p>This operation is similar to <code>nvvm.mma.sync</code> but with structured sparsity in the A operand. The sparsity follows the 2:4 structured sparse pattern where 2 out of every 4 elements are non-zero.</p><p>All the threads in the warp must execute the same <code>mma.sp.sync</code> operation.</p><p>The <code>sparseMetadata</code> operand provides the sparsity indices that indicate which elements in the A operand are non-zero. The <code>sparsitySelector</code> controls how the indices are distributed among threads in the warp and should typically be 0 or 1.</p><p>The optional <code>orderedMetadata</code> attribute specifies the metadata ordering:</p><ul><li><p>Absence (default): Uses standard sparse metadata ordering</p></li><li><p>Presence: Uses ordered metadata (PTX ISA 8.5+, sm_90+)</p></li></ul><p>The optional <code>kind</code> attribute specifies mixed-precision modes for FP8 operations:</p><ul><li><p><code>f8f6f4</code>: Enables e3m2, e2m3, e2m1 FP8 types and f16 accumulator (PTX ISA 8.7+, sm_90+)</p></li><li><p>Only valid with ordered metadata and m16n8k64 shape</p></li></ul><p>The shapes, layouts, and data types follow the same constraints as the regular <code>nvvm.mma.sync</code> operation, but the A operand contains only the non-zero elements in compressed format.</p><p><strong>Example</strong></p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>%d = nvvm.mma.sp.sync A[%a0, %a1] B[%b0, %b1] C[%c0, %c1]</span></span>
<span class="line"><span>                      sparseMetadata[%meta] selector[%sel]</span></span>
<span class="line"><span>                      {shape = {k = 32 : i32, m = 16 : i32, n = 8 : i32}}</span></span>
<span class="line"><span>    : (vector&lt;2xf16&gt;, vector&lt;2xf16&gt;, vector&lt;2xf16&gt;) -&gt; !llvm.struct&lt;(vector&lt;2xf16&gt;, vector&lt;2xf16&gt;)&gt;</span></span>
<span class="line"><span></span></span>
<span class="line"><span>// With ordered metadata:</span></span>
<span class="line"><span>%d = nvvm.mma.sp.sync A[%a0, %a1] B[%b0, %b1] C[%c0, %c1]</span></span>
<span class="line"><span>                      sparseMetadata[%meta] selector[%sel]</span></span>
<span class="line"><span>                      {orderedMetadata, shape = {k = 32 : i32, m = 16 : i32, n = 8 : i32}}</span></span>
<span class="line"><span>    : (vector&lt;2xf16&gt;, vector&lt;2xf16&gt;, vector&lt;2xf16&gt;) -&gt; !llvm.struct&lt;(vector&lt;2xf16&gt;, vector&lt;2xf16&gt;)&gt;</span></span></code></pre></div><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L4362-L4404" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.mma_sync-Tuple{Vector{Reactant.MLIR.IR.Value}, Vector{Reactant.MLIR.IR.Value}, Vector{Reactant.MLIR.IR.Value}}" href="#Reactant.MLIR.Dialects.nvvm.mma_sync-Tuple{Vector{Reactant.MLIR.IR.Value}, Vector{Reactant.MLIR.IR.Value}, Vector{Reactant.MLIR.IR.Value}}"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.mma_sync</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>mma_sync</code></p><p>The <code>nvvm.mma.sync</code> operation collectively performs the operation <code>D = matmul(A, B) + C</code> using all threads in a warp.</p><p>All the threads in the warp must execute the same <code>mma.sync</code> operation.</p><p>For each possible multiplicand PTX data type, there are one or more possible instruction shapes given as &quot;mMnNkK&quot;. The below table describes the posssibilities as well as the types required for the operands. Note that the data type for C (the accumulator) and D (the result) can vary independently when there are multiple possibilities in the &quot;C/D Type&quot; column.</p><p>When an optional attribute cannot be immediately inferred from the types of the operands and the result during parsing or validation, an error will be raised.</p><p><code>b1Op</code> is only relevant when the binary (b1) type is given to <code>multiplicandDataType</code>. It specifies how the multiply-and-acumulate is performed and is either <code>xor_popc</code> or <code>and_poc</code>. The default is <code>xor_popc</code>.</p><p><code>intOverflowBehavior</code> is only relevant when the <code>multiplicandType</code> attribute is one of <code>u8, s8, u4, s4</code>, this attribute describes how overflow is handled in the accumulator. When the attribute is <code>satfinite</code>, the accumulator values are clamped in the int32 range on overflow. This is the default behavior. Alternatively, accumulator behavior <code>wrapped</code> can also be specified, in which case overflow wraps from one end of the range to the other.</p><p><code>layoutA</code> and <code>layoutB</code> are required and should generally be set to <code>#nvvm.mma_layout&lt;row&gt;</code> and <code>#nvvm.mma_layout&lt;col&gt;</code> respectively, but other combinations are possible for certain layouts according to the table below.</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>| A/B Type | Shape     | ALayout | BLayout | A Type   | B Type   | C/D Type          |</span></span>
<span class="line"><span>|----------|-----------|---------|---------|----------|----------|-------------------|</span></span>
<span class="line"><span>| f64      | .m8n8k4   | row     | col     | 1x f64   | 1x f64   | 2x f64            |</span></span>
<span class="line"><span>| f16      | .m8n8k4   | row/col | row/col | 2x f16x2 | 2x f16x2 | 4x f16x2 or 8xf32 |</span></span>
<span class="line"><span>|          | .m16n8k8  | row     | col     | 2x f16x2 | 1x f16x2 | 2x f16x2 or 4 f32 |</span></span>
<span class="line"><span>|          | .m16n8k16 | row     | col     | 4x f16x2 | 2x f16x2 | 2x f16x2 or 4 f32 |</span></span>
<span class="line"><span>| bf16     | .m16n8k8  | row     | col     | 2x i32   | 1x i32   | 4x f32            |</span></span>
<span class="line"><span>|          | .m16n8k16 | row     | col     | 4x i32   | 2x i32   | 4x f32            |</span></span>
<span class="line"><span>| tf32     | .m16n8k4  | row     | col     | 2x i32   | 1x i32   | 4x f32            |</span></span>
<span class="line"><span>|          | .m16n8k8  | row     | col     | 4x i32   | 2x i32   | 2x f16x2 or 4 f32 |</span></span>
<span class="line"><span>| u8/s8    | .m8n8k16  | row     | col     | 1x i32   | 1x i32   | 2x i32            |</span></span>
<span class="line"><span>|          | .m16n8k16 | row     | col     | 2x i32   | 1x i32   | 4x i32            |</span></span>
<span class="line"><span>|          | .m16n8k32 | row     | col     | 4x i32   | 2x i32   | 4x i32            |</span></span>
<span class="line"><span>| u4/s4    | .m8n8k32  | row     | col     | 1x i32   | 1x i32   | 2x i32            |</span></span>
<span class="line"><span>|          | m16n8k32  | row     | col     | 2x i32   | 1x i32   | 4x i32            |</span></span>
<span class="line"><span>|          | m16n8k64  | row     | col     | 4x i32   | 2x i32   | 4x i32            |</span></span>
<span class="line"><span>| b1       | m8n8k128  | row     | col     | 1x i32   | 1x i32   | 2x i32            |</span></span>
<span class="line"><span>|          | m16n8k128 | row     | col     | 2x i32   | 1x i32   | 4x i32            |</span></span></code></pre></div><p><strong>Example</strong></p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span></span></span>
<span class="line"><span>%128 = nvvm.mma.sync A[%120, %121, %122, %123]</span></span>
<span class="line"><span>                     B[%124, %125]</span></span>
<span class="line"><span>                     C[%126, %127]</span></span>
<span class="line"><span>                     {layoutA = #nvvm.mma_layout&lt;row&gt;,</span></span>
<span class="line"><span>                      layoutB = #nvvm.mma_layout&lt;col&gt;,</span></span>
<span class="line"><span>                      shape = {k = 16 : i32, m = 16 : i32, n = 8 : i32}}</span></span>
<span class="line"><span>    : (vector&lt;2xf16&gt;, vector&lt;2xf16&gt;, vector&lt;2xf16&gt;)</span></span>
<span class="line"><span>       -&gt; !llvm.struct&lt;(vector&lt;2xf16&gt;, vector&lt;2xf16&gt;)&gt;</span></span></code></pre></div><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L4137-L4204" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.nanosleep-Tuple{Reactant.MLIR.IR.Value}" href="#Reactant.MLIR.Dialects.nvvm.nanosleep-Tuple{Reactant.MLIR.IR.Value}"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.nanosleep</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>nanosleep</code></p><p>The op suspends the thread for a sleep duration approximately close to the delay <code>$duration</code>, specified in nanoseconds.</p><p>The sleep duration is approximated, but guaranteed to be in the interval [0, 2*t]. The maximum sleep duration is 1 millisecond. The implementation may reduce the sleep duration for individual threads within a warp such that all sleeping threads in the warp wake up together.</p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#miscellaneous-instructions-nanosleep" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L4453-L4465" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.pmevent-Tuple{}" href="#Reactant.MLIR.Dialects.nvvm.pmevent-Tuple{}"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.pmevent</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>pmevent</code></p><p>Triggers one or more of a fixed number of performance monitor events, with event index or mask specified by immediate operand.</p><p>Without <code>mask</code> it triggers a single performance monitor event indexed by immediate operand a, in the range 0..15.</p><p>With <code>mask</code> it triggers one or more of the performance monitor events. Each bit in the 16-bit immediate operand controls an event.</p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#miscellaneous-instructions-pmevent" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L4485-L4498" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.prefetch" href="#Reactant.MLIR.Dialects.nvvm.prefetch"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.prefetch</span></a> <span class="VPBadge info jlObjectType jlFunction"><!--[-->Function<!--]--></span></summary><p><code>prefetch</code></p><p>Prefetches the cache line containing the address given by <code>addr</code>. The operand may be a global, local, or generic pointer. When <code>tensormap</code> is specified, the operand may instead be a constant or generic pointer. If the address maps to shared memory, the operation has no effect.</p><p>At most one of <code>cacheLevel</code> or <code>tensormap</code> may be present. The <code>cacheLevel</code> attribute selects the target cache level. When combined with <code>uniform</code>, the prefetch is performed to the uniform cache, in which case <code>addr</code> must be a generic pointer.</p><p>When <code>tensormap</code> is used, the line containing <code>addr</code> is brought from the constant or parameter state space for later use by <code>cp.async.bulk.tensor</code>. If <code>in_param_space</code> is specified, the generic pointer is interpreted as referring to the parameter state space.</p><p><code>uniform</code> can be specified after the <code>cacheLevel</code> to indicate that the prefetch is performed to the specified uniform cache level. If <code>uniform</code> is specified, <code>addr</code> must be a generic address pointer and no operation is performed if <code>addr</code> maps to a <code>const</code>, <code>local</code>, or <code>shared</code> memory location.</p><p>The <code>evictPriority</code> attribute is optional and specifies the cache eviction priority when <code>cacheLevel</code> is L2.</p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#data-movement-and-conversion-instructions-prefetch-prefetchu" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L4627-L4654" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.prmt" href="#Reactant.MLIR.Dialects.nvvm.prmt"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.prmt</span></a> <span class="VPBadge info jlObjectType jlFunction"><!--[-->Function<!--]--></span></summary><p><code>prmt</code></p><p>The <code>nvvm.prmt</code> operation constructs a permutation of the bytes of the first one or two operands, selecting based on the 2 least significant bits of the final operand.</p><p>The bytes in the first one or two source operands are numbered. The first source operand (%lo) is numbered {b3, b2, b1, b0}, in the case of the &#39;<mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.464ex;" xmlns="http://www.w3.org/2000/svg" width="7.457ex" height="2.059ex" role="img" focusable="false" viewBox="0 -705 3296 910" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(520,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(986,0)"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(1536,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(2065,0)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(2637,0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(2935,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>d</mi><mi>e</mi><mi>f</mi><mi>a</mi><mi>u</mi><mi>l</mi><mi>t</mi></math></mjx-assistive-mml></mjx-container>&#39;, &#39;<mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.464ex;" xmlns="http://www.w3.org/2000/svg" width="3.43ex" height="2.059ex" role="img" focusable="false" viewBox="0 -705 1516 910" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z" style="stroke-width:3;"></path></g><g data-mml-node="mn" transform="translate(550,0)"><path data-c="34" d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(1050,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi><mn>4</mn><mi>e</mi></math></mjx-assistive-mml></mjx-container>&#39; and &#39;<mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.025ex;" xmlns="http://www.w3.org/2000/svg" width="3.156ex" height="1.595ex" role="img" focusable="false" viewBox="0 -694 1395 705" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z" style="stroke-width:3;"></path></g><g data-mml-node="mn" transform="translate(429,0)"><path data-c="34" d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(929,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>b</mi><mn>4</mn><mi>e</mi></math></mjx-assistive-mml></mjx-container>&#39; variants, the second source operand (%hi) is numbered {b7, b6, b5, b4}.</p><p>Modes:</p><ul><li><p><code>default</code>: Index mode - each nibble in <code>selector</code> selects a byte from the 8-byte pool</p></li><li><p><code>f4e</code> : Forward 4 extract - extracts 4 contiguous bytes starting from position in <code>selector</code></p></li><li><p><code>b4e</code> : Backward 4 extract - extracts 4 contiguous bytes in reverse order</p></li><li><p><code>rc8</code> : Replicate 8 - replicates the lower 8 bits across the 32-bit result</p></li><li><p><code>ecl</code> : Edge clamp left - clamps out-of-range indices to the leftmost valid byte</p></li><li><p><code>ecr</code> : Edge clamp right - clamps out-of-range indices to the rightmost valid byte</p></li><li><p><code>rc16</code> : Replicate 16 - replicates the lower 16 bits across the 32-bit result</p></li></ul><p>Depending on the 2 least significant bits of the %selector operand, the result of the permutation is defined as follows:</p><p>+––––––+––––––––+–––––––+ | Mode | %selector[1:0] | Output | +––––––+––––––––+–––––––+ | &#39;<mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.464ex;" xmlns="http://www.w3.org/2000/svg" width="3.43ex" height="2.059ex" role="img" focusable="false" viewBox="0 -705 1516 910" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z" style="stroke-width:3;"></path></g><g data-mml-node="mn" transform="translate(550,0)"><path data-c="34" d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(1050,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi><mn>4</mn><mi>e</mi></math></mjx-assistive-mml></mjx-container>&#39; | 0 | {3, 2, 1, 0} | | +––––––––+–––––––+ | | 1 | {4, 3, 2, 1} | | +––––––––+–––––––+ | | 2 | {5, 4, 3, 2} | | +––––––––+–––––––+ | | 3 | {6, 5, 4, 3} | +––––––+––––––––+–––––––+ | &#39;<mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.025ex;" xmlns="http://www.w3.org/2000/svg" width="3.156ex" height="1.595ex" role="img" focusable="false" viewBox="0 -694 1395 705" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z" style="stroke-width:3;"></path></g><g data-mml-node="mn" transform="translate(429,0)"><path data-c="34" d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(929,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>b</mi><mn>4</mn><mi>e</mi></math></mjx-assistive-mml></mjx-container>&#39; | 0 | {5, 6, 7, 0} | | +––––––––+–––––––+ | | 1 | {6, 7, 0, 1} | | +––––––––+–––––––+ | | 2 | {7, 0, 1, 2} | | +––––––––+–––––––+ | | 3 | {0, 1, 2, 3} | +––––––+––––––––+–––––––+ | &#39;<mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.05ex;" xmlns="http://www.w3.org/2000/svg" width="3.131ex" height="1.557ex" role="img" focusable="false" viewBox="0 -666 1384 688" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(451,0)"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z" style="stroke-width:3;"></path></g><g data-mml-node="mn" transform="translate(884,0)"><path data-c="38" d="M70 417T70 494T124 618T248 666Q319 666 374 624T429 515Q429 485 418 459T392 417T361 389T335 371T324 363L338 354Q352 344 366 334T382 323Q457 264 457 174Q457 95 399 37T249 -22Q159 -22 101 29T43 155Q43 263 172 335L154 348Q133 361 127 368Q70 417 70 494ZM286 386L292 390Q298 394 301 396T311 403T323 413T334 425T345 438T355 454T364 471T369 491T371 513Q371 556 342 586T275 624Q268 625 242 625Q201 625 165 599T128 534Q128 511 141 492T167 463T217 431Q224 426 228 424L286 386ZM250 21Q308 21 350 55T392 137Q392 154 387 169T375 194T353 216T330 234T301 253T274 270Q260 279 244 289T218 306L210 311Q204 311 181 294T133 239T107 157Q107 98 150 60T250 21Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>r</mi><mi>c</mi><mn>8</mn></math></mjx-assistive-mml></mjx-container>&#39; | 0 | {0, 0, 0, 0} | | +––––––––+–––––––+ | | 1 | {1, 1, 1, 1} | | +––––––––+–––––––+ | | 2 | {2, 2, 2, 2} | | +––––––––+–––––––+ | | 3 | {3, 3, 3, 3} | +––––––+––––––––+–––––––+ | &#39;<mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.025ex;" xmlns="http://www.w3.org/2000/svg" width="2.708ex" height="1.595ex" role="img" focusable="false" viewBox="0 -694 1197 705" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(466,0)"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(899,0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>e</mi><mi>c</mi><mi>l</mi></math></mjx-assistive-mml></mjx-container>&#39; | 0 | {3, 2, 1, 0} | | +––––––––+–––––––+ | | 1 | {3, 2, 1, 1} | | +––––––––+–––––––+ | | 2 | {3, 2, 2, 2} | | +––––––––+–––––––+ | | 3 | {3, 3, 3, 3} | +––––––+––––––––+–––––––+ | &#39;<mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.025ex;" xmlns="http://www.w3.org/2000/svg" width="3.054ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 1350 453" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(466,0)"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(899,0)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>e</mi><mi>c</mi><mi>r</mi></math></mjx-assistive-mml></mjx-container>&#39; | 0 | {0, 0, 0, 0} | | +––––––––+–––––––+ | | 1 | {1, 1, 1, 0} | | +––––––––+–––––––+ | | 2 | {2, 2, 1, 0} | | +––––––––+–––––––+ | | 3 | {3, 2, 1, 0} | +––––––+––––––––+–––––––+ | &#39;<mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.05ex;" xmlns="http://www.w3.org/2000/svg" width="4.262ex" height="1.557ex" role="img" focusable="false" viewBox="0 -666 1884 688" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(451,0)"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z" style="stroke-width:3;"></path></g><g data-mml-node="mn" transform="translate(884,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z" style="stroke-width:3;"></path><path data-c="36" d="M42 313Q42 476 123 571T303 666Q372 666 402 630T432 550Q432 525 418 510T379 495Q356 495 341 509T326 548Q326 592 373 601Q351 623 311 626Q240 626 194 566Q147 500 147 364L148 360Q153 366 156 373Q197 433 263 433H267Q313 433 348 414Q372 400 396 374T435 317Q456 268 456 210V192Q456 169 451 149Q440 90 387 34T253 -22Q225 -22 199 -14T143 16T92 75T56 172T42 313ZM257 397Q227 397 205 380T171 335T154 278T148 216Q148 133 160 97T198 39Q222 21 251 21Q302 21 329 59Q342 77 347 104T352 209Q352 289 347 316T329 361Q302 397 257 397Z" transform="translate(500,0)" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>r</mi><mi>c</mi><mn>16</mn></math></mjx-assistive-mml></mjx-container>&#39; | 0 | {1, 0, 1, 0} | | +––––––––+–––––––+ | | 1 | {3, 2, 3, 2} | | +––––––––+–––––––+ | | 2 | {1, 0, 1, 0} | | +––––––––+–––––––+ | | 3 | {3, 2, 3, 2} | +––––––+––––––––+–––––––+</p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#data-movement-and-conversion-instructions-prmt" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L4521-L4599" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.redux_sync-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}" href="#Reactant.MLIR.Dialects.nvvm.redux_sync-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.redux_sync</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>redux_sync</code></p><p><code>redux.sync</code> performs a reduction operation <code>kind</code> of the 32 bit source register across all non-exited threads in the membermask.</p><p>The <code>abs</code> and <code>nan</code> attributes can be used in the case of f32 input type, where the <code>abs</code> attribute causes the absolute value of the input to be used in the reduction operation, and the <code>nan</code> attribute causes the reduction operation to return NaN if any of the inputs to participating threads are NaN.</p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#parallel-synchronization-and-communication-instructions-redux-sync" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L4710-L4723" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.shfl_sync-NTuple{4, Reactant.MLIR.IR.Value}" href="#Reactant.MLIR.Dialects.nvvm.shfl_sync-NTuple{4, Reactant.MLIR.IR.Value}"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.shfl_sync</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>shfl_sync</code></p><p>The <code>shfl.sync</code> Op implements data shuffle within threads of a warp. The <code>thread_mask</code> denotes the threads participating in the Op where the bit position corresponds to a particular thread&#39;s laneid. The <code>offset</code> specifies a source lane or source lane offset (depending on <code>kind</code>). The <code>val</code> is the input value to be copied from the source. The <code>mask_and_clamp</code> contains two packed values specifying a mask for logically splitting warps into sub-segments and an upper bound for clamping the source lane index.</p><p>The <code>return_value_and_is_valid</code> unit attribute can be specified to indicate that the return value is a two-element struct, where the first element is the result value and the second element is a predicate indicating if the computed source lane index is valid.</p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#data-movement-and-conversion-instructions-shfl-sync" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L4774-L4792" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.st_bulk-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}" href="#Reactant.MLIR.Dialects.nvvm.st_bulk-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.st_bulk</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>st_bulk</code></p><p>Initializes a region of shared memory at the address given by <code>addr</code>. The <code>size</code> operand specifies the number of bytes to initialize and must be a multiple of 8. The <code>initVal</code> operand specifies the value to initialize the memory to. The only supported value is 0.</p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#data-movement-and-conversion-instructions-st-bulk" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L372-L382" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.stmatrix-Tuple{Reactant.MLIR.IR.Value, Vector{Reactant.MLIR.IR.Value}}" href="#Reactant.MLIR.Dialects.nvvm.stmatrix-Tuple{Reactant.MLIR.IR.Value, Vector{Reactant.MLIR.IR.Value}}"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.stmatrix</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>stmatrix</code></p><p>Collectively store one or more matrices across all threads in a warp to the location indicated by the address operand ptr in shared memory.</p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-store-instruction-stmatrix" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L4865-L4872" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.tcgen05_alloc-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}" href="#Reactant.MLIR.Dialects.nvvm.tcgen05_alloc-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.tcgen05_alloc</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>tcgen05_alloc</code></p><p>The <code>tcgen05.alloc</code> Op allocates tensor core memory for the amount specified by <code>nCols</code> and writes the destination address to the <code>addr</code> argument. The <code>nCols</code> operand specifies the number of columns to be allocated and it must be a power-of-two. <a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-memory-alloc-manage-instructions" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L4944-L4952" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.tcgen05_commit" href="#Reactant.MLIR.Dialects.nvvm.tcgen05_commit"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.tcgen05_commit</span></a> <span class="VPBadge info jlObjectType jlFunction"><!--[-->Function<!--]--></span></summary><p><code>tcgen05_commit</code></p><p>The <code>tcgen05.commit</code> makes the <em>mbarrier object</em>, specified by the operand <code>addr</code>, track the completion of all the prior async-tcgen05 operations initiated by the executing thread. The multicast variants allow signaling on the <em>mbarrier objects</em> of multiple CTAs within the cluster. Operand <code>multicastMask</code>, when present, specifies the destination CTAs in the cluster such that each bit position in the 16-bit <code>multicastMask</code> operand corresponds to the <code>nvvm.read.ptx.sreg.ctaid</code> of the destination CTA. <a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen-async-sync-operations-commit" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L4973-L4985" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.tcgen05_cp-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}" href="#Reactant.MLIR.Dialects.nvvm.tcgen05_cp-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.tcgen05_cp</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>tcgen05_cp</code></p><p>Instruction tcgen05.cp initiates an asynchronous copy operation from shared memory to the location specified by the address operand <code>taddr</code> in the Tensor Memory. The 64-bit register operand <code>smem_desc</code> specifies the matrix descriptor representing the source matrix in the shared memory that needs to be copied.</p><p><strong>Example</strong></p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>  nvvm.tcgen05.cp %taddr, %smem_desc {</span></span>
<span class="line"><span>    group = #nvvm.tcgen05_group&lt;cta_2&gt;,</span></span>
<span class="line"><span>    shape = #nvvm.tcgen05_cp_shape&lt;shape_64x128b&gt;,</span></span>
<span class="line"><span>    multicast = #nvvm.tcgen05_cp_multicast&lt;warpx2_01_23&gt;,</span></span>
<span class="line"><span>    srcFormat = #nvvm.tcgen05_cp_src_fmt&lt;b6x16_p32&gt;</span></span>
<span class="line"><span>  }</span></span></code></pre></div><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#tensorcore-5th-generation-instructions-tcgen05-cp" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L5012-L5031" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.tcgen05_dealloc-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}" href="#Reactant.MLIR.Dialects.nvvm.tcgen05_dealloc-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.tcgen05_dealloc</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>tcgen05_dealloc</code></p><p>The <code>tcgen05.dealloc</code> Op de-allocates the tensor core memory specified by <code>tmemAddr</code>, which must be from a previous tensor memory allocation. The <code>nCols</code> operand specifies the number of columns to be de-allocated, and it must be a power-of-two. <a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-memory-alloc-manage-instructions" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L5062-L5070" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.tcgen05_fence-Tuple{}" href="#Reactant.MLIR.Dialects.nvvm.tcgen05_fence-Tuple{}"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.tcgen05_fence</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>tcgen05_fence</code></p><p>The <code>tcgen05.fence&lt;before&gt;</code> orders all prior async tcgen05 operations with respect to the subsequent tcgen05 and execution ordering operations. The <code>tcgen05.fence&lt;after&gt;</code> orders all subsequent async tcgen05 operations with respect to the prior tcgen05 and execution ordering operations.</p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#tensorcore-5th-generation-instructions-tcgen05-fence" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L5091-L5100" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.tcgen05_ld" href="#Reactant.MLIR.Dialects.nvvm.tcgen05_ld"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.tcgen05_ld</span></a> <span class="VPBadge info jlObjectType jlFunction"><!--[-->Function<!--]--></span></summary><p><code>tcgen05_ld</code></p><p>Instruction <code>tcgen05.ld</code> asynchronously loads data from the Tensor Memory at the location specified by the 32-bit address operand <code>tmemAddr</code> into the destination register <code>res</code>, collectively across all threads of the warps.</p><p>The <code>shape</code> and the <code>num</code> attribute together determines the total dimension of the data which is loaded from the Tensor Memory. The <code>shape</code> attribute indicates the base dimension of data to be accessed as described in the Data Movement Shape. The <code>num</code> attribute indicates the repeat factor on the base dimension resulting in the total dimension of the data that is accessed.</p><p>The shape <code>16x32bx2</code> performs two accesses into Tensor Memory of the shape <code>16x32b</code>. The base address of the first access is specified by <code>tmemAddr</code> and the base address of the second access is specified by <code>tmemAddr + offset</code>, where <code>offset</code> is an immediate argument.</p><p>The unit attribute <code>pack</code> can be used to pack two 16-bit elements from adjacent columns into a single 32-bit element during the load.</p><p>The following table describes the size of the vector for various combinations of <code>num</code> and <code>shape</code> attributes:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>|=====================================================================|</span></span>
<span class="line"><span>| num/shape      |     16x32bx2/16x64b/32x32b |  16x128b   | 16x256b  |</span></span>
<span class="line"><span>|=====================================================================|</span></span>
<span class="line"><span>| x1             |          1                 |    2       |    4     |</span></span>
<span class="line"><span>| x2             |          2                 |    4       |    8     |</span></span>
<span class="line"><span>| x4             |          4                 |    8       |    16    |</span></span>
<span class="line"><span>| x8             |          8                 |    16      |    32    |</span></span>
<span class="line"><span>| x16            |          16                |    32      |    64    |</span></span>
<span class="line"><span>| x32            |          32                |    64      |    128   |</span></span>
<span class="line"><span>| x64            |          64                |    128     |    NA    |</span></span>
<span class="line"><span>| x128           |          128               |    NA      |    NA    |</span></span>
<span class="line"><span>|=====================================================================|</span></span></code></pre></div><p><strong>Example</strong></p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>  nvvm.tcgen05.ld %tmemAddr, %offset pack {</span></span>
<span class="line"><span>    shape = #nvvm.tcgen05_ldst_shape&lt;shape_16x32bx2&gt;,</span></span>
<span class="line"><span>  } : &lt;2xi32&gt;</span></span></code></pre></div><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-instructions-tcgen05-st" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L5120-L5167" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.tcgen05_ld_red" href="#Reactant.MLIR.Dialects.nvvm.tcgen05_ld_red"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.tcgen05_ld_red</span></a> <span class="VPBadge info jlObjectType jlFunction"><!--[-->Function<!--]--></span></summary><p><code>tcgen05_ld_red</code></p><p>Instruction <code>tcgen05.ld.red</code> asynchronously loads data from the Tensor Memory at the location specified by the 32-bit address operand <code>addr</code> into the destination register <code>data</code>, collectively across all threads of the warp. The operation also performs reduction operation specified by <code>op</code> on the loaded data across columns in each lane and stored into <code>redVal</code></p><p>The <code>shape</code> and the <code>num</code> attribute together determines the total dimension of the data which is loaded from the Tensor Memory. The <code>shape</code> attribute indicates the base dimension of data to be accessed as described in the Data Movement Shape. The <code>num</code> attribute indicates the repeat factor on the base dimension resulting in the total dimension of the data that is accessed.</p><p>The shape <code>16x32bx2</code> performs two accesses into Tensor Memory of the shape <code>16x32b</code>. The base address of the first access is specified by <code>addr</code> and the base address of the second access is specified by <code>addr + offset</code>, where <code>offset</code> is an immediate argument.</p><p>The following table describes the size of the vector for various combinations of <code>num</code> and <code>shape</code> attributes:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>|=============================================|</span></span>
<span class="line"><span>| num/shape      |     16x32bx2/32x32b        |</span></span>
<span class="line"><span>|=============================================|</span></span>
<span class="line"><span>| x2             |             2              |</span></span>
<span class="line"><span>| x4             |             4              |</span></span>
<span class="line"><span>| x8             |             8              |</span></span>
<span class="line"><span>| x16            |             16             |</span></span>
<span class="line"><span>| x32            |             32             |</span></span>
<span class="line"><span>| x64            |             64             |</span></span>
<span class="line"><span>| x128           |             128            |</span></span>
<span class="line"><span>|=============================================|</span></span></code></pre></div><p><strong>Example</strong></p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>  %data, %redval = nvvm.tcgen05.ld,red %addr, %offset {</span></span>
<span class="line"><span>    shape = #nvvm.tcgen05_ldst_shape&lt;shape_16x32bx2&gt;,</span></span>
<span class="line"><span>  } : &lt;2xi32&gt;, i32</span></span>
<span class="line"><span></span></span>
<span class="line"><span>  %data, %redval = nvvm.tcgen05.ld,red %addr {</span></span>
<span class="line"><span>    shape = #nvvm.tcgen05_ldst_shape&lt;shape_32x32b&gt;,</span></span>
<span class="line"><span>  } : &lt;2xf32&gt;, f32</span></span></code></pre></div><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-instructions-tcgen05-ld" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L5196-L5245" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.tcgen05_mma" href="#Reactant.MLIR.Dialects.nvvm.tcgen05_mma"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.tcgen05_mma</span></a> <span class="VPBadge info jlObjectType jlFunction"><!--[-->Function<!--]--></span></summary><p><code>tcgen05_mma</code></p><p>The <code>tcgen05.mma</code> operation is an asynchronous tensor core instruction that performs matrix multiplication, accumulation in a single fused operation. It targets 5th-generation tensor cores, providing developers with fine-grained control over execution and scheduling.</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>D = A * B + (D * 2^ -scaleInputD)    // if `scaleInputD` is provided</span></span>
<span class="line"><span>D = A * B                            // if `enableInputD` is false</span></span>
<span class="line"><span>D = A * B + D                        // otherwise</span></span></code></pre></div><p>where:</p><ul><li><p>A is an <code>M x K</code> matrix in tensor memory or described using shared memory descriptor</p></li><li><p>B is a <code>K x N</code> matrix described using shared memory descriptor</p></li><li><p>D is an <code>M x N</code> accumulator matrix in tensor memory</p></li></ul><p>The <code>shared memory descriptor</code> can be generated using <code>tcgen05.mma_smem_desc</code> Op</p><ul><li>idesc is a 32-bit value representing the <a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-instruction-descriptor" target="_blank" rel="noreferrer">Instruction Descriptor</a></li></ul><p>Optional Operands:</p><ul><li><p><code>scaleInputD</code> is an Immediate value operand used for scaling D matrix by 2 ^ (-scaleInputD). The valid range is [0, 15]</p></li><li><p><code>disableOutputLane</code> is a vector mask for selective output</p><ul><li><p>vector&lt;4 x i32&gt; when ctaGroup is CTA_1</p></li><li><p>vector&lt;8 x i32&gt; when ctaGroup is CTA_2</p></li></ul></li></ul><p>Required Attributes:</p><ul><li><p><code>kind</code> is a Tcgen05MMAKind attribute</p></li><li><p><code>ctaGroup</code> specifies CTA group configuration</p><ul><li><p>cta_1: MMA will be performed on the current thread&#39;s CTA</p></li><li><p>cta_2: MMA will be performed on the current thread and it&#39;s peer CTA</p></li></ul></li></ul><p>Default Attributes:</p><ul><li><p>collectorOp is a Tcgen05MMACollectorOp attribute with matrix A as the collector buffer</p></li><li><p><code>aShift</code> shifts the rows of the A matrix down by one row and can only be applied if A is in tensor memory</p></li></ul><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-mma-instructions-mma" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L5349-L5393" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.tcgen05_mma_block_scale-NTuple{7, Reactant.MLIR.IR.Value}" href="#Reactant.MLIR.Dialects.nvvm.tcgen05_mma_block_scale-NTuple{7, Reactant.MLIR.IR.Value}"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.tcgen05_mma_block_scale</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>tcgen05_mma_block_scale</code></p><p>The <code>tcgen05.mma.block_scale</code> operation is an asynchronous tensor core instruction that performs matrix multiplication, accumulation with block scaling in a single fused operation. It targets 5th-generation tensor cores, providing developers with fine-grained control over execution and scheduling.</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>D = (A * scale_a)  * (B * scale_b)`      // if `enableInputD` is false</span></span>
<span class="line"><span>D = (A * scale_a)  * (B * scale_b) + D`</span></span></code></pre></div><p>where:</p><ul><li><p>A is an M x (K / 2) matrix in tensor memory or described using shared memory descriptor</p></li><li><p>B is a K x N matrix described using shared memory descriptor</p></li><li><p>D is an M x N accumulator matrix in tensor memory</p></li><li><p><code>scale_a</code> and <code>scale_b</code> are matrices in tensor memory used to scale <code>A</code> and <code>B</code> respectively</p></li></ul><p>The <code>shared memory descriptor</code> can be generated using <code>tcgen05.mma_smem_desc</code> Op</p><ul><li><code>idesc</code> is a 32 bit value representing the <a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-instruction-descriptor" target="_blank" rel="noreferrer">Instruction Descriptor</a></li></ul><p>Required Attributes:</p><ul><li><p><code>kind</code> is a MMABlockScaleKind attribute</p></li><li><p><code>ctaGroup</code> specifies CTA group configuration</p><ul><li><p>cta_1: MMA will be performed on the current thread&#39;s CTA</p></li><li><p>cta_2: MMA will be performed on the current thread and it&#39;s peer CTA</p></li></ul></li></ul><p>Default Attributes:</p><ul><li>collectorOp is a Tcgen05MMACollectorOp attribute with matrix A as the collector buffer</li></ul><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-mma-instructions-mma" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L5278-L5312" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.tcgen05_mma_smem_desc-NTuple{6, Reactant.MLIR.IR.Value}" href="#Reactant.MLIR.Dialects.nvvm.tcgen05_mma_smem_desc-NTuple{6, Reactant.MLIR.IR.Value}"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.tcgen05_mma_smem_desc</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>tcgen05_mma_smem_desc</code></p><p>The <code>nvvm.tcgen05_mma_smem_desc</code> constructs a Shared Memory descriptor for tcgen05.mma. This descriptor is a 64-bit value which describes the properties of multiplicand matrix in shared memory including its location in the shared memory of the current CTA.</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>+-----------+------+------------------------------------------------------+</span></span>
<span class="line"><span>| Bit-field | Size | Description                                          |</span></span>
<span class="line"><span>+-----------+------+------------------------------------------------------+</span></span>
<span class="line"><span>| 0-13      | 14   | Matrix start address                                 |</span></span>
<span class="line"><span>| 14-15     | 2    | Reserved                                             |</span></span>
<span class="line"><span>| 16-29     | 14   | Leading dim relative-offset (or) absolute-address    |</span></span>
<span class="line"><span>| 30-31     | 2    | Reserved                                             |</span></span>
<span class="line"><span>| 32-45     | 14   | Stride dimension byte offset                         |</span></span>
<span class="line"><span>| 46-48     | 3    | Fixed constant value of 0b001                        |</span></span>
<span class="line"><span>| 49-51     | 3    | Matrix base offset                                   |</span></span>
<span class="line"><span>| 52        | 1    | Leading dimension stride mode:                       |</span></span>
<span class="line"><span>|           |      |   0: byte offset relative                            |</span></span>
<span class="line"><span>|           |      |   1: byte address absolute                           |</span></span>
<span class="line"><span>| 53-60     | 8    | Fixed constant value of 0xb00000000                  |</span></span>
<span class="line"><span>| 61-63     | 3    | Swizzling mode:                                      |</span></span>
<span class="line"><span>|           |      |   0: No swizzling                                    |</span></span>
<span class="line"><span>|           |      |   1: 128-Byte with 32B atomic swizzling              |</span></span>
<span class="line"><span>|           |      |   2: 128-Byte swizzling                              |</span></span>
<span class="line"><span>|           |      |   4: 64-Byte swizzling                               |</span></span>
<span class="line"><span>|           |      |   6: 32-Byte swizzling                               |</span></span>
<span class="line"><span>|           |      |   (Values 3, 5 and 7 are invalid)                    |</span></span>
<span class="line"><span>+-----------+------+------------------------------------------------------+</span></span></code></pre></div><p><strong>Example</strong></p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>  %desc = nvvm.tcgen05.mma_smem_desc (%startAddr, %leadingDimOffset, %strideDimOffset,</span></span>
<span class="line"><span>                                      %baseOffset, %leadingDimMode, %swizzleMode) : (i32, i32, i32, i8, i1, i8) -&gt; i64</span></span></code></pre></div><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-shared-memory-descriptor" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L5707-L5746" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.tcgen05_mma_sp" href="#Reactant.MLIR.Dialects.nvvm.tcgen05_mma_sp"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.tcgen05_mma_sp</span></a> <span class="VPBadge info jlObjectType jlFunction"><!--[-->Function<!--]--></span></summary><p><code>tcgen05_mma_sp</code></p><p>The <code>tcgen05.mma.sp</code> operation is an asynchronous tensor core instruction that performs matrix multiplication, accumulation with sparse <code>A</code> matrix in a single fused operation. It targets 5th-generation tensor cores, providing developers with fine-grained control over execution and scheduling.</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>D = A * B + (D * 2^ -scaleInputD)    // if `scaleInputD` is provided</span></span>
<span class="line"><span>D = A * B                            // if `enableInputD` is false</span></span>
<span class="line"><span>D = A * B + D                        // otherwise</span></span></code></pre></div><p>where:</p><ul><li><p>A is an <code>M x (K / 2)</code> matrix in tensor memory or described using shared memory descriptor</p></li><li><p>B is a <code>K x N</code> matrix described using shared memory descriptor</p></li><li><p>D is an <code>M x N</code> accumulator matrix in tensor memory</p></li><li><p>sparseMetadata located in tensor memory specifies the mapping of the <code>K / 2</code></p></li></ul><p>non-zero elements to the K elements before performing the MMA operation</p><p>Other attributes and operands are similar to that of tcgen05.mma Op</p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-mma-instructions-mma-sp" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L5501-L5525" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.tcgen05_mma_sp_block_scale-NTuple{8, Reactant.MLIR.IR.Value}" href="#Reactant.MLIR.Dialects.nvvm.tcgen05_mma_sp_block_scale-NTuple{8, Reactant.MLIR.IR.Value}"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.tcgen05_mma_sp_block_scale</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>tcgen05_mma_sp_block_scale</code></p><p>The <code>tcgen05.mma.sp.block_scale</code> operation is an asynchronous tensor core instruction that performs matrix multiplication, accumulation with block scaling, and sparse <code>A</code> matrix in a single fused operation. It targets 5th-generation tensor cores, providing developers with fine-grained control over execution, and scheduling.</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>D = (A * scale_a)  * (B * scale_b)      // if `enableInputD` is specified</span></span>
<span class="line"><span>D = (A * scale_a)  * (B * scale_b) + D  // otherwise</span></span></code></pre></div><p>where:</p><ul><li><p>A is an M x (K / 2) matrix in tensor memory or described using shared memory descriptor</p></li><li><p>B is a K x N matrix described using shared memory descriptor</p></li><li><p>D is an M x N accumulator matrix in tensor memory</p></li><li><p><code>scale_a</code> and <code>scale_b</code> are matrices in tensor memory used to scale <code>A</code> and <code>B</code> respectively</p></li></ul><p>Other attributes and operands are similar to that of tcgen05.mma.block_scale Op</p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-mma-instructions-mma-sp" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L5438-L5461" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.tcgen05_mma_ws" href="#Reactant.MLIR.Dialects.nvvm.tcgen05_mma_ws"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.tcgen05_mma_ws</span></a> <span class="VPBadge info jlObjectType jlFunction"><!--[-->Function<!--]--></span></summary><p><code>tcgen05_mma_ws</code></p><p>The <code>tcgen05.mma.ws</code> operation is an asynchronous tensor core instruction that performs weight stationary convolution matrix multiplication, accumulation in a single fused operation. It targets 5th-generation tensor cores, providing developers with fine-grained control over execution, and scheduling.</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>D = A * B`      // if `enableInputD` is false</span></span>
<span class="line"><span>D = A * B + D`  // otherwise</span></span></code></pre></div><p>where:</p><ul><li><p>A is an <code>M x K</code> matrix in tensor memory or described using shared memory descriptor</p></li><li><p>B is a <code>K x N</code> matrix described using shared memory descriptor</p></li><li><p>D is an <code>M x N</code> accumulator matrix in tensor memory</p></li></ul><p>The <code>shared memory descriptor</code> can be generated using <code>tcgen05.mma_smem_desc</code> Op</p><ul><li>idesc is a 32-bit value representing the <a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-instruction-descriptor" target="_blank" rel="noreferrer">Instruction Descriptor</a></li></ul><p>Optional Operands:</p><ul><li>zeroColMask is a 64 bit value representing the <a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-zero-column-mask-descriptor" target="_blank" rel="noreferrer">Zero-column mask descriptor</a></li></ul><p>Required Attributes:</p><ul><li><code>kind</code> is a Tcgen05MMAKind attribute</li></ul><p>Default Valued Attributes:</p><ul><li><p>collectorBBuffer specifies collector buffer for matrix B: b0 (default), b1, b2, b3</p></li><li><p>collectorOp is a Tcgen05MMACollectorOp attribute with matrix B as the collector buffer</p></li></ul><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-mma-instructions-mma-ws" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L5578-L5612" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.tcgen05_mma_ws_sp" href="#Reactant.MLIR.Dialects.nvvm.tcgen05_mma_ws_sp"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.tcgen05_mma_ws_sp</span></a> <span class="VPBadge info jlObjectType jlFunction"><!--[-->Function<!--]--></span></summary><p><code>tcgen05_mma_ws_sp</code></p><p>The <code>tcgen05.mma.ws.sp</code> operation is an asynchronous tensor core instruction that performs weight stationary convolution matrix multiplication, accumulation with sparse <code>A</code> matrix in a single fused operation. It targets 5th-generation tensor cores, providing developers with fine-grained control over execution, and scheduling.</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>D = A * B`      // if `enableInputD` is false</span></span>
<span class="line"><span>D = A * B + D`  // otherwise</span></span></code></pre></div><p>where:</p><ul><li><p>A is an M x (K / 2) matrix in memory or descriptor format</p></li><li><p>B is a K x N matrix</p></li><li><p>D is an M x N accumulator matrix</p></li><li><p>sparseMetadata located in tensor memory specifies the mapping of the <code>K / 2</code></p></li></ul><p>non-zero elements to the K elements before performing the MMA operation</p><p>Other attributes and operands are similar to that of tcgen05.mma.ws Op</p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-mma-instructions-mma-ws-sp" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L5647-L5671" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.tcgen05_relinquish_alloc_permit-Tuple{}" href="#Reactant.MLIR.Dialects.nvvm.tcgen05_relinquish_alloc_permit-Tuple{}"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.tcgen05_relinquish_alloc_permit</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>tcgen05_relinquish_alloc_permit</code></p><p>The <code>tcgen05.relinquish_alloc_permit</code> Op specifies that the CTA of the executing thread is relinquishing the right to allocate Tensor Memory. So, it is illegal for a CTA to perform <code>tcgen05.alloc</code> after any of its constituent threads execute <code>tcgen05.relinquish_alloc_permit</code>. <a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-memory-alloc-manage-instructions" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L5782-L5790" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.tcgen05_shift-Tuple{Reactant.MLIR.IR.Value}" href="#Reactant.MLIR.Dialects.nvvm.tcgen05_shift-Tuple{Reactant.MLIR.IR.Value}"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.tcgen05_shift</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>tcgen05_shift</code></p><p>The <code>tcgen05.shift</code> is an asynchronous instruction which initiates the shifting of 32-byte elements downwards across all the rows, except the last, by one row. The operand <code>taddr</code> specifies the base address of the matrix in Tensor Memory whose rows must be down shifted.</p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-instructions-tcgen05-shift" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L5811-L5820" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.tcgen05_st" href="#Reactant.MLIR.Dialects.nvvm.tcgen05_st"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.tcgen05_st</span></a> <span class="VPBadge info jlObjectType jlFunction"><!--[-->Function<!--]--></span></summary><p><code>tcgen05_st</code></p><p>Instruction <code>tcgen05.st</code> asynchronously stores data from the source register <code>r</code> into the Tensor Memory at the location specified by the 32-bit address operand <code>tmemAddr</code>, collectively across all threads of the warps.</p><p>The <code>shape</code> and the <code>num</code> attribute together determines the total dimension of the data which is stored to the Tensor Memory. The <code>shape</code> indicates the base dimension of data to be accessed. The <code>num</code> attribute indicates the repeat factor on the base dimension resulting in the total dimension of the data that is accessed.</p><p>The shape <code>16x32bx2</code> performs two accesses into Tensor Memory of the shape <code>16x32b</code>. The base address of the first access is specified by <code>tmemAddr</code> and the base address of the second access is specified by <code>tmemAddr + offset</code>, where <code>offset</code> is an immediate argument.</p><p>The unit attribute <code>unpack</code> can be used to unpack a 32-bit element in the register into two 16-bit elements and store them in adjacent columns.</p><p>The following table describes the size of the vector for various combinations of <code>num</code> and <code>shape</code> attributes:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>|=====================================================================|</span></span>
<span class="line"><span>| num/shape      |     16x32bx2/16x64b/32x32b |  16x128b   | 16x256b  |</span></span>
<span class="line"><span>|=====================================================================|</span></span>
<span class="line"><span>| x1             |          1                 |    2       |    4     |</span></span>
<span class="line"><span>| x2             |          2                 |    4       |    8     |</span></span>
<span class="line"><span>| x4             |          4                 |    8       |    16    |</span></span>
<span class="line"><span>| x8             |          8                 |    16      |    32    |</span></span>
<span class="line"><span>| x16            |          16                |    32      |    64    |</span></span>
<span class="line"><span>| x32            |          32                |    64      |    128   |</span></span>
<span class="line"><span>| x64            |          64                |    128     |    NA    |</span></span>
<span class="line"><span>| x128           |          128               |    NA      |    NA    |</span></span>
<span class="line"><span>|=====================================================================|</span></span></code></pre></div><p><strong>Example</strong></p><div class="language-mlir vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">mlir</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>  nvvm.tcgen05.st %tmemAddr, %val, %offset unpack {</span></span>
<span class="line"><span>    shape = #nvvm.tcgen05_ldst_shape&lt;shape_16x32bx2&gt;,</span></span>
<span class="line"><span>  } : &lt;2xi32&gt;</span></span></code></pre></div><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-instructions-tcgen05-st" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L5841-L5887" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.tcgen05_wait-Tuple{}" href="#Reactant.MLIR.Dialects.nvvm.tcgen05_wait-Tuple{}"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.tcgen05_wait</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>tcgen05_wait</code></p><p>The <code>tcgen05.wait&lt;load&gt;</code> causes the executing thread to block until all prior <code>tcgen05.ld</code> operations issued by the executing thread have completed. Similarly, the <code>tcgen05.wait&lt;store&gt;</code> causes the executing thread to block until all prior <code>tcgen05.st</code> operations issued by the executing thread have completed. <a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-instructions-tcgen05-wait" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L5916-L5925" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.tensormap_replace" href="#Reactant.MLIR.Dialects.nvvm.tensormap_replace"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.tensormap_replace</span></a> <span class="VPBadge info jlObjectType jlFunction"><!--[-->Function<!--]--></span></summary><p><code>tensormap_replace</code></p><p>The <code>nvvm.tensormap.replace</code> replaces the specified field of the tensor-map object at the location specified by <code>addr</code> with a new value (specified by <code>new_value</code> or <code>new_value_attr</code>).</p><p>The <code>field</code> argument specifies the field of the tensor-map object to replace.</p><p><code>new_value</code> is an <code>i32</code>/<code>i64</code> argument that specifies the new value to replace the <code>field</code> with for the <code>global_address</code>, <code>rank</code>, <code>box_dim</code>, <code>global_dim</code>, <code>global_stride</code>, and <code>element_stride</code> fields. It must be an <code>i64</code> for the <code>global_address</code> and <code>global_stride</code> fields and <code>i32</code> for the remaining fields.</p><p>For <code>rank</code>, <code>new_value</code> must be one less than the desired tensor rank as this field uses zero-based numbering.</p><p><code>new_value_attr</code> is an attribute that specifies the new value to replace the <code>field</code> with for the <code>elemtype</code>, <code>interleave_layout</code>, <code>swizzle_mode</code>, <code>swizzle_atomicity</code>, and <code>fill_mode</code> fields. It takes the place of <code>new_value</code> for these fields. It must be a valid attribute corresponding to the <code>field</code> type.</p><p>The ordinal <code>ord</code> is an immediate integer argument that specifies the ordinal of the <code>field</code> across the tensor which needs to be replaced and is required only for the <code>box_dim</code>, <code>global_dim</code>, <code>global_stride</code>, and <code>element_stride</code> fields.</p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#data-movement-and-conversion-instructions-tensormap-replace" target="_blank" rel="noreferrer">For more information, see PTX ISA.</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L5945-L5976" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.vote_sync-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}" href="#Reactant.MLIR.Dialects.nvvm.vote_sync-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.vote_sync</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>vote_sync</code></p><p>The <code>vote.sync</code> op will cause executing thread to wait until all non-exited threads corresponding to membermask have executed <code>vote.sync</code> with the same qualifiers and same membermask value before resuming execution.</p><p>The vote operation kinds are:</p><ul><li><p><code>any</code>: True if source predicate is True for some thread in membermask.</p></li><li><p><code>all</code>: True if source predicate is True for all non-exited threads in membermask.</p></li><li><p><code>uni</code>: True if source predicate has the same value in all non-exited threads in membermask.</p></li><li><p><code>ballot</code>: In the ballot form, the destination result is a 32 bit integer. In this form, the predicate from each thread in membermask are copied into the corresponding bit position of the result, where the bit position corresponds to the thread&#39;s lane id.</p></li></ul><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#parallel-synchronization-and-communication-instructions-vote-sync" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L6086-L6105" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.wgmma_commit_group_sync_aligned-Tuple{}" href="#Reactant.MLIR.Dialects.nvvm.wgmma_commit_group_sync_aligned-Tuple{}"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.wgmma_commit_group_sync_aligned</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>wgmma_commit_group_sync_aligned</code></p><p>Commits all prior uncommitted warpgroup level matrix multiplication operations.</p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#asynchronous-warpgroup-level-matrix-instructions-wgmma-commit-group" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L6322-L6328" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.wgmma_fence_aligned-Tuple{}" href="#Reactant.MLIR.Dialects.nvvm.wgmma_fence_aligned-Tuple{}"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.wgmma_fence_aligned</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>wgmma_fence_aligned</code></p><p>Enforce an ordering of register accesses between warpgroup level matrix multiplication and other operations.</p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#asynchronous-warpgroup-level-matrix-instructions-wgmma-fence" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L6295-L6302" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.wgmma_mma_async-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}" href="#Reactant.MLIR.Dialects.nvvm.wgmma_mma_async-Tuple{Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value, Reactant.MLIR.IR.Value}"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.wgmma_mma_async</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>wgmma_mma_async</code></p><p>The warpgroup (128 threads) level matrix multiply and accumulate operation has either of the following forms, where matrix D is called accumulator: D = A * B + D D = A * B, where the input from accumulator D is disabled.</p><p>Supported shapes:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>|--------------|--------------|------------|--------------|---------------|</span></span>
<span class="line"><span>|              |              |            |              |f16+=e4m3*e4m3 |</span></span>
<span class="line"><span>|              |              |            |              |f16+=e5m2*e5m2 |</span></span>
<span class="line"><span>|f32+=tf32*tf32|f16+=f16 *f16 | s32+=s8*s8 |s32 += b1 * b1|f16+=e5m2*e4m3 |</span></span>
<span class="line"><span>|              |f32+=f16 *f16 | s32+=u8*u8 |              |f16+=e4m3*e5m2 |</span></span>
<span class="line"><span>|              |f32+=bf16*bf16| s32+=u8*u8 |              |f16+=e4m3*e5m2 |</span></span>
<span class="line"><span>|              |f32+=bf16*bf16| s32+=s8*u8 |              |f32+=e4m3*e4m3 |</span></span>
<span class="line"><span>|              |              | s32+=u8*s8 |              |f32+=e5m2*e5m2 |</span></span>
<span class="line"><span>|              |              |            |              |f32+=e4m3*e5m2 |</span></span>
<span class="line"><span>|              |              |            |              |f32+=e4m3*e5m2 |</span></span>
<span class="line"><span>|--------------|--------------|------------|--------------|---------------|</span></span>
<span class="line"><span>|   .m64n8k8   |  .m64n8k16   | .m64n8k32  | .m64n8k256   | .m64n8k32     |</span></span>
<span class="line"><span>|   .m64n16k8  |  .m64n16k16  | .m64n16k32 | .m64n16k256  | .m64n16k32    |</span></span>
<span class="line"><span>|   .m64n24k8  |  .m64n24k16  | .m64n24k32 | .m64n24k256  | .m64n24k32    |</span></span>
<span class="line"><span>|   .m64n32k8  |  .m64n32k16  | .m64n32k32 | .m64n32k256  | .m64n32k32    |</span></span>
<span class="line"><span>|   .m64n40k8  |  .m64n40k16  | .m64n48k32 | .m64n48k256  | .m64n40k32    |</span></span>
<span class="line"><span>|   .m64n48k8  |  .m64n48k16  | .m64n64k32 | .m64n64k256  | .m64n48k32    |</span></span>
<span class="line"><span>|   .m64n56k8  |  .m64n56k16  | .m64n80k32 | .m64n80k256  | .m64n56k32    |</span></span>
<span class="line"><span>|   .m64n64k8  |  .m64n64k16  | .m64n96k32 | .m64n96k256  | .m64n64k32    |</span></span>
<span class="line"><span>|   .m64n72k8  |  .m64n72k16  | .m64n112k32| .m64n112k256 | .m64n72k32    |</span></span>
<span class="line"><span>|   .m64n80k8  |  .m64n80k16  | .m64n128k32| .m64n128k256 | .m64n80k32    |</span></span>
<span class="line"><span>|   .m64n88k8  |  .m64n88k16  | .m64n144k32| .m64n144k256 | .m64n88k32    |</span></span>
<span class="line"><span>|   .m64n96k8  |  .m64n96k16  | .m64n160k32| .m64n160k256 | .m64n96k32    |</span></span>
<span class="line"><span>|   .m64n104k8 |  .m64n104k16 | .m64n176k32| .m64n176k256 | .m64n104k32   |</span></span>
<span class="line"><span>|   .m64n112k8 |  .m64n112k16 | .m64n192k32| .m64n192k256 | .m64n112k32   |</span></span>
<span class="line"><span>|   .m64n120k8 |  .m64n120k16 | .m64n208k32| .m64n208k256 | .m64n120k32   |</span></span>
<span class="line"><span>|   .m64n128k8 |  .m64n128k16 | .m64n224k32| .m64n224k256 | .m64n128k32   |</span></span>
<span class="line"><span>|   .m64n136k8 |  .m64n136k16 | .m64n240k32| .m64n240k256 | .m64n136k32   |</span></span>
<span class="line"><span>|   .m64n144k8 |  .m64n144k16 | .m64n256k32| .m64n256k256 | .m64n144k32   |</span></span>
<span class="line"><span>|   .m64n152k8 |  .m64n152k16 |            |              | .m64n152k32   |</span></span>
<span class="line"><span>|   .m64n160k8 |  .m64n160k16 |            |              | .m64n160k32   |</span></span>
<span class="line"><span>|   .m64n168k8 |  .m64n168k16 |            |              | .m64n168k32   |</span></span>
<span class="line"><span>|   .m64n176k8 |  .m64n176k16 |            |              | .m64n176k32   |</span></span>
<span class="line"><span>|   .m64n184k8 |  .m64n184k16 |            |              | .m64n184k32   |</span></span>
<span class="line"><span>|   .m64n192k8 |  .m64n192k16 |            |              | .m64n192k32   |</span></span>
<span class="line"><span>|   .m64n200k8 |  .m64n200k16 |            |              | .m64n200k32   |</span></span>
<span class="line"><span>|   .m64n208k8 |  .m64n208k16 |            |              | .m64n208k32   |</span></span>
<span class="line"><span>|   .m64n216k8 |  .m64n216k16 |            |              | .m64n216k32   |</span></span>
<span class="line"><span>|   .m64n224k8 |  .m64n224k16 |            |              | .m64n224k32   |</span></span>
<span class="line"><span>|   .m64n232k8 |  .m64n232k16 |            |              | .m64n232k32   |</span></span>
<span class="line"><span>|   .m64n240k8 |  .m64n240k16 |            |              | .m64n240k32   |</span></span>
<span class="line"><span>|   .m64n248k8 |  .m64n248k16 |            |              | .m64n248k32   |</span></span>
<span class="line"><span>|   .m64n256k8 |  .m64n256k16 |            |              | .m64n256k32   |</span></span>
<span class="line"><span>|--------------|--------------|------------|--------------|---------------|</span></span></code></pre></div><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#asynchronous-warpgroup-level-matrix-instructions" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L6348-L6406" target="_blank" rel="noreferrer">source</a><!--]--></span></details><details class="jldocstring custom-block"><summary><a id="Reactant.MLIR.Dialects.nvvm.wgmma_wait_group_sync_aligned-Tuple{}" href="#Reactant.MLIR.Dialects.nvvm.wgmma_wait_group_sync_aligned-Tuple{}"><span class="jlbinding">Reactant.MLIR.Dialects.nvvm.wgmma_wait_group_sync_aligned</span></a> <span class="VPBadge info jlObjectType jlMethod"><!--[-->Method<!--]--></span></summary><p><code>wgmma_wait_group_sync_aligned</code></p><p>Signal the completion of a preceding warpgroup operation.</p><p><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#asynchronous-warpgroup-level-matrix-instructions-wgmma-wait-group" target="_blank" rel="noreferrer">For more information, see PTX ISA</a></p><span class="VPBadge info source-link"><!--[--><a href="https://github.com/EnzymeAD/Reactant.jl/blob/a1f8f2d8b7f5714b8f76eb9d8f54547a03c6c66d/src/mlir/Dialects/Nvvm.jl#L6453-L6459" target="_blank" rel="noreferrer">source</a><!--]--></span></details></div></div></main><footer class="VPDocFooter" data-v-83890dd9 data-v-4f9813fa><!--[--><!--]--><div class="edit-info" data-v-4f9813fa><div class="edit-link" data-v-4f9813fa><a class="VPLink link vp-external-link-icon no-icon edit-link-button" href="https://github.com/EnzymeAD/Reactant.jl/edit/main/docs/src/api/dialects/nvvm.md" target="_blank" rel="noreferrer" data-v-4f9813fa><!--[--><span class="vpi-square-pen edit-link-icon" data-v-4f9813fa></span> Edit this page on GitHub<!--]--></a></div><!----></div><nav class="prev-next" aria-labelledby="doc-footer-aria-label" data-v-4f9813fa><span class="visually-hidden" id="doc-footer-aria-label" data-v-4f9813fa>Pager</span><div class="pager" data-v-4f9813fa><a class="VPLink link pager-link prev" href="/Reactant.jl/previews/PR2506/api/dialects/mosaicgpu" data-v-4f9813fa><!--[--><span class="desc" data-v-4f9813fa>Previous page</span><span class="title" data-v-4f9813fa>Mosaic GPU</span><!--]--></a></div><div class="pager" data-v-4f9813fa><a class="VPLink link pager-link next" href="/Reactant.jl/previews/PR2506/api/dialects/shape" data-v-4f9813fa><!--[--><span class="desc" data-v-4f9813fa>Next page</span><span class="title" data-v-4f9813fa>Shape</span><!--]--></a></div></nav></footer><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><footer class="VPFooter has-sidebar" data-v-a9a9e638 data-v-c970a860><div class="container" data-v-c970a860><p class="message" data-v-c970a860>Made with <a href="https://documenter.juliadocs.org/stable/" target="_blank"><strong>Documenter.jl</strong></a>, <a href="https://vitepress.dev" target="_blank"><strong>VitePress</strong></a> and <a href="https://luxdl.github.io/DocumenterVitepress.jl/stable" target="_blank"><strong>DocumenterVitepress.jl</strong></a><br>Released under the MIT License. Powered by the <a href="https://www.julialang.org">Julia Programming Language</a>.<br></p><p class="copyright" data-v-c970a860>© Copyright 2026 Reactant Development Team.</p></div></footer><!--[--><!--]--></div></div>
    <script>window.__VP_HASH_MAP__=JSON.parse("{\"api_api.md\":\"BUf5o-ts\",\"api_config.md\":\"BT4g_Vly\",\"api_dialects_affine.md\":\"CAOdNTJW\",\"api_dialects_arith.md\":\"BJU82DDp\",\"api_dialects_builtin.md\":\"CoQZ6_oP\",\"api_dialects_chlo.md\":\"MzWf6f9d\",\"api_dialects_complex.md\":\"U2HaPU1Z\",\"api_dialects_cuda_tile.md\":\"9COhWkLZ\",\"api_dialects_enzyme.md\":\"ByMkbPSw\",\"api_dialects_enzymexla.md\":\"zAXNCrw2\",\"api_dialects_func.md\":\"Dj0K83-3\",\"api_dialects_gpu.md\":\"CkgDInVT\",\"api_dialects_llvm.md\":\"Bd8vNo-e\",\"api_dialects_memref.md\":\"BfMp78z1\",\"api_dialects_mosaicgpu.md\":\"NiazfFgP\",\"api_dialects_mpi.md\":\"Bc7QeYvJ\",\"api_dialects_nvvm.md\":\"CQQqgKi5\",\"api_dialects_shape.md\":\"BbN5cgKW\",\"api_dialects_shardy.md\":\"Bxqij0OC\",\"api_dialects_sparsetensor.md\":\"CDpxDNNO\",\"api_dialects_stablehlo.md\":\"CTPlj9hz\",\"api_dialects_tensor.md\":\"n2mWPOi8\",\"api_dialects_tpu.md\":\"I6a0kx2Y\",\"api_dialects_triton.md\":\"B0AjFfTd\",\"api_dialects_tritonext.md\":\"WSKx3h5g\",\"api_dialects_vhlo.md\":\"86mrs3z1\",\"api_internal.md\":\"B0kv3Ks9\",\"api_mlirc.md\":\"nF77o9d6\",\"api_ops.md\":\"CxCBDzCF\",\"api_serialization.md\":\"DJSLIzvV\",\"api_sharding.md\":\"Bu1qZ9_n\",\"api_xla.md\":\"BiaZkQ94\",\"index.md\":\"B69u3cpW\",\"introduction_configuration.md\":\"CklhtBzW\",\"introduction_faqs.md\":\"DKCDcX5N\",\"introduction_index.md\":\"BfBweon0\",\"tutorials_automatic-differentiation.md\":\"Dq0h5_z0\",\"tutorials_control-flow.md\":\"Cl6qMaWT\",\"tutorials_debugging.md\":\"BBOGSlvY\",\"tutorials_index.md\":\"ChjTvDUP\",\"tutorials_kernels.md\":\"C3c7j76A\",\"tutorials_local-build.md\":\"NFMZlCJy\",\"tutorials_multihost.md\":\"pvijEqq_\",\"tutorials_partial-evaluation.md\":\"D2mMD9VE\",\"tutorials_persistent_compile_cache.md\":\"BLShNJw2\",\"tutorials_profiling.md\":\"ChaGWeNf\",\"tutorials_raising.md\":\"BHniTd8T\",\"tutorials_sharding.md\":\"CH-Hyw1K\"}");window.__VP_SITE_DATA__=JSON.parse("{\"lang\":\"en-US\",\"dir\":\"ltr\",\"title\":\"Reactant.jl\",\"description\":\"Documentation for Reactant.jl\",\"base\":\"/Reactant.jl/previews/PR2506/\",\"head\":[],\"router\":{\"prefetchLinks\":true},\"appearance\":true,\"themeConfig\":{\"outline\":\"deep\",\"logo\":{\"light\":\"/logo.svg\",\"dark\":\"/logo.svg\"},\"search\":{\"provider\":\"local\",\"options\":{\"detailedView\":true}},\"nav\":[{\"text\":\"Home\",\"link\":\"/\"},{\"text\":\"Getting Started\",\"items\":[{\"text\":\"Introduction\",\"link\":\"/introduction\"},{\"text\":\"Configuration\",\"link\":\"/introduction/configuration\"},{\"text\":\"FAQs\",\"link\":\"/introduction/FAQs\"}]},{\"text\":\"Benchmarks\",\"link\":\"https://enzymead.github.io/Reactant.jl/benchmarks/\"},{\"text\":\"Tutorials\",\"items\":[{\"text\":\"Overview\",\"link\":\"/tutorials/\"},{\"text\":\"Partial Evaluation\",\"link\":\"/tutorials/partial-evaluation\"},{\"text\":\"Control Flow\",\"link\":\"/tutorials/control-flow\"},{\"text\":\"Automatic Differentiation\",\"link\":\"/tutorials/automatic-differentiation\"},{\"text\":\"Sharding\",\"link\":\"/tutorials/sharding\"},{\"text\":\"Profiling\",\"link\":\"/tutorials/profiling\"},{\"text\":\"Multi-Host Environments\",\"link\":\"/tutorials/multihost\"},{\"text\":\"Local build\",\"link\":\"/tutorials/local-build\"},{\"text\":\"Persistent Compilation Cache\",\"link\":\"/tutorials/persistent_compile_cache\"},{\"text\":\"Raising\",\"link\":\"/tutorials/raising\"},{\"text\":\"Computational kernels\",\"link\":\"/tutorials/kernels\"},{\"text\":\"Debugging compilation errors\",\"link\":\"/tutorials/debugging\"}]},{\"text\":\"API\",\"items\":[{\"text\":\"Core Reactant API\",\"link\":\"/api/api\"},{\"text\":\"Sharding\",\"link\":\"/api/sharding\"},{\"text\":\"Serialization\",\"link\":\"/api/serialization\"},{\"text\":\"Ops\",\"link\":\"/api/ops\"},{\"text\":\"Configuration\",\"link\":\"/api/config\"},{\"text\":\"MLIR Dialects\",\"items\":[{\"text\":\"ArithOps\",\"link\":\"/api/dialects/arith\"},{\"text\":\"Affine\",\"link\":\"/api/dialects/affine\"},{\"text\":\"Builtin\",\"link\":\"/api/dialects/builtin\"},{\"text\":\"Chlo\",\"link\":\"/api/dialects/chlo\"},{\"text\":\"Complex\",\"link\":\"/api/dialects/complex\"},{\"text\":\"CUDA Tile\",\"link\":\"/api/dialects/cuda_tile\"},{\"text\":\"Enzyme\",\"link\":\"/api/dialects/enzyme\"},{\"text\":\"EnzymeXLA\",\"link\":\"/api/dialects/enzymexla\"},{\"text\":\"Func\",\"link\":\"/api/dialects/func\"},{\"text\":\"GPU\",\"link\":\"/api/dialects/gpu\"},{\"text\":\"LLVM\",\"link\":\"/api/dialects/llvm\"},{\"text\":\"MPI\",\"link\":\"/api/dialects/mpi\"},{\"text\":\"MemRef\",\"link\":\"/api/dialects/memref\"},{\"text\":\"Mosaic GPU\",\"link\":\"/api/dialects/mosaicgpu\"},{\"text\":\"NVVM\",\"link\":\"/api/dialects/nvvm\"},{\"text\":\"Shape\",\"link\":\"/api/dialects/shape\"},{\"text\":\"Shardy\",\"link\":\"/api/dialects/shardy\"},{\"text\":\"SparseTensor\",\"link\":\"/api/dialects/sparsetensor\"},{\"text\":\"StableHLO\",\"link\":\"/api/dialects/stablehlo\"},{\"text\":\"Tensor\",\"link\":\"/api/dialects/tensor\"},{\"text\":\"Triton\",\"link\":\"/api/dialects/triton\"},{\"text\":\"TritonExt\",\"link\":\"/api/dialects/tritonext\"},{\"text\":\"TPU\",\"link\":\"/api/dialects/tpu\"},{\"text\":\"VHLO\",\"link\":\"/api/dialects/vhlo\"}]},{\"text\":\"Low-Level API\",\"items\":[{\"text\":\"MLIR API\",\"link\":\"/api/mlirc\"},{\"text\":\"XLA\",\"link\":\"/api/xla\"}]},{\"text\":\"Internal API\",\"link\":\"/api/internal\"}]},{\"component\":\"VersionPicker\"}],\"sidebar\":{\"/introduction/\":[{\"text\":\"Getting Started\",\"collapsed\":false,\"items\":[{\"text\":\"Introduction\",\"link\":\"/introduction\"},{\"text\":\"Configuration\",\"link\":\"/introduction/configuration\"},{\"text\":\"FAQs\",\"link\":\"/introduction/FAQs\"}]}],\"/tutorials/\":[{\"text\":\"Tutorials\",\"collapsed\":false,\"items\":[{\"text\":\"Overview\",\"link\":\"/tutorials/\"},{\"text\":\"Partial Evaluation\",\"link\":\"/tutorials/partial-evaluation\"},{\"text\":\"Control Flow\",\"link\":\"/tutorials/control-flow\"},{\"text\":\"Automatic Differentiation\",\"link\":\"/tutorials/automatic-differentiation\"},{\"text\":\"Sharding\",\"link\":\"/tutorials/sharding\"},{\"text\":\"Profiling\",\"link\":\"/tutorials/profiling\"},{\"text\":\"Multi-Host Environments\",\"link\":\"/tutorials/multihost\"},{\"text\":\"Local build\",\"link\":\"/tutorials/local-build\"},{\"text\":\"Persistent Compilation Cache\",\"link\":\"/tutorials/persistent_compile_cache\"},{\"text\":\"Raising\",\"link\":\"/tutorials/raising\"},{\"text\":\"Computational kernels\",\"link\":\"/tutorials/kernels\"},{\"text\":\"Debugging compilation errors\",\"link\":\"/tutorials/debugging\"}]}],\"/api/\":[{\"text\":\"API Reference\",\"collapsed\":false,\"items\":[{\"text\":\"Reactant API\",\"link\":\"/api/api\"},{\"text\":\"Sharding\",\"link\":\"/api/sharding\"},{\"text\":\"Serialization\",\"link\":\"/api/serialization\"},{\"text\":\"Ops\",\"link\":\"/api/ops\"},{\"text\":\"Configuration\",\"link\":\"/api/config\"},{\"text\":\"MLIR Dialects\",\"collapsed\":false,\"items\":[{\"text\":\"ArithOps\",\"link\":\"/api/dialects/arith\"},{\"text\":\"Affine\",\"link\":\"/api/dialects/affine\"},{\"text\":\"Builtin\",\"link\":\"/api/dialects/builtin\"},{\"text\":\"Chlo\",\"link\":\"/api/dialects/chlo\"},{\"text\":\"Complex\",\"link\":\"/api/dialects/complex\"},{\"text\":\"CUDA Tile\",\"link\":\"/api/dialects/cuda_tile\"},{\"text\":\"Enzyme\",\"link\":\"/api/dialects/enzyme\"},{\"text\":\"EnzymeXLA\",\"link\":\"/api/dialects/enzymexla\"},{\"text\":\"Func\",\"link\":\"/api/dialects/func\"},{\"text\":\"GPU\",\"link\":\"/api/dialects/gpu\"},{\"text\":\"LLVM\",\"link\":\"/api/dialects/llvm\"},{\"text\":\"MPI\",\"link\":\"/api/dialects/mpi\"},{\"text\":\"MemRef\",\"link\":\"/api/dialects/memref\"},{\"text\":\"Mosaic GPU\",\"link\":\"/api/dialects/mosaicgpu\"},{\"text\":\"NVVM\",\"link\":\"/api/dialects/nvvm\"},{\"text\":\"Shape\",\"link\":\"/api/dialects/shape\"},{\"text\":\"Shardy\",\"link\":\"/api/dialects/shardy\"},{\"text\":\"SparseTensor\",\"link\":\"/api/dialects/sparsetensor\"},{\"text\":\"StableHLO\",\"link\":\"/api/dialects/stablehlo\"},{\"text\":\"Tensor\",\"link\":\"/api/dialects/tensor\"},{\"text\":\"Triton\",\"link\":\"/api/dialects/triton\"},{\"text\":\"TritonExt\",\"link\":\"/api/dialects/tritonext\"},{\"text\":\"TPU\",\"link\":\"/api/dialects/tpu\"},{\"text\":\"VHLO\",\"link\":\"/api/dialects/vhlo\"}]},{\"text\":\"Low-Level API\",\"collapsed\":false,\"items\":[{\"text\":\"MLIR API\",\"link\":\"/api/mlirc\"},{\"text\":\"XLA\",\"link\":\"/api/xla\"}]},{\"text\":\"Internal API\",\"link\":\"/api/internal\"}]}]},\"editLink\":{\"pattern\":\"https://github.com/EnzymeAD/Reactant.jl/edit/main/docs/src/:path\",\"text\":\"Edit this page on GitHub\"},\"socialLinks\":[{\"icon\":\"slack\",\"link\":\"https://julialang.org/slack/\"}],\"footer\":{\"message\":\"Made with <a href=\\\"https://documenter.juliadocs.org/stable/\\\" target=\\\"_blank\\\"><strong>Documenter.jl</strong></a>, <a href=\\\"https://vitepress.dev\\\" target=\\\"_blank\\\"><strong>VitePress</strong></a> and <a href=\\\"https://luxdl.github.io/DocumenterVitepress.jl/stable\\\" target=\\\"_blank\\\"><strong>DocumenterVitepress.jl</strong></a><br>Released under the MIT License. Powered by the <a href=\\\"https://www.julialang.org\\\">Julia Programming Language</a>.<br>\",\"copyright\":\"© Copyright 2026 Reactant Development Team.\"},\"lastUpdated\":{\"text\":\"Updated at\",\"formatOptions\":{\"dateStyle\":\"full\",\"timeStyle\":\"medium\"}}},\"locales\":{},\"scrollOffset\":134,\"cleanUrls\":true}");</script>
    
  </body>
</html>